{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Task description:\n",
        "You will repeat the process we did on the IMDb dataset (in the lab part 1 notebook) on the [consumer finance complaints dataset](https://catalog.data.gov/dataset/consumer-complaint-database). This time, however, you will use a pretrained word embedding layer on top of your classification model and evaluate its performance. in addition, You will also train another model with your own randomly initialized embedding layer (like we did in the lab) and compare their performance. I have uploaded a smaller, cleaner version of this data set to [this drive link](https://drive.google.com/file/d/1qr32I8pzfsGIOHrDM3jTqqhMoMnZhMx-/view?usp=sharing), This is what you'll be working work with. This will be a 10 class classification problem. refer to this keras [tutorial](https://keras.io/examples/nlp/pretrained_word_embeddings/) to learn how to deal wth pretrained word embeddings. "
      ],
      "metadata": {
        "id": "n0UTRAMg_2CC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import io\n",
        "import os\n",
        "import re\n",
        "import shutil\n",
        "import string\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import Dense, Embedding, GlobalAveragePooling1D, MaxPooling1D, GlobalMaxPooling1D, Dropout\n",
        "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization"
      ],
      "metadata": {
        "id": "QZyoOwo7sfSP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Download the dataset and explore the dataset structure: "
      ],
      "metadata": {
        "id": "W4YswcFhXZcv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### download the dataset from the drive [link](https://drive.google.com/file/d/1qr32I8pzfsGIOHrDM3jTqqhMoMnZhMx-/view?usp=sharing). "
      ],
      "metadata": {
        "id": "okII-0Q29Hdm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Reuse the code from the lab file, get the file ID from the url \n",
        "# Import PyDrive and associated libraries.\n",
        "# This only needs to be done once per notebook.\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "# Authenticate and create the PyDrive client.\n",
        "# This only needs to be done once per notebook.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "\n",
        "# Download a file based on its file ID.\n",
        "#\n",
        "# A file ID looks like: laggVyWshwcyP6kEI-y_W3P8D26sz\n",
        "file_id = '1qr32I8pzfsGIOHrDM3jTqqhMoMnZhMx-'\n",
        "downloaded = drive.CreateFile({'id': file_id})\n",
        "downloaded.GetContentFile('consumer_complaint_10class.csv')"
      ],
      "metadata": {
        "id": "UnkdM02iS9IJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Read the dataset with pandas, and display a sample.\n",
        "\n",
        "your input data is in the `complaints` column, and your target classes are encoded as integer numbers in the `categories_id` column.The `categories` column containes the category name associated with each category index."
      ],
      "metadata": {
        "id": "cFrc0XLN9Tbc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "XFqvEWwULNSZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv(\"consumer_complaint_10class.csv\")"
      ],
      "metadata": {
        "id": "RYx9k04RtsRO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "6pgA6gzIt0vQ",
        "outputId": "9328c438-bf56-45f8-b654-49237ae18695"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   Unnamed: 0                                          complaint  \\\n",
              "0          15  It should be illegal. I havent use my credit c...   \n",
              "1          16  I have been a Kohls credit card holder for ove...   \n",
              "2          30  Banking services or operating as expected. Sun...   \n",
              "3          31  In accordance with the Fair Credit Reporting a...   \n",
              "4          32  hello dear agency referring to my report i too...   \n",
              "\n",
              "                                            category  category_id  \n",
              "0                        Credit card or prepaid card            0  \n",
              "1  Credit reporting, credit repair services, or o...            1  \n",
              "2                        Checking or savings account            2  \n",
              "3  Credit reporting, credit repair services, or o...            1  \n",
              "4  Credit reporting, credit repair services, or o...            1  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-1629c80b-e673-4b01-b44a-9b6f93a1583a\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>complaint</th>\n",
              "      <th>category</th>\n",
              "      <th>category_id</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>15</td>\n",
              "      <td>It should be illegal. I havent use my credit c...</td>\n",
              "      <td>Credit card or prepaid card</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>16</td>\n",
              "      <td>I have been a Kohls credit card holder for ove...</td>\n",
              "      <td>Credit reporting, credit repair services, or o...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>30</td>\n",
              "      <td>Banking services or operating as expected. Sun...</td>\n",
              "      <td>Checking or savings account</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>31</td>\n",
              "      <td>In accordance with the Fair Credit Reporting a...</td>\n",
              "      <td>Credit reporting, credit repair services, or o...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>32</td>\n",
              "      <td>hello dear agency referring to my report i too...</td>\n",
              "      <td>Credit reporting, credit repair services, or o...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-1629c80b-e673-4b01-b44a-9b6f93a1583a')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-1629c80b-e673-4b01-b44a-9b6f93a1583a button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-1629c80b-e673-4b01-b44a-9b6f93a1583a');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### display the class names and frequencies (value counts)."
      ],
      "metadata": {
        "id": "CHLbZcsn-ZYL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data.category"
      ],
      "metadata": {
        "id": "DuIj6F9rLPP2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eda9a709-230d-41f3-ee73-63a1ba353ad8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0                               Credit card or prepaid card\n",
              "1         Credit reporting, credit repair services, or o...\n",
              "2                               Checking or savings account\n",
              "3         Credit reporting, credit repair services, or o...\n",
              "4         Credit reporting, credit repair services, or o...\n",
              "                                ...                        \n",
              "781713                                     Credit reporting\n",
              "781714                                          Credit card\n",
              "781715                                      Debt collection\n",
              "781716                                             Mortgage\n",
              "781717                                     Credit reporting\n",
              "Name: category, Length: 781718, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data.category.value_counts()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d7G4WsIvuNBs",
        "outputId": "ba7af8ff-f541-4f13-bb7b-2c3aeb546dd6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Credit reporting, credit repair services, or other personal consumer reports    293228\n",
              "Debt collection                                                                 165980\n",
              "Mortgage                                                                         91070\n",
              "Credit card or prepaid card                                                      69094\n",
              "Checking or savings account                                                      44435\n",
              "Student loan                                                                     30426\n",
              "Credit reporting                                                                 29827\n",
              "Money transfer, virtual currency, or money service                               21797\n",
              "Credit card                                                                      18757\n",
              "Vehicle loan or lease                                                            17104\n",
              "Name: category, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# split into training, test and validation. Create a dataset object with a `1024` batch size.\n"
      ],
      "metadata": {
        "id": "N1pRSD9TXhls"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### split into  train, test and validation:\n",
        "split with ratio `60%` train, `20%`validation and, `20%` test. You can use  `sklearn.model_selection.train_test_split`.  "
      ],
      "metadata": {
        "id": "Pap8pifIVJdc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.utils import to_categorical "
      ],
      "metadata": {
        "id": "D3QAKlVRLTxs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = data.complaint\n",
        "y = to_categorical(data.category_id)"
      ],
      "metadata": {
        "id": "aZu8PqD1woUB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.33, random_state=42)"
      ],
      "metadata": {
        "id": "QcSHGBZPZyFE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### create tensorflow dataset objects for the training, validation and test sets:\n",
        " batch size= `1024`\n",
        "\n",
        " check out [`tf.data.Dataset.from_tensor_slices`](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#from_tensor_slices)"
      ],
      "metadata": {
        "id": "c80AgWoq_6Rn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 1024\n",
        "seed = 123\n",
        "\n",
        "train_ds = (tf.data.Dataset.from_tensor_slices((X_train, y_train)).batch(batch_size))\n",
        "test_ds = (tf.data.Dataset.from_tensor_slices((X_test, y_test)).batch(batch_size))\n",
        "val_ds = (tf.data.Dataset.from_tensor_slices((X_val, y_val)).batch(batch_size))"
      ],
      "metadata": {
        "id": "K4jHlNxsLViC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for text_batch, label_batch in train_ds.take(1): #Creates a Dataset with at most 1 element from this dataset.\n",
        "  for i in range(5):\n",
        "    print('label: ', label_batch[i].numpy())\n",
        "    print( 'review text:\\n', text_batch.numpy()[i])\n",
        "    print(\"*\"*20)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TaL23Fwx0S8o",
        "outputId": "3db6ffef-a09f-4bd6-bb4b-52970d83aca9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "label:  5\n",
            "review text:\n",
            " b\"Case number : XXXX ( now XXXX ) : XXXXSenator XXXX, It 's been about 18 months of filing complaints with the CFPB XXXX still we are no closer to a resolution than when we started. \\n\\nOur first CFPB complaint was with XXXX XXXX XXXX who legally IAW RESPA transferred our mortgage to Homeward/Ocwen XXXX. \\n\\nIn the first CFPB complaint it appears that XXXX XXXX XXXX told CFPB that they may have made an error in the mortgage company that they named in the RESPA Notice but they corrected any alleged error by phone call to us. Their claims clearly were not true and not IAW RESPA the CFPB did absolutely nothing. RESPA requires corrections in writing within specified time limits and since none have been received we continue to make payments to Homeward/Ocwen XXXX who continues to accept and process the payments. \\n\\nAfter the Ocwen Loan Servicing LLC letter which was n't and is n't IAW RESPA we contacted Homeward/Ocwen XXXX by phone and they said that we did n't have a loan with them based on the loan number but they had cashed XXXX processed our preceding payment. \\n\\nThere are claims that Ocwen Loan Servicing LLC XXXX told the XXXX government that they were affiliates of Homeward/Ocwen XXXX and that they could direct us to make payments to another place based on that affiliation. \\n\\nEven if an affiliation exists, under RESPA whenever the mortgage payment name or address or amount is changed a proper legal RESPA notice is required to be sent to us and none has been received changing Homeward/Ocwen XXXX. \\n\\nSo, because one affiliate does n't have any legal authority to direct payment changes of another affiliate, we filed a complaint against Ocwen Loan Servicing LLC XXXX. \\n\\nOcwen Loan Servicing LLC XXXX appears to contend that they can say and do anything they want relative to mortgages and they are n't afraid of the federal government and their piddly fines. I guess this is why the CFPB does nothing, it is n't worth it. \\n\\nThe XX/XX/XXXX XXXX XXXX Notice says make payments to Homeward Residential Inc in XXXX XXXX not Ocwen Loan Servicing LLC . Everyone wants our mortgage payment. Seems everyone can send a Notice telling us to change our payment location anytime they want and it apparently is not illegal because nothing appears to ever happen to them. If our mortgage payments are made to the wrong place, it appears the state and federal government does n't care. \\n\\nBelow is the last we sent to the CFPB. \\n\\nXXXX and XXXX XXXX number : XXXX ( now XXXX ) As previously stated every thing remains the same about the non-response from Ocwen to the actual complaint and who has again not addressed the issues and RESPA at all and claims they can send a so-called Transferee Notice saying anything they want in total disregard of the Transferor Notice. \\nIF the CFPB rules the OCWEN Notice of XXXX 2014 is lawful, then CFPB must rule that the XXXX XXXX Notice of XXXX 2014 is also lawful and supercedes the Ocwen Notice and payments will be made to Homeward Residential in TX. \\nI have presented what the CFPB claims RESPA says and what RESPA itself says and OCWEN IS NOT IN COMPLIANCE. Ocwen again has not made specific reference where we are alleged to have claimed untimeliness and they made false reference to it again. \\nThe CFPB must decide if XXXX XXXX XXXX Transferred Servicing to Homeward/Ocwen XXXX XXXX as the XXXX Notice claims or Ocwen as Ocwen 's XXXX unfounded and unmatching Notice claims. \\nTheir attorney and Ocwen claim that Homeward/Ocwen in XXXX and Ocwen Loan Servicing LLC in XXXX are the same identical company so now CFPB must rule on this matter and take the appropriate action while also remembering the XXXX XXXX Notice of XXXX 2014 which supercedes Ocwen 's Notice. We await a CFPB ruling stating both companies with different names and different locations are the same identical\"\n",
            "********************\n",
            "label:  3\n",
            "review text:\n",
            " b'XXXX of XXXX and XXXX NJ is coming after me for a debt we went to Supreme Court In XXXX XXXX NY in XX/XX/XXXX. THEY were not awarded the judge ent and they are also a third party collector for a debt that goes back to XX/XX/XXXX? \\nThey sent me another letter this week demanding all my personal info including banking, car and mortgage! I was horrified and did not provide it to them! \\nWhat should I do???'\n",
            "********************\n",
            "label:  1\n",
            "review text:\n",
            " b'Equifax is denying my request to remove credit inquiry that has been on the report for over 2 years.'\n",
            "********************\n",
            "label:  1\n",
            "review text:\n",
            " b\"My personal information has been used XXXX I found 18 Hard Inquire In one year XXXX XXXX XXXX XXXX XXXX XXXX XXXX XXXX XXXX XXXX XXXX XXXX XXXX XXXX XXXX XXXX XXXX XXXX XXXX XXXX XXXX XXXX XXXX XXXX XXXX XXXX XXXX XXXX XXXX XXXX XXXX XXXX XXXX XXXX XXXX XXXX XXXX XXXX XXXX XXXX XXXX XXXX XXXX XXXX XXXX XXXX XXXX XXXX XXXX XXXX XXXX XXXX XXXX XXXX XXXX XXXX XXXX XXXX XXXX XXXX XXXX XXXX XXXX XXXX on XXXX TransUnion hard inquiries XXXX XXXX XXXX XXXX XXXX XXXX XXXX XXXX XXXX XXXX XXXX XXXX XXXX XXXX XXXX XXXX XXXX XXXX XXXX XXXX XXXX XXXX XXXX XXXX XXXX XXXX XXXX XXXX XXXXXXXX XXXX XXXX XXXX XXXX XXXX XXXX XXXX XXXX XXXX XXXX XXXX XXXX XXXX XXXX was not aware that this was happening until recently I have a XXXX XXXX account from XXXX XXXX for {$2200.00} that I 'm working on trying to get off my credit as we speak I did a father account did a police report TransUnion game medical XXXX XXXX XXXX {$310.00} XXXX XXXX XXXX XXXX XXXX XXXX XXXX {$85.00} XXXX XXXX I 'm just now at XXXX trying to get my credit together together and and I also would have some to update my personal information some addresses if you can just give me a call at XXXX XXXX XXXX XXXX XXXX birthday XXXX XXXX\"\n",
            "********************\n",
            "label:  1\n",
            "review text:\n",
            " b'This is XXXX XXXX, who is submitting this CFPB complaint myself, and to inform you that there is no third party involved in the process. Upon reviewing my updates, incorrect information was found in my reports. The TransUnion Credit Bureau has not complied with the Fair Credit Reporting Act, 15 USC Sections 1681i, and continued reporting as unverified information without any proof provided, within the time allowed by law, is not authorized. Below are the accounts that are reporting on my Credit Reports : XXXX XXXX XXXXXXXX  Amount : XXXX  XXXXDate Filed/Reported : XX/XX/2014 XXXX XXXX ( Original Creditor : XXXX XXXX XXXX XXXX ) XXXXXXXX {$180.00} XXXX XXXX ( Original Creditor : XXXX XXXX XXXX XXXX ) XXXXXXXX {$240.00} XXXX XXXX ( Original Creditor : XXXX XXXX XXXX ) XXXXXXXX {$1700.00} XXXX  XXXXXXXX {$5700.00} XXXXXXXX {$0.00} XXXXXXXX {$0.00} XXXX XXXXXXXX {$0.00} XXXX  XXXXXXXX {$440.00} XXXXXXXX {$2800.00} XXXX XXXXXXXX {$7000.00} XXXX XXXXXXXX {$200.00} XXXX XXXXXXXX  {$500.00} XXXXL XXXXXXXX {$0.00} XXXX XXXX XXXXXXXXXXXX {$0.00}'\n",
            "********************\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# define the preprocessing vectorize layer. \n",
        "\n",
        "1. create a custom preprocessing function (like we did in the lab) to lowercase the text and strip punctuations. You are free to add any extra preprocessing steps you see fit. \n",
        "\n",
        "\n",
        "2. define the vectorizer with `max_tokens=20000` (vocab size=20000) and the custom preprocessing function. \n",
        "\n",
        "\n",
        "3. `Adapt` the vectorizer on a text only version of our dataset (like we did in the lab)."
      ],
      "metadata": {
        "id": "rTd_EKVqZgo5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a custom preprocessing function to convert to lowercase\n",
        "# and strip HTML break tags '<br />'.\n",
        "\n",
        "def custom_preprocessing(input_data):\n",
        "  # convert to lowercase\n",
        "  lowercase = tf.strings.lower(input_data)\n",
        "  # remove html tags\n",
        "  stripped_html = tf.strings.regex_replace(lowercase, '<br />', ' ')\n",
        "  # remove punctuation/special characters and return\n",
        "  return tf.strings.regex_replace(stripped_html,\n",
        "                                  f'[{re.escape(string.punctuation)}]', '')\n",
        "\n",
        "\n",
        "# Max vocabulary size \n",
        "vocab_size = 20000\n",
        "\n",
        "# Use the text vectorization layer to normalize, tokenize, and map strings to\n",
        "# integers. Note that the layer uses the custom preprocessing function defined above.\n",
        "vectorize_layer = TextVectorization(\n",
        "    standardize=custom_preprocessing,\n",
        "    max_tokens=vocab_size,\n",
        "    output_mode='int',\n",
        "    )\n",
        "\n",
        "# Make a text-only dataset (no labels) and call adapt to build the vocabulary.\n",
        "text_ds = train_ds.map(lambda x, y: x)\n",
        "vectorize_layer.adapt(text_ds) #Fits the state of the preprocessing layer to the dataset."
      ],
      "metadata": {
        "id": "i6ZLi2erLfxZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# download a pretrained embedding layer to use in your model. \n",
        "Here you will need to refer to the tensorflow [tutorial](https://keras.io/examples/nlp/pretrained_word_embeddings/). We will use a pretrained Glove model with `embedding dimensionality = 50`. for your convenience I have uploaded it [here](https://drive.google.com/file/d/1wAEvp6qEljV6pOoZVa_LuaPt9q2o41X_/view?usp=sharing)"
      ],
      "metadata": {
        "id": "Fmot6zRPYXjs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### download the model from drive."
      ],
      "metadata": {
        "id": "OAcccWWfDKVi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Download a file based on its file ID.\n",
        "#\n",
        "# A file ID looks like: laggVyWshwcyP6kEI-y_W3P8D26sz\n",
        "file_id = '1wAEvp6qEljV6pOoZVa_LuaPt9q2o41X_'\n",
        "downloaded = drive.CreateFile({'id': file_id})\n",
        "downloaded.GetContentFile('glove.6B.50d.zip')"
      ],
      "metadata": {
        "id": "lBrPMQ3mLiqv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vNodBhEJPKCp"
      },
      "source": [
        "unzip the file into an appropriately named folder.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Unzip the model."
      ],
      "metadata": {
        "id": "SrxcRhBODOLa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip -q glove.6B.50d.zip "
      ],
      "metadata": {
        "id": "WYz9Dd-jLn91",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "55890c98-26cf-404c-b1f0-97727f2c5993"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "replace glove.6B.50d.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### read the words and corresponding embedding from the text file into a dictionary. \n",
        "- read the file line by line. \n",
        "- store the first word of each line as key in the dict.\n",
        "- convert the rest of the line to numpy array and store as value in the dict.\n",
        "\n",
        "checkout `numpy.fromstring`. You can refer to the code from the tensorflow tutorial."
      ],
      "metadata": {
        "id": "6jZqxY2tDST1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import random \n",
        "from glob import glob\n",
        "from pathlib import Path\n",
        "\n",
        "path_to_glove_file = \"glove.6B.50d.txt\"\n",
        "\n",
        "embeddings_index = {}\n",
        "with open(path_to_glove_file) as f:\n",
        "    for line in f:\n",
        "        word, coefs = line.split(maxsplit=1)\n",
        "        coefs = np.fromstring(coefs, \"f\", sep=\" \")\n",
        "        embeddings_index[word] = coefs\n",
        "\n",
        "print(\"Found %s word vectors.\" % len(embeddings_index))"
      ],
      "metadata": {
        "id": "uInd5PZ2Lwf8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ade77859-b09f-4e0c-9b2e-255968bda5ec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 400000 word vectors.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### construct an embedding matrix from the embedding dict just obtained.\n",
        "\n",
        "1. get the vocab list from the vectorizer layer `vectorizer.get_vocabulary()`.\n",
        "2. convert the vocab list into dictionary {'word string': word index}. lets call this ***word_index***.\n",
        "3. for every word in your vocab get the corresponding vector from the embeddings dict and use it to populate the embedding matrix at column number=word index.\n",
        "4. if word is not in embedding dictionary just assign a vector of zeros to it.\n",
        "\n",
        "you can refer to the code from the tensorflow tutorial."
      ],
      "metadata": {
        "id": "8PDGXRB5EYkl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "voc = vectorize_layer.get_vocabulary()\n",
        "word_index = dict(zip(voc, range(len(voc))))"
      ],
      "metadata": {
        "id": "orH591rHLzPX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_tokens = len(voc) + 2\n",
        "embedding_dim = 50  #Change dim to 50\n",
        "hits = 0\n",
        "misses = 0\n",
        "\n",
        "# Prepare embedding matrix\n",
        "embedding_matrix = np.zeros((num_tokens, embedding_dim))\n",
        "for word, i in word_index.items():\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        # Words not found in embedding index will be all-zeros.\n",
        "        # This includes the representation for \"padding\" and \"OOV\"\n",
        "        embedding_matrix[i] = embedding_vector\n",
        "        hits += 1\n",
        "    else:\n",
        "        misses += 1\n",
        "print(\"Converted %d words (%d misses)\" % (hits, misses))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fYu1OGti33PC",
        "outputId": "b1ceee95-e5fe-4400-b83b-65afcab2616c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Converted 16687 words (3313 misses)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# define the model, compile and train. \n",
        "\n"
      ],
      "metadata": {
        "id": "4HZn4SLbcRzy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### define an embedding layer from the pre-trained embeddings matrix. \n",
        "\n",
        "initialize a `tf.keras.layers.embedding` layer object and set the `embeddings_initializer` from your embedding matrix, and set `trainable=False`."
      ],
      "metadata": {
        "id": "ZLGws0hNGzQl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import Embedding\n",
        "\n",
        "embedding_layer = Embedding(\n",
        "    num_tokens,\n",
        "    embedding_dim,\n",
        "    embeddings_initializer=keras.initializers.Constant(embedding_matrix),\n",
        "    trainable=False,\n",
        ")"
      ],
      "metadata": {
        "id": "E35c_VyYAaN8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### define a sequential model containing:\n",
        "1. the vectorize layer. \n",
        "2. the embedding layer.\n",
        "3. a global average pooling 1d layer. \n",
        "4. any number of hidden layers.\n",
        "5. an output layer with the correct shape.\n",
        "\n",
        "Refer to the lab part 1 notebook.  "
      ],
      "metadata": {
        "id": "qzGXwoa5Ha1G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras import layers\n",
        "\n",
        "model = Sequential([\n",
        "  vectorize_layer,\n",
        "  embedding_layer,\n",
        "  GlobalAveragePooling1D(),\n",
        "  Dense(512, activation=\"relu\"),\n",
        "  Dense(256, activation=\"relu\"),\n",
        "  Dense(128, activation=\"relu\"),\n",
        "  Dropout(0.5),\n",
        "  Dense(10, activation=\"softmax\")\n",
        "])\n",
        "model.compile(optimizer='adam',\n",
        "              loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "AXVkC6wZKZca",
        "outputId": "7ffeb8c6-0b75-4dbb-9d31-68e8e4aec872",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " text_vectorization_1 (TextV  (None, None)             0         \n",
            " ectorization)                                                   \n",
            "                                                                 \n",
            " embedding_3 (Embedding)     (None, None, 50)          1000100   \n",
            "                                                                 \n",
            " global_average_pooling1d_4   (None, 50)               0         \n",
            " (GlobalAveragePooling1D)                                        \n",
            "                                                                 \n",
            " dense_16 (Dense)            (None, 512)               26112     \n",
            "                                                                 \n",
            " dense_17 (Dense)            (None, 256)               131328    \n",
            "                                                                 \n",
            " dense_18 (Dense)            (None, 128)               32896     \n",
            "                                                                 \n",
            " dropout_4 (Dropout)         (None, 128)               0         \n",
            "                                                                 \n",
            " dense_19 (Dense)            (None, 10)                1290      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,191,726\n",
            "Trainable params: 191,626\n",
            "Non-trainable params: 1,000,100\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### start the training with any callbacks you want."
      ],
      "metadata": {
        "id": "6IRne2HGH7_I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "early_stop = tf.keras.callbacks.EarlyStopping(patience=5, min_delta=0.005)"
      ],
      "metadata": {
        "id": "bzKUkv0cMAZg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(\n",
        "    train_ds,\n",
        "    validation_data=val_ds,\n",
        "    epochs=35,\n",
        "    callbacks=[early_stop])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FqPeGDh5BsoD",
        "outputId": "026fe776-ab28-419d-c275-6bedcdd8060d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/35\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py:1082: UserWarning: \"`categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a sigmoid or softmax activation and thus does not represent logits. Was this intended?\"\n",
            "  return dispatch_target(*args, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "512/512 [==============================] - 42s 80ms/step - loss: 1.5833 - accuracy: 0.4584 - val_loss: 1.3285 - val_accuracy: 0.5445\n",
            "Epoch 2/35\n",
            "512/512 [==============================] - 40s 78ms/step - loss: 1.2996 - accuracy: 0.5617 - val_loss: 1.2133 - val_accuracy: 0.5847\n",
            "Epoch 3/35\n",
            "512/512 [==============================] - 40s 79ms/step - loss: 1.2303 - accuracy: 0.5874 - val_loss: 1.1590 - val_accuracy: 0.6125\n",
            "Epoch 4/35\n",
            "512/512 [==============================] - 40s 79ms/step - loss: 1.1810 - accuracy: 0.6098 - val_loss: 1.1163 - val_accuracy: 0.6292\n",
            "Epoch 5/35\n",
            "512/512 [==============================] - 40s 78ms/step - loss: 1.1435 - accuracy: 0.6245 - val_loss: 1.0818 - val_accuracy: 0.6410\n",
            "Epoch 6/35\n",
            "512/512 [==============================] - 40s 79ms/step - loss: 1.1179 - accuracy: 0.6346 - val_loss: 1.0597 - val_accuracy: 0.6492\n",
            "Epoch 7/35\n",
            "512/512 [==============================] - 40s 78ms/step - loss: 1.0991 - accuracy: 0.6403 - val_loss: 1.0442 - val_accuracy: 0.6541\n",
            "Epoch 8/35\n",
            "512/512 [==============================] - 40s 79ms/step - loss: 1.0823 - accuracy: 0.6461 - val_loss: 1.0315 - val_accuracy: 0.6589\n",
            "Epoch 9/35\n",
            "512/512 [==============================] - 40s 79ms/step - loss: 1.0699 - accuracy: 0.6502 - val_loss: 1.0191 - val_accuracy: 0.6624\n",
            "Epoch 10/35\n",
            "512/512 [==============================] - 40s 79ms/step - loss: 1.0583 - accuracy: 0.6537 - val_loss: 1.0117 - val_accuracy: 0.6644\n",
            "Epoch 11/35\n",
            "512/512 [==============================] - 40s 79ms/step - loss: 1.0481 - accuracy: 0.6571 - val_loss: 1.0025 - val_accuracy: 0.6665\n",
            "Epoch 12/35\n",
            "512/512 [==============================] - 40s 78ms/step - loss: 1.0394 - accuracy: 0.6597 - val_loss: 0.9957 - val_accuracy: 0.6687\n",
            "Epoch 13/35\n",
            "512/512 [==============================] - 40s 78ms/step - loss: 1.0323 - accuracy: 0.6618 - val_loss: 0.9930 - val_accuracy: 0.6690\n",
            "Epoch 14/35\n",
            "512/512 [==============================] - 40s 79ms/step - loss: 1.0234 - accuracy: 0.6643 - val_loss: 0.9846 - val_accuracy: 0.6708\n",
            "Epoch 15/35\n",
            "512/512 [==============================] - 40s 79ms/step - loss: 1.0157 - accuracy: 0.6663 - val_loss: 0.9766 - val_accuracy: 0.6729\n",
            "Epoch 16/35\n",
            "512/512 [==============================] - 40s 79ms/step - loss: 1.0093 - accuracy: 0.6679 - val_loss: 0.9765 - val_accuracy: 0.6724\n",
            "Epoch 17/35\n",
            "512/512 [==============================] - 40s 78ms/step - loss: 1.0039 - accuracy: 0.6697 - val_loss: 0.9655 - val_accuracy: 0.6765\n",
            "Epoch 18/35\n",
            "512/512 [==============================] - 40s 79ms/step - loss: 0.9980 - accuracy: 0.6712 - val_loss: 0.9611 - val_accuracy: 0.6767\n",
            "Epoch 19/35\n",
            "512/512 [==============================] - 40s 78ms/step - loss: 0.9916 - accuracy: 0.6726 - val_loss: 0.9563 - val_accuracy: 0.6791\n",
            "Epoch 20/35\n",
            "512/512 [==============================] - 40s 78ms/step - loss: 0.9872 - accuracy: 0.6743 - val_loss: 0.9555 - val_accuracy: 0.6793\n",
            "Epoch 21/35\n",
            "512/512 [==============================] - 40s 78ms/step - loss: 0.9830 - accuracy: 0.6750 - val_loss: 0.9518 - val_accuracy: 0.6793\n",
            "Epoch 22/35\n",
            "512/512 [==============================] - 40s 79ms/step - loss: 0.9791 - accuracy: 0.6765 - val_loss: 0.9468 - val_accuracy: 0.6810\n",
            "Epoch 23/35\n",
            "512/512 [==============================] - 40s 78ms/step - loss: 0.9739 - accuracy: 0.6777 - val_loss: 0.9425 - val_accuracy: 0.6828\n",
            "Epoch 24/35\n",
            "512/512 [==============================] - 40s 78ms/step - loss: 0.9704 - accuracy: 0.6788 - val_loss: 0.9476 - val_accuracy: 0.6815\n",
            "Epoch 25/35\n",
            "512/512 [==============================] - 40s 78ms/step - loss: 0.9671 - accuracy: 0.6791 - val_loss: 0.9377 - val_accuracy: 0.6840\n",
            "Epoch 26/35\n",
            "512/512 [==============================] - 40s 78ms/step - loss: 0.9648 - accuracy: 0.6801 - val_loss: 0.9358 - val_accuracy: 0.6849\n",
            "Epoch 27/35\n",
            "512/512 [==============================] - 40s 78ms/step - loss: 0.9600 - accuracy: 0.6815 - val_loss: 0.9348 - val_accuracy: 0.6849\n",
            "Epoch 28/35\n",
            "512/512 [==============================] - 40s 78ms/step - loss: 0.9566 - accuracy: 0.6827 - val_loss: 0.9371 - val_accuracy: 0.6848\n",
            "Epoch 29/35\n",
            "512/512 [==============================] - 40s 78ms/step - loss: 0.9538 - accuracy: 0.6839 - val_loss: 0.9398 - val_accuracy: 0.6839\n",
            "Epoch 30/35\n",
            "512/512 [==============================] - 40s 79ms/step - loss: 0.9508 - accuracy: 0.6838 - val_loss: 0.9317 - val_accuracy: 0.6863\n",
            "Epoch 31/35\n",
            "512/512 [==============================] - 40s 78ms/step - loss: 0.9477 - accuracy: 0.6852 - val_loss: 0.9365 - val_accuracy: 0.6855\n",
            "Epoch 32/35\n",
            "512/512 [==============================] - 40s 78ms/step - loss: 0.9452 - accuracy: 0.6856 - val_loss: 0.9343 - val_accuracy: 0.6858\n",
            "Epoch 33/35\n",
            "512/512 [==============================] - 40s 78ms/step - loss: 0.9418 - accuracy: 0.6869 - val_loss: 0.9336 - val_accuracy: 0.6866\n",
            "Epoch 34/35\n",
            "512/512 [==============================] - 49s 97ms/step - loss: 0.9403 - accuracy: 0.6869 - val_loss: 0.9278 - val_accuracy: 0.6881\n",
            "Epoch 35/35\n",
            "512/512 [==============================] - 40s 78ms/step - loss: 0.9369 - accuracy: 0.6880 - val_loss: 0.9275 - val_accuracy: 0.6874\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f55fe210a90>"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluate your model on the test set."
      ],
      "metadata": {
        "id": "VYPT-IW_d9jz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.evaluate(test_ds)"
      ],
      "metadata": {
        "id": "Zly04tyCMCaH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9057bade-6eab-4d0f-95a0-9a714a53a4b0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "153/153 [==============================] - 8s 52ms/step - loss: 0.9283 - accuracy: 0.6871\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.9282825589179993, 0.6870810389518738]"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# create another model with a randomly initialized trainable embedding layer. Use the same architecture and the same vectorize layer.\n",
        "\n",
        "This is what we did in the lab. refer to the part 1 notebook. It's important that you use the same architecture with the same number of hidden layers so we can fairly compare the two approaches."
      ],
      "metadata": {
        "id": "Y4Lz08DSeFRi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras import layers\n",
        "\n",
        "model = Sequential([\n",
        "  vectorize_layer,\n",
        "  Embedding(vocab_size, embedding_dim, name=\"embedding\"),\n",
        "  GlobalAveragePooling1D(),\n",
        "  Dense(512, activation=\"relu\"),\n",
        "  Dense(256, activation=\"relu\"),\n",
        "  Dense(128, activation=\"relu\"),\n",
        "  Dropout(0.5),\n",
        "  Dense(10, activation=\"softmax\")\n",
        "])\n",
        "model.compile(optimizer='adam',\n",
        "              loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "oDClv87QL6RE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "573f2d12-f5e0-4096-f4d8-f1fab7cca07c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_6\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " text_vectorization_2 (TextV  (None, None)             0         \n",
            " ectorization)                                                   \n",
            "                                                                 \n",
            " embedding (Embedding)       (None, None, 50)          1000000   \n",
            "                                                                 \n",
            " global_average_pooling1d_6   (None, 50)               0         \n",
            " (GlobalAveragePooling1D)                                        \n",
            "                                                                 \n",
            " dense_24 (Dense)            (None, 512)               26112     \n",
            "                                                                 \n",
            " dense_25 (Dense)            (None, 256)               131328    \n",
            "                                                                 \n",
            " dense_26 (Dense)            (None, 128)               32896     \n",
            "                                                                 \n",
            " dropout_6 (Dropout)         (None, 128)               0         \n",
            "                                                                 \n",
            " dense_27 (Dense)            (None, 10)                1290      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,191,626\n",
            "Trainable params: 1,191,626\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# train this other model (with the same parameters)."
      ],
      "metadata": {
        "id": "ioAsFbUyeRy9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "early_stop = tf.keras.callbacks.EarlyStopping(patience=5, min_delta=0.005)"
      ],
      "metadata": {
        "id": "2ODAFCA9KNZ0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(\n",
        "    train_ds,\n",
        "    validation_data=val_ds,\n",
        "    epochs=35,\n",
        "    callbacks=[early_stop])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5c042ce1-4379-4294-f09e-0891d069c957",
        "id": "sUDFam79KNZ0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/35\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py:1082: UserWarning: \"`categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a sigmoid or softmax activation and thus does not represent logits. Was this intended?\"\n",
            "  return dispatch_target(*args, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "512/512 [==============================] - 60s 115ms/step - loss: 1.5903 - accuracy: 0.4401 - val_loss: 1.3253 - val_accuracy: 0.5124\n",
            "Epoch 2/35\n",
            "512/512 [==============================] - 59s 115ms/step - loss: 1.1990 - accuracy: 0.5945 - val_loss: 0.9781 - val_accuracy: 0.6889\n",
            "Epoch 3/35\n",
            "512/512 [==============================] - 59s 115ms/step - loss: 0.9513 - accuracy: 0.6929 - val_loss: 0.8185 - val_accuracy: 0.7329\n",
            "Epoch 4/35\n",
            "512/512 [==============================] - 59s 115ms/step - loss: 0.8422 - accuracy: 0.7270 - val_loss: 0.7651 - val_accuracy: 0.7511\n",
            "Epoch 5/35\n",
            "512/512 [==============================] - 59s 115ms/step - loss: 0.7913 - accuracy: 0.7463 - val_loss: 0.7643 - val_accuracy: 0.7473\n",
            "Epoch 6/35\n",
            "512/512 [==============================] - 59s 115ms/step - loss: 0.7593 - accuracy: 0.7580 - val_loss: 0.7299 - val_accuracy: 0.7592\n",
            "Epoch 7/35\n",
            "512/512 [==============================] - 59s 115ms/step - loss: 0.7256 - accuracy: 0.7692 - val_loss: 0.7092 - val_accuracy: 0.7670\n",
            "Epoch 8/35\n",
            "512/512 [==============================] - 59s 115ms/step - loss: 0.6952 - accuracy: 0.7779 - val_loss: 0.7101 - val_accuracy: 0.7708\n",
            "Epoch 9/35\n",
            "512/512 [==============================] - 59s 115ms/step - loss: 0.6776 - accuracy: 0.7823 - val_loss: 0.6598 - val_accuracy: 0.7849\n",
            "Epoch 10/35\n",
            "512/512 [==============================] - 59s 115ms/step - loss: 0.6627 - accuracy: 0.7851 - val_loss: 0.6458 - val_accuracy: 0.7828\n",
            "Epoch 11/35\n",
            "512/512 [==============================] - 59s 115ms/step - loss: 0.6477 - accuracy: 0.7887 - val_loss: 0.6401 - val_accuracy: 0.7896\n",
            "Epoch 12/35\n",
            "512/512 [==============================] - 59s 115ms/step - loss: 0.6239 - accuracy: 0.7946 - val_loss: 0.6269 - val_accuracy: 0.7887\n",
            "Epoch 13/35\n",
            "512/512 [==============================] - 59s 115ms/step - loss: 0.6208 - accuracy: 0.7949 - val_loss: 0.6174 - val_accuracy: 0.7918\n",
            "Epoch 14/35\n",
            "512/512 [==============================] - 59s 115ms/step - loss: 0.6083 - accuracy: 0.7976 - val_loss: 0.6185 - val_accuracy: 0.7894\n",
            "Epoch 15/35\n",
            "512/512 [==============================] - 59s 115ms/step - loss: 0.5967 - accuracy: 0.8002 - val_loss: 0.6038 - val_accuracy: 0.7954\n",
            "Epoch 16/35\n",
            "512/512 [==============================] - 59s 115ms/step - loss: 0.5913 - accuracy: 0.8011 - val_loss: 0.6091 - val_accuracy: 0.7957\n",
            "Epoch 17/35\n",
            "512/512 [==============================] - 59s 115ms/step - loss: 0.5759 - accuracy: 0.8047 - val_loss: 0.6013 - val_accuracy: 0.7991\n",
            "Epoch 18/35\n",
            "512/512 [==============================] - 59s 115ms/step - loss: 0.5723 - accuracy: 0.8058 - val_loss: 0.5976 - val_accuracy: 0.8000\n",
            "Epoch 19/35\n",
            "512/512 [==============================] - 59s 115ms/step - loss: 0.5600 - accuracy: 0.8085 - val_loss: 0.6001 - val_accuracy: 0.8018\n",
            "Epoch 20/35\n",
            "512/512 [==============================] - 64s 125ms/step - loss: 0.5511 - accuracy: 0.8110 - val_loss: 0.5911 - val_accuracy: 0.8040\n",
            "Epoch 21/35\n",
            "512/512 [==============================] - 59s 115ms/step - loss: 0.5420 - accuracy: 0.8141 - val_loss: 0.6221 - val_accuracy: 0.7941\n",
            "Epoch 22/35\n",
            "512/512 [==============================] - 60s 117ms/step - loss: 0.5359 - accuracy: 0.8162 - val_loss: 0.5837 - val_accuracy: 0.8069\n",
            "Epoch 23/35\n",
            "512/512 [==============================] - 59s 115ms/step - loss: 0.5246 - accuracy: 0.8195 - val_loss: 0.6096 - val_accuracy: 0.7994\n",
            "Epoch 24/35\n",
            "512/512 [==============================] - 58s 114ms/step - loss: 0.5198 - accuracy: 0.8208 - val_loss: 0.5858 - val_accuracy: 0.8085\n",
            "Epoch 25/35\n",
            "512/512 [==============================] - 58s 114ms/step - loss: 0.5090 - accuracy: 0.8243 - val_loss: 0.5773 - val_accuracy: 0.8106\n",
            "Epoch 26/35\n",
            "512/512 [==============================] - 58s 114ms/step - loss: 0.5072 - accuracy: 0.8246 - val_loss: 0.5763 - val_accuracy: 0.8128\n",
            "Epoch 27/35\n",
            "512/512 [==============================] - 59s 114ms/step - loss: 0.4916 - accuracy: 0.8290 - val_loss: 0.5787 - val_accuracy: 0.8129\n",
            "Epoch 28/35\n",
            "512/512 [==============================] - 58s 114ms/step - loss: 0.4872 - accuracy: 0.8303 - val_loss: 0.5900 - val_accuracy: 0.8096\n",
            "Epoch 29/35\n",
            "512/512 [==============================] - 59s 114ms/step - loss: 0.4799 - accuracy: 0.8321 - val_loss: 0.5806 - val_accuracy: 0.8148\n",
            "Epoch 30/35\n",
            "512/512 [==============================] - 59s 115ms/step - loss: 0.4788 - accuracy: 0.8324 - val_loss: 0.5822 - val_accuracy: 0.8137\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f55fef70ad0>"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluate this model on the test set."
      ],
      "metadata": {
        "id": "oazWUbKMJl67"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.evaluate(test_ds)"
      ],
      "metadata": {
        "id": "Vkb41m4rMRqV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "58c78eae-cad1-4573-8d5b-94710985f5c8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "153/153 [==============================] - 8s 51ms/step - loss: 0.5839 - accuracy: 0.8138\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.5839284062385559, 0.8137632608413696]"
            ]
          },
          "metadata": {},
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## compare the two and comment on your results."
      ],
      "metadata": {
        "id": "XlagfGV3eWPE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "****\n",
        "\n",
        "*   **The first model that uses the Pretrained Embedding Layer gives an accuracy of 68%.**\n",
        "*   **The seconed model that uses the randomly intialized Embedding Layer gives an accuracy of 81%.**\n",
        "*   **The reason behind this may be that the pretrained layer was trained using a dataset that doesn't include words of our dataset.**\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "BFlO_cuMev2q"
      }
    }
  ]
}