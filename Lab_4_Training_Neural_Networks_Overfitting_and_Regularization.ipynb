{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s3i0W-wSauZ7"
      },
      "source": [
        "Spring 2022\n",
        "<img src=\"https://www.cuipcairo.org/sites/default/files/styles/medium/public/49658177_2020840098011471_2893445443553853440_n.png?itok=672tzxcF\"\n",
        " width=\"80px\" align=\"right\">\n",
        "\n",
        "CIE 555, Neural Networks and Deep Learning\n",
        "\n",
        "University of Science and Technology, Zewail City\n",
        "\n",
        "<br>\n",
        "\n",
        "<h1 align=\"center\">Lab 4 </h3>\n",
        "<h1 align=\"center\">Training Neural Networks: Part I</h3>\n",
        "<h2 align=\"center\">Overfitting and Regularization</h2>\n",
        "<h3 align=\"center\">Anhar Hassan</h3>\n",
        "\n",
        "---\n",
        "\n",
        "Ibrahim Hamada Ibrahim    201800739\n",
        "\n",
        "\n",
        "Lab Partner: Hazem Tarek 201800283"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Based on Eng. Shahd Seddik previous work.**"
      ],
      "metadata": {
        "id": "LelXrK2FlWyK"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pZ8jaDWAdsaA"
      },
      "source": [
        "#Lab Objectives\n",
        "* Understand the key steps when training a deep learning model.\n",
        "* Understand the relationship between model complexity and overfitting.\n",
        "* Learn how to detect overfitting and underfitting from a neural network's learning curve.\n",
        "* Learn how to avoid overfitting using different regularization techniques."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IcDsqIRl1YoC"
      },
      "source": [
        "# Steps of Training a Deep Learning Model\n",
        "\n",
        "1. Define your problem\n",
        "1. Prepare your data\n",
        "1. Choose your model and appropriately initialize it\n",
        "1. Train the chosen model\n",
        "1. Improve results\n",
        "1. Present results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oZ-PcmeCpfoR"
      },
      "source": [
        "# Dataset\n",
        "\n",
        "One of the most often cited papers about deep learning in combination with a physics application:\n",
        "*Searching for Exotic Particles in High-Energy Physics with Deep Learning* by Pierre Baldi, Peter Sadowski, Daniel Whiteson.\n",
        "\n",
        "We will use the [SUSY Dataset](https://archive.ics.uci.edu/ml/datasets/SUSY) to train a binary classification model.\n",
        "\n",
        "The dataset contains 5,000,000 instances, with 18 features each."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "HuhPbO1H-MJG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/00279/SUSY.csv.gz'\n",
        "columns=[\"signal\", \"lepton 1 pT\", \"lepton 1 eta\", \"lepton 1 phi\", \"lepton 2 pT\", \"lepton 2 eta\", \n",
        "         \"lepton 2 phi\", \"missing energy magnitude\", \"missing energy phi\", \"MET_rel\", \n",
        "         \"axial MET\", \"M_R\", \"M_TR_2\", \"R\", \"MT2\", \"S_R\", \"M_Delta_R\", \"dPhi_r_b\", \"cos(theta_r1)\"]\n",
        "raw_data = pd.read_csv(url, names = columns)"
      ],
      "metadata": {
        "id": "gvyN_-i7-YCX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 305
        },
        "id": "p4J2h1RJYwGS",
        "outputId": "eb70a9ec-bed6-46e5-f968-3dd9bacf6b15"
      },
      "source": [
        "dataset = raw_data.copy()\n",
        "dataset.tail()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "         signal  lepton 1 pT  lepton 1 eta  lepton 1 phi  lepton 2 pT  \\\n",
              "4999995     1.0     0.853325     -0.961783     -1.487277     0.678190   \n",
              "4999996     0.0     0.951581      0.139370      1.436884     0.880440   \n",
              "4999997     0.0     0.840389      1.419162     -1.218766     1.195631   \n",
              "4999998     1.0     1.784218     -0.833565     -0.560091     0.953342   \n",
              "4999999     0.0     0.761500      0.680454     -1.186213     1.043521   \n",
              "\n",
              "         lepton 2 eta  lepton 2 phi  missing energy magnitude  \\\n",
              "4999995      0.493580      1.647969                  1.843867   \n",
              "4999996     -0.351948     -0.740852                  0.290863   \n",
              "4999997      1.695645      0.663756                  0.490888   \n",
              "4999998     -0.688969     -1.428233                  2.660703   \n",
              "4999999     -0.316755      0.246879                  1.120280   \n",
              "\n",
              "         missing energy phi   MET_rel  axial MET       M_R    M_TR_2  \\\n",
              "4999995            0.276954  1.025105  -1.486535  0.892879  1.684429   \n",
              "4999996           -0.732360  0.001360   0.257738  0.802871  0.545319   \n",
              "4999997           -0.509186  0.704289   0.045744  0.825015  0.723530   \n",
              "4999998           -0.861344  2.116892   2.906151  1.232334  0.952444   \n",
              "4999999            0.998479  1.640881  -0.797688  0.854212  1.121858   \n",
              "\n",
              "                R       MT2       S_R  M_Delta_R  dPhi_r_b  cos(theta_r1)  \n",
              "4999995  1.674084  3.366298  1.046707   2.646649  1.389226       0.364599  \n",
              "4999996  0.602730  0.002998  0.748959   0.401166  0.443471       0.239953  \n",
              "4999997  0.778236  0.752942  0.838953   0.614048  1.210595       0.026692  \n",
              "4999998  0.685846  0.000000  0.781874   0.676003  1.197807       0.093689  \n",
              "4999999  1.165438  1.498351  0.931580   1.293524  1.539167       0.187496  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-a0e4ec95-5fb5-4fe4-94c9-45ee5494d922\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>signal</th>\n",
              "      <th>lepton 1 pT</th>\n",
              "      <th>lepton 1 eta</th>\n",
              "      <th>lepton 1 phi</th>\n",
              "      <th>lepton 2 pT</th>\n",
              "      <th>lepton 2 eta</th>\n",
              "      <th>lepton 2 phi</th>\n",
              "      <th>missing energy magnitude</th>\n",
              "      <th>missing energy phi</th>\n",
              "      <th>MET_rel</th>\n",
              "      <th>axial MET</th>\n",
              "      <th>M_R</th>\n",
              "      <th>M_TR_2</th>\n",
              "      <th>R</th>\n",
              "      <th>MT2</th>\n",
              "      <th>S_R</th>\n",
              "      <th>M_Delta_R</th>\n",
              "      <th>dPhi_r_b</th>\n",
              "      <th>cos(theta_r1)</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>4999995</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0.853325</td>\n",
              "      <td>-0.961783</td>\n",
              "      <td>-1.487277</td>\n",
              "      <td>0.678190</td>\n",
              "      <td>0.493580</td>\n",
              "      <td>1.647969</td>\n",
              "      <td>1.843867</td>\n",
              "      <td>0.276954</td>\n",
              "      <td>1.025105</td>\n",
              "      <td>-1.486535</td>\n",
              "      <td>0.892879</td>\n",
              "      <td>1.684429</td>\n",
              "      <td>1.674084</td>\n",
              "      <td>3.366298</td>\n",
              "      <td>1.046707</td>\n",
              "      <td>2.646649</td>\n",
              "      <td>1.389226</td>\n",
              "      <td>0.364599</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4999996</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.951581</td>\n",
              "      <td>0.139370</td>\n",
              "      <td>1.436884</td>\n",
              "      <td>0.880440</td>\n",
              "      <td>-0.351948</td>\n",
              "      <td>-0.740852</td>\n",
              "      <td>0.290863</td>\n",
              "      <td>-0.732360</td>\n",
              "      <td>0.001360</td>\n",
              "      <td>0.257738</td>\n",
              "      <td>0.802871</td>\n",
              "      <td>0.545319</td>\n",
              "      <td>0.602730</td>\n",
              "      <td>0.002998</td>\n",
              "      <td>0.748959</td>\n",
              "      <td>0.401166</td>\n",
              "      <td>0.443471</td>\n",
              "      <td>0.239953</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4999997</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.840389</td>\n",
              "      <td>1.419162</td>\n",
              "      <td>-1.218766</td>\n",
              "      <td>1.195631</td>\n",
              "      <td>1.695645</td>\n",
              "      <td>0.663756</td>\n",
              "      <td>0.490888</td>\n",
              "      <td>-0.509186</td>\n",
              "      <td>0.704289</td>\n",
              "      <td>0.045744</td>\n",
              "      <td>0.825015</td>\n",
              "      <td>0.723530</td>\n",
              "      <td>0.778236</td>\n",
              "      <td>0.752942</td>\n",
              "      <td>0.838953</td>\n",
              "      <td>0.614048</td>\n",
              "      <td>1.210595</td>\n",
              "      <td>0.026692</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4999998</th>\n",
              "      <td>1.0</td>\n",
              "      <td>1.784218</td>\n",
              "      <td>-0.833565</td>\n",
              "      <td>-0.560091</td>\n",
              "      <td>0.953342</td>\n",
              "      <td>-0.688969</td>\n",
              "      <td>-1.428233</td>\n",
              "      <td>2.660703</td>\n",
              "      <td>-0.861344</td>\n",
              "      <td>2.116892</td>\n",
              "      <td>2.906151</td>\n",
              "      <td>1.232334</td>\n",
              "      <td>0.952444</td>\n",
              "      <td>0.685846</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.781874</td>\n",
              "      <td>0.676003</td>\n",
              "      <td>1.197807</td>\n",
              "      <td>0.093689</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4999999</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.761500</td>\n",
              "      <td>0.680454</td>\n",
              "      <td>-1.186213</td>\n",
              "      <td>1.043521</td>\n",
              "      <td>-0.316755</td>\n",
              "      <td>0.246879</td>\n",
              "      <td>1.120280</td>\n",
              "      <td>0.998479</td>\n",
              "      <td>1.640881</td>\n",
              "      <td>-0.797688</td>\n",
              "      <td>0.854212</td>\n",
              "      <td>1.121858</td>\n",
              "      <td>1.165438</td>\n",
              "      <td>1.498351</td>\n",
              "      <td>0.931580</td>\n",
              "      <td>1.293524</td>\n",
              "      <td>1.539167</td>\n",
              "      <td>0.187496</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-a0e4ec95-5fb5-4fe4-94c9-45ee5494d922')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-a0e4ec95-5fb5-4fe4-94c9-45ee5494d922 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-a0e4ec95-5fb5-4fe4-94c9-45ee5494d922');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "features = np.array(dataset.drop(columns='signal'))\n",
        "labels =np.array(dataset['signal']).reshape(-1,1)"
      ],
      "metadata": {
        "id": "Qv0GKYrTQ7iN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rUptyOnSJGt4"
      },
      "source": [
        "We will take a subset of the data to save time. Skip the next two cells if you want to use the entire dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "svjEFoQSJFtH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8589d0f1-0871-4f73-ecd9-b2151e03302d"
      },
      "source": [
        "n_samples = 20000 # number of samples to take\n",
        "\n",
        "indices = np.random.randint(features.shape[0], size = n_samples) # generate random indices\n",
        "\n",
        "features = features[indices, :]\n",
        "labels = labels[indices, :]\n",
        "print(features.shape)\n",
        "print(labels.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(20000, 18)\n",
            "(20000, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Tkio0DeKdAH"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2njlmce9dbxN"
      },
      "source": [
        "## Pre-processing\n",
        "\n",
        "Let's split our dataset into training and test sets.\n",
        "\n",
        "Then, we will fit a pre-processing function to standardize features by removing the mean and scaling to unit variance.\n",
        "\n",
        "The standard score of a sample $x$ is calculated as\n",
        "\n",
        "$z = (x - \\mu) / \\sigma$\n",
        "\n",
        "where $\\mu$ is the mean of the training samples, and $\\sigma$ is the standard deviation of the training samples."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RdmPZVO-5978",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "70f72eff-7b55-4be0-84c5-1722cb1884ef"
      },
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Split dataset into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "        features, labels, test_size=0.10, random_state=42, shuffle=True)\n",
        "\n",
        "# Set up preprocessing \n",
        "preprocessing_input = StandardScaler()\n",
        "preprocessing_input.fit(X_train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "StandardScaler()"
            ]
          },
          "metadata": {},
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GNaayixgfCQL"
      },
      "source": [
        "# Overfitting and Underfitting\n",
        "\n",
        "Supervised machine learning can be thought of as approximating a target function $f$ that maps input point $X$ to output $y$ (i.e. $y=f(X)$).\n",
        "\n",
        "It is crucial that the learned function be able to generalize well over new, unseen data. This allows us to make accurate predictions in the future on data the model has never seen.\n",
        "\n",
        "So, what is overfitting? And when does it happen? (Discussion)\n",
        "\n",
        "<center><img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/1/19/Overfitting.svg/1200px-Overfitting.svg.png\"\n",
        " width=\"200px\"> </center>\n",
        " \n",
        ">- Overfitting refers to the problem when a model learns (or memorizes) the detail and noise in the training data to the extent that it negatively impacts the performance of the model on new data.\n",
        "- What is meant by the representational capacity of a model?\n",
        "- (Fill in the blank) Overfitting is more likely to happen when the model has ............... (larger/smaller) representational capacity.\n",
        "\n",
        "\n",
        "On the other hand, underfitting happens when the model is neither able to model the training data nor able to generalize to new data.\n",
        "\n",
        "To summarize:\n",
        "<center><img src=\"https://gblobscdn.gitbook.com/assets%2F-LvBP1svpACTB1R1x_U4%2F-LvNWUoWieQqaGmU_gl9%2F-LvNoby-llz4QzAK15nL%2Fimage.png?alt=media&token=41720ce9-bb66-4419-9bd8-640abf1fc415\"\n",
        " width=\"400px\"> </center>\n",
        "\n",
        " In the context of neural networks, overfitting and underfitting can be detected by looking at the training and validation loss during training.\n",
        " \n",
        "<center><img src=\"https://drek4537l1klr.cloudfront.net/cai/Figures/08fig06_alt.jpg\"\n",
        " width=\"600px\"> </center>\n",
        "\n",
        " >- (Q) Assuming the three graphs above were generated from three different models over the same dataset. Which model is the most complex? Which is the least?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dIhSys2BwFBo"
      },
      "source": [
        "# Models\n",
        "\n",
        "Let's build neural nets to solve this classification problem! We will build 4 different models with varying complexities to explore overfitting and underfitting.\n",
        "\n",
        "Each model will use the same `compile` and `fit` methods, so let's code functions to make our code cleaner and more modular."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "30Yr1AxMwOTf"
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers.core import Dense\n",
        "from keras.optimizers import adam_v2\n",
        "\n",
        "BATCH_SIZE = 500\n",
        "EPOCHS = 200\n",
        "\n",
        "def get_optimizer():\n",
        "  return adam_v2.Adam()\n",
        "\n",
        "def compile_and_fit(model, optimizer=None, max_epochs=1000):\n",
        "  if optimizer is None:\n",
        "    optimizer = get_optimizer()\n",
        "  \n",
        "  # Compile model and print summary\n",
        "  model.compile(optimizer = optimizer,\n",
        "                loss = \"binary_crossentropy\",\n",
        "                metrics = ['accuracy'])\n",
        "\n",
        "  model.summary()\n",
        "\n",
        "  # Fit model to training data\n",
        "  history = model.fit(\n",
        "    x = preprocessing_input.transform(X_train), # do not forget to normalize X\n",
        "    y = y_train,\n",
        "    batch_size = BATCH_SIZE,\n",
        "    epochs = max_epochs,\n",
        "    validation_split = 0.1)\n",
        "\n",
        "  return history\n",
        "\n",
        "# Create a dictionary to store the histories of all trained models\n",
        "model_histories = {}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RS2ldugHoDwL"
      },
      "source": [
        "Now, let's get to building our 4 models! We will call them tiny model, small model, medium model, and large model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F_6jdazun7iI"
      },
      "source": [
        "shape = features.shape[1]\n",
        "# Model 1\n",
        "tiny_model = Sequential([\n",
        "    Dense(8, activation='relu', input_shape=(shape,)),\n",
        "    Dense(1, activation=\"sigmoid\")\n",
        "])\n",
        "\n",
        "# Model 2\n",
        "small_model = Sequential([\n",
        "    Dense(16, activation='relu', input_shape=(shape,)),\n",
        "    Dense(16, activation='relu'),\n",
        "    Dense(1, activation=\"sigmoid\")\n",
        "])\n",
        "\n",
        "# Model 3\n",
        "medium_model = Sequential([\n",
        "    Dense(64, activation='relu', input_shape=(shape,)),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dense(1, activation=\"sigmoid\")\n",
        "])\n",
        "\n",
        "# Model 4\n",
        "large_model = Sequential([\n",
        "    Dense(512, activation='relu', input_shape=(shape,)),\n",
        "    Dense(512, activation='relu'),\n",
        "    Dense(512, activation='relu'),\n",
        "    Dense(512, activation='relu'),\n",
        "    Dense(1, activation=\"sigmoid\")\n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BaZiw14l2PVC"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JIcplKO72D76",
        "outputId": "7b6937e1-5b16-4a98-9b87-32fc05398ba4"
      },
      "source": [
        "model_histories['Tiny'] = compile_and_fit(tiny_model, max_epochs = EPOCHS)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_26\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_123 (Dense)           (None, 8)                 152       \n",
            "                                                                 \n",
            " dense_124 (Dense)           (None, 1)                 9         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 161\n",
            "Trainable params: 161\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/200\n",
            "33/33 [==============================] - 1s 7ms/step - loss: 0.8840 - accuracy: 0.4610 - val_loss: 0.7911 - val_accuracy: 0.4961\n",
            "Epoch 2/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.7353 - accuracy: 0.5500 - val_loss: 0.6757 - val_accuracy: 0.5861\n",
            "Epoch 3/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.6439 - accuracy: 0.6351 - val_loss: 0.6109 - val_accuracy: 0.6606\n",
            "Epoch 4/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.5938 - accuracy: 0.6893 - val_loss: 0.5767 - val_accuracy: 0.6922\n",
            "Epoch 5/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.5667 - accuracy: 0.7163 - val_loss: 0.5569 - val_accuracy: 0.7133\n",
            "Epoch 6/200\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.5501 - accuracy: 0.7327 - val_loss: 0.5440 - val_accuracy: 0.7217\n",
            "Epoch 7/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.5385 - accuracy: 0.7425 - val_loss: 0.5339 - val_accuracy: 0.7322\n",
            "Epoch 8/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.5295 - accuracy: 0.7470 - val_loss: 0.5258 - val_accuracy: 0.7400\n",
            "Epoch 9/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.5219 - accuracy: 0.7519 - val_loss: 0.5191 - val_accuracy: 0.7439\n",
            "Epoch 10/200\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.5154 - accuracy: 0.7560 - val_loss: 0.5133 - val_accuracy: 0.7467\n",
            "Epoch 11/200\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.5099 - accuracy: 0.7594 - val_loss: 0.5079 - val_accuracy: 0.7506\n",
            "Epoch 12/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.5049 - accuracy: 0.7628 - val_loss: 0.5034 - val_accuracy: 0.7539\n",
            "Epoch 13/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.5003 - accuracy: 0.7651 - val_loss: 0.4992 - val_accuracy: 0.7561\n",
            "Epoch 14/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.4962 - accuracy: 0.7680 - val_loss: 0.4954 - val_accuracy: 0.7589\n",
            "Epoch 15/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.4925 - accuracy: 0.7700 - val_loss: 0.4922 - val_accuracy: 0.7622\n",
            "Epoch 16/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.4892 - accuracy: 0.7722 - val_loss: 0.4895 - val_accuracy: 0.7678\n",
            "Epoch 17/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.4861 - accuracy: 0.7736 - val_loss: 0.4869 - val_accuracy: 0.7683\n",
            "Epoch 18/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.4833 - accuracy: 0.7746 - val_loss: 0.4847 - val_accuracy: 0.7694\n",
            "Epoch 19/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.4807 - accuracy: 0.7764 - val_loss: 0.4828 - val_accuracy: 0.7700\n",
            "Epoch 20/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.4784 - accuracy: 0.7780 - val_loss: 0.4809 - val_accuracy: 0.7700\n",
            "Epoch 21/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.4763 - accuracy: 0.7793 - val_loss: 0.4794 - val_accuracy: 0.7683\n",
            "Epoch 22/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.4745 - accuracy: 0.7809 - val_loss: 0.4778 - val_accuracy: 0.7700\n",
            "Epoch 23/200\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.4727 - accuracy: 0.7817 - val_loss: 0.4763 - val_accuracy: 0.7683\n",
            "Epoch 24/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.4711 - accuracy: 0.7827 - val_loss: 0.4750 - val_accuracy: 0.7689\n",
            "Epoch 25/200\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.4696 - accuracy: 0.7828 - val_loss: 0.4739 - val_accuracy: 0.7683\n",
            "Epoch 26/200\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.4682 - accuracy: 0.7836 - val_loss: 0.4728 - val_accuracy: 0.7694\n",
            "Epoch 27/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.4669 - accuracy: 0.7851 - val_loss: 0.4718 - val_accuracy: 0.7706\n",
            "Epoch 28/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.4657 - accuracy: 0.7853 - val_loss: 0.4706 - val_accuracy: 0.7689\n",
            "Epoch 29/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.4645 - accuracy: 0.7862 - val_loss: 0.4697 - val_accuracy: 0.7700\n",
            "Epoch 30/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.4634 - accuracy: 0.7864 - val_loss: 0.4686 - val_accuracy: 0.7722\n",
            "Epoch 31/200\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.4624 - accuracy: 0.7871 - val_loss: 0.4677 - val_accuracy: 0.7733\n",
            "Epoch 32/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.4612 - accuracy: 0.7877 - val_loss: 0.4669 - val_accuracy: 0.7728\n",
            "Epoch 33/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.4602 - accuracy: 0.7875 - val_loss: 0.4661 - val_accuracy: 0.7761\n",
            "Epoch 34/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.4591 - accuracy: 0.7880 - val_loss: 0.4653 - val_accuracy: 0.7750\n",
            "Epoch 35/200\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.4583 - accuracy: 0.7892 - val_loss: 0.4647 - val_accuracy: 0.7783\n",
            "Epoch 36/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.4574 - accuracy: 0.7890 - val_loss: 0.4640 - val_accuracy: 0.7772\n",
            "Epoch 37/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.4565 - accuracy: 0.7893 - val_loss: 0.4635 - val_accuracy: 0.7794\n",
            "Epoch 38/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.4555 - accuracy: 0.7904 - val_loss: 0.4627 - val_accuracy: 0.7767\n",
            "Epoch 39/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.4547 - accuracy: 0.7914 - val_loss: 0.4621 - val_accuracy: 0.7778\n",
            "Epoch 40/200\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.4539 - accuracy: 0.7918 - val_loss: 0.4616 - val_accuracy: 0.7783\n",
            "Epoch 41/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.4530 - accuracy: 0.7930 - val_loss: 0.4608 - val_accuracy: 0.7794\n",
            "Epoch 42/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.4523 - accuracy: 0.7931 - val_loss: 0.4603 - val_accuracy: 0.7778\n",
            "Epoch 43/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.4516 - accuracy: 0.7934 - val_loss: 0.4594 - val_accuracy: 0.7783\n",
            "Epoch 44/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.4510 - accuracy: 0.7939 - val_loss: 0.4589 - val_accuracy: 0.7789\n",
            "Epoch 45/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.4503 - accuracy: 0.7949 - val_loss: 0.4582 - val_accuracy: 0.7806\n",
            "Epoch 46/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.4496 - accuracy: 0.7944 - val_loss: 0.4578 - val_accuracy: 0.7811\n",
            "Epoch 47/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.4489 - accuracy: 0.7957 - val_loss: 0.4571 - val_accuracy: 0.7817\n",
            "Epoch 48/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.4482 - accuracy: 0.7954 - val_loss: 0.4566 - val_accuracy: 0.7800\n",
            "Epoch 49/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.4476 - accuracy: 0.7959 - val_loss: 0.4560 - val_accuracy: 0.7800\n",
            "Epoch 50/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.4469 - accuracy: 0.7959 - val_loss: 0.4556 - val_accuracy: 0.7817\n",
            "Epoch 51/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.4465 - accuracy: 0.7964 - val_loss: 0.4553 - val_accuracy: 0.7833\n",
            "Epoch 52/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.4457 - accuracy: 0.7957 - val_loss: 0.4550 - val_accuracy: 0.7828\n",
            "Epoch 53/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.4453 - accuracy: 0.7963 - val_loss: 0.4545 - val_accuracy: 0.7861\n",
            "Epoch 54/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.4447 - accuracy: 0.7964 - val_loss: 0.4544 - val_accuracy: 0.7867\n",
            "Epoch 55/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.4442 - accuracy: 0.7967 - val_loss: 0.4541 - val_accuracy: 0.7872\n",
            "Epoch 56/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.4437 - accuracy: 0.7973 - val_loss: 0.4539 - val_accuracy: 0.7867\n",
            "Epoch 57/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.4433 - accuracy: 0.7978 - val_loss: 0.4536 - val_accuracy: 0.7861\n",
            "Epoch 58/200\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.4426 - accuracy: 0.7977 - val_loss: 0.4532 - val_accuracy: 0.7872\n",
            "Epoch 59/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.4422 - accuracy: 0.7976 - val_loss: 0.4529 - val_accuracy: 0.7867\n",
            "Epoch 60/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.4418 - accuracy: 0.7985 - val_loss: 0.4527 - val_accuracy: 0.7861\n",
            "Epoch 61/200\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.4415 - accuracy: 0.7983 - val_loss: 0.4524 - val_accuracy: 0.7861\n",
            "Epoch 62/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.4411 - accuracy: 0.7988 - val_loss: 0.4522 - val_accuracy: 0.7844\n",
            "Epoch 63/200\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.4408 - accuracy: 0.7989 - val_loss: 0.4520 - val_accuracy: 0.7861\n",
            "Epoch 64/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.4405 - accuracy: 0.7981 - val_loss: 0.4521 - val_accuracy: 0.7856\n",
            "Epoch 65/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.4401 - accuracy: 0.7985 - val_loss: 0.4519 - val_accuracy: 0.7883\n",
            "Epoch 66/200\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.4398 - accuracy: 0.7986 - val_loss: 0.4519 - val_accuracy: 0.7867\n",
            "Epoch 67/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.4396 - accuracy: 0.7993 - val_loss: 0.4516 - val_accuracy: 0.7894\n",
            "Epoch 68/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.4393 - accuracy: 0.7991 - val_loss: 0.4515 - val_accuracy: 0.7867\n",
            "Epoch 69/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.4391 - accuracy: 0.7997 - val_loss: 0.4512 - val_accuracy: 0.7894\n",
            "Epoch 70/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.4388 - accuracy: 0.7996 - val_loss: 0.4512 - val_accuracy: 0.7839\n",
            "Epoch 71/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.4388 - accuracy: 0.7988 - val_loss: 0.4510 - val_accuracy: 0.7883\n",
            "Epoch 72/200\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.4384 - accuracy: 0.7993 - val_loss: 0.4508 - val_accuracy: 0.7867\n",
            "Epoch 73/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.4383 - accuracy: 0.7993 - val_loss: 0.4505 - val_accuracy: 0.7889\n",
            "Epoch 74/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.4380 - accuracy: 0.7988 - val_loss: 0.4505 - val_accuracy: 0.7878\n",
            "Epoch 75/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.4379 - accuracy: 0.7987 - val_loss: 0.4503 - val_accuracy: 0.7867\n",
            "Epoch 76/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.4378 - accuracy: 0.7990 - val_loss: 0.4499 - val_accuracy: 0.7872\n",
            "Epoch 77/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.4375 - accuracy: 0.7996 - val_loss: 0.4501 - val_accuracy: 0.7867\n",
            "Epoch 78/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.4372 - accuracy: 0.7993 - val_loss: 0.4497 - val_accuracy: 0.7856\n",
            "Epoch 79/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.4371 - accuracy: 0.7985 - val_loss: 0.4499 - val_accuracy: 0.7856\n",
            "Epoch 80/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.4369 - accuracy: 0.7986 - val_loss: 0.4496 - val_accuracy: 0.7867\n",
            "Epoch 81/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.4369 - accuracy: 0.7988 - val_loss: 0.4496 - val_accuracy: 0.7867\n",
            "Epoch 82/200\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.4366 - accuracy: 0.7993 - val_loss: 0.4493 - val_accuracy: 0.7867\n",
            "Epoch 83/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.4364 - accuracy: 0.7994 - val_loss: 0.4495 - val_accuracy: 0.7850\n",
            "Epoch 84/200\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.4362 - accuracy: 0.7994 - val_loss: 0.4492 - val_accuracy: 0.7867\n",
            "Epoch 85/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.4362 - accuracy: 0.7990 - val_loss: 0.4490 - val_accuracy: 0.7883\n",
            "Epoch 86/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.4360 - accuracy: 0.7993 - val_loss: 0.4491 - val_accuracy: 0.7867\n",
            "Epoch 87/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.4360 - accuracy: 0.7991 - val_loss: 0.4488 - val_accuracy: 0.7878\n",
            "Epoch 88/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.4358 - accuracy: 0.7993 - val_loss: 0.4487 - val_accuracy: 0.7894\n",
            "Epoch 89/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.4359 - accuracy: 0.7992 - val_loss: 0.4488 - val_accuracy: 0.7894\n",
            "Epoch 90/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.4357 - accuracy: 0.7991 - val_loss: 0.4488 - val_accuracy: 0.7889\n",
            "Epoch 91/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.4355 - accuracy: 0.7993 - val_loss: 0.4486 - val_accuracy: 0.7889\n",
            "Epoch 92/200\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.4355 - accuracy: 0.7995 - val_loss: 0.4486 - val_accuracy: 0.7900\n",
            "Epoch 93/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.4352 - accuracy: 0.7996 - val_loss: 0.4484 - val_accuracy: 0.7894\n",
            "Epoch 94/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.4351 - accuracy: 0.7991 - val_loss: 0.4483 - val_accuracy: 0.7894\n",
            "Epoch 95/200\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.4351 - accuracy: 0.8003 - val_loss: 0.4483 - val_accuracy: 0.7900\n",
            "Epoch 96/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.4350 - accuracy: 0.7999 - val_loss: 0.4482 - val_accuracy: 0.7889\n",
            "Epoch 97/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.4349 - accuracy: 0.7996 - val_loss: 0.4483 - val_accuracy: 0.7900\n",
            "Epoch 98/200\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.4347 - accuracy: 0.7993 - val_loss: 0.4481 - val_accuracy: 0.7900\n",
            "Epoch 99/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.4348 - accuracy: 0.7999 - val_loss: 0.4485 - val_accuracy: 0.7917\n",
            "Epoch 100/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.4347 - accuracy: 0.7996 - val_loss: 0.4481 - val_accuracy: 0.7906\n",
            "Epoch 101/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.4346 - accuracy: 0.8003 - val_loss: 0.4482 - val_accuracy: 0.7911\n",
            "Epoch 102/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.4345 - accuracy: 0.7997 - val_loss: 0.4480 - val_accuracy: 0.7906\n",
            "Epoch 103/200\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.4344 - accuracy: 0.7999 - val_loss: 0.4479 - val_accuracy: 0.7928\n",
            "Epoch 104/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.4343 - accuracy: 0.7998 - val_loss: 0.4480 - val_accuracy: 0.7917\n",
            "Epoch 105/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.4342 - accuracy: 0.7996 - val_loss: 0.4480 - val_accuracy: 0.7922\n",
            "Epoch 106/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.4342 - accuracy: 0.8007 - val_loss: 0.4480 - val_accuracy: 0.7933\n",
            "Epoch 107/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.4342 - accuracy: 0.8002 - val_loss: 0.4480 - val_accuracy: 0.7928\n",
            "Epoch 108/200\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.4341 - accuracy: 0.7983 - val_loss: 0.4480 - val_accuracy: 0.7933\n",
            "Epoch 109/200\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.4342 - accuracy: 0.8000 - val_loss: 0.4478 - val_accuracy: 0.7922\n",
            "Epoch 110/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.4340 - accuracy: 0.8002 - val_loss: 0.4477 - val_accuracy: 0.7928\n",
            "Epoch 111/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.4340 - accuracy: 0.7999 - val_loss: 0.4477 - val_accuracy: 0.7928\n",
            "Epoch 112/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.4339 - accuracy: 0.8006 - val_loss: 0.4479 - val_accuracy: 0.7928\n",
            "Epoch 113/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.4339 - accuracy: 0.8001 - val_loss: 0.4473 - val_accuracy: 0.7917\n",
            "Epoch 114/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.4338 - accuracy: 0.8001 - val_loss: 0.4476 - val_accuracy: 0.7917\n",
            "Epoch 115/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.4338 - accuracy: 0.8012 - val_loss: 0.4475 - val_accuracy: 0.7928\n",
            "Epoch 116/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.4339 - accuracy: 0.8005 - val_loss: 0.4474 - val_accuracy: 0.7939\n",
            "Epoch 117/200\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.4336 - accuracy: 0.8007 - val_loss: 0.4474 - val_accuracy: 0.7933\n",
            "Epoch 118/200\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.4336 - accuracy: 0.8010 - val_loss: 0.4475 - val_accuracy: 0.7911\n",
            "Epoch 119/200\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.4336 - accuracy: 0.8010 - val_loss: 0.4476 - val_accuracy: 0.7922\n",
            "Epoch 120/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.4335 - accuracy: 0.8002 - val_loss: 0.4473 - val_accuracy: 0.7933\n",
            "Epoch 121/200\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.4335 - accuracy: 0.8007 - val_loss: 0.4475 - val_accuracy: 0.7928\n",
            "Epoch 122/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.4334 - accuracy: 0.8004 - val_loss: 0.4474 - val_accuracy: 0.7928\n",
            "Epoch 123/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.4333 - accuracy: 0.8002 - val_loss: 0.4471 - val_accuracy: 0.7944\n",
            "Epoch 124/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.4334 - accuracy: 0.8010 - val_loss: 0.4474 - val_accuracy: 0.7939\n",
            "Epoch 125/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.4332 - accuracy: 0.8004 - val_loss: 0.4471 - val_accuracy: 0.7944\n",
            "Epoch 126/200\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.4333 - accuracy: 0.8004 - val_loss: 0.4471 - val_accuracy: 0.7933\n",
            "Epoch 127/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.4335 - accuracy: 0.8010 - val_loss: 0.4470 - val_accuracy: 0.7933\n",
            "Epoch 128/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.4333 - accuracy: 0.7999 - val_loss: 0.4473 - val_accuracy: 0.7950\n",
            "Epoch 129/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.4332 - accuracy: 0.7999 - val_loss: 0.4473 - val_accuracy: 0.7933\n",
            "Epoch 130/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.4331 - accuracy: 0.8003 - val_loss: 0.4469 - val_accuracy: 0.7950\n",
            "Epoch 131/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.4332 - accuracy: 0.8013 - val_loss: 0.4474 - val_accuracy: 0.7928\n",
            "Epoch 132/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.4331 - accuracy: 0.8009 - val_loss: 0.4469 - val_accuracy: 0.7939\n",
            "Epoch 133/200\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.4328 - accuracy: 0.8006 - val_loss: 0.4470 - val_accuracy: 0.7950\n",
            "Epoch 134/200\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.4329 - accuracy: 0.8006 - val_loss: 0.4470 - val_accuracy: 0.7933\n",
            "Epoch 135/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.4329 - accuracy: 0.8006 - val_loss: 0.4470 - val_accuracy: 0.7922\n",
            "Epoch 136/200\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.4329 - accuracy: 0.8007 - val_loss: 0.4469 - val_accuracy: 0.7944\n",
            "Epoch 137/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.4328 - accuracy: 0.8008 - val_loss: 0.4470 - val_accuracy: 0.7944\n",
            "Epoch 138/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.4329 - accuracy: 0.8006 - val_loss: 0.4468 - val_accuracy: 0.7933\n",
            "Epoch 139/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.4328 - accuracy: 0.8014 - val_loss: 0.4471 - val_accuracy: 0.7928\n",
            "Epoch 140/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.4329 - accuracy: 0.8011 - val_loss: 0.4467 - val_accuracy: 0.7939\n",
            "Epoch 141/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.4327 - accuracy: 0.8005 - val_loss: 0.4471 - val_accuracy: 0.7939\n",
            "Epoch 142/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.4325 - accuracy: 0.8004 - val_loss: 0.4470 - val_accuracy: 0.7933\n",
            "Epoch 143/200\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.4326 - accuracy: 0.8010 - val_loss: 0.4466 - val_accuracy: 0.7944\n",
            "Epoch 144/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.4326 - accuracy: 0.8006 - val_loss: 0.4468 - val_accuracy: 0.7933\n",
            "Epoch 145/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.4326 - accuracy: 0.8002 - val_loss: 0.4469 - val_accuracy: 0.7939\n",
            "Epoch 146/200\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.4325 - accuracy: 0.8002 - val_loss: 0.4469 - val_accuracy: 0.7944\n",
            "Epoch 147/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.4325 - accuracy: 0.8012 - val_loss: 0.4467 - val_accuracy: 0.7944\n",
            "Epoch 148/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.4325 - accuracy: 0.8010 - val_loss: 0.4468 - val_accuracy: 0.7944\n",
            "Epoch 149/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.4325 - accuracy: 0.8009 - val_loss: 0.4467 - val_accuracy: 0.7933\n",
            "Epoch 150/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.4325 - accuracy: 0.8007 - val_loss: 0.4466 - val_accuracy: 0.7939\n",
            "Epoch 151/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.4324 - accuracy: 0.8007 - val_loss: 0.4468 - val_accuracy: 0.7944\n",
            "Epoch 152/200\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.4323 - accuracy: 0.8009 - val_loss: 0.4463 - val_accuracy: 0.7939\n",
            "Epoch 153/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.4323 - accuracy: 0.8008 - val_loss: 0.4465 - val_accuracy: 0.7922\n",
            "Epoch 154/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.4323 - accuracy: 0.8003 - val_loss: 0.4467 - val_accuracy: 0.7928\n",
            "Epoch 155/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.4322 - accuracy: 0.8004 - val_loss: 0.4464 - val_accuracy: 0.7956\n",
            "Epoch 156/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.4322 - accuracy: 0.8012 - val_loss: 0.4461 - val_accuracy: 0.7944\n",
            "Epoch 157/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.4322 - accuracy: 0.8009 - val_loss: 0.4464 - val_accuracy: 0.7950\n",
            "Epoch 158/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.4322 - accuracy: 0.8010 - val_loss: 0.4467 - val_accuracy: 0.7944\n",
            "Epoch 159/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.4321 - accuracy: 0.8006 - val_loss: 0.4463 - val_accuracy: 0.7961\n",
            "Epoch 160/200\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.4319 - accuracy: 0.8008 - val_loss: 0.4465 - val_accuracy: 0.7939\n",
            "Epoch 161/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.4319 - accuracy: 0.8006 - val_loss: 0.4462 - val_accuracy: 0.7961\n",
            "Epoch 162/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.4318 - accuracy: 0.8006 - val_loss: 0.4464 - val_accuracy: 0.7978\n",
            "Epoch 163/200\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.4318 - accuracy: 0.8007 - val_loss: 0.4460 - val_accuracy: 0.7950\n",
            "Epoch 164/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.4318 - accuracy: 0.8012 - val_loss: 0.4465 - val_accuracy: 0.7956\n",
            "Epoch 165/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.4318 - accuracy: 0.8017 - val_loss: 0.4463 - val_accuracy: 0.7972\n",
            "Epoch 166/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.4319 - accuracy: 0.8011 - val_loss: 0.4465 - val_accuracy: 0.7972\n",
            "Epoch 167/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.4319 - accuracy: 0.8012 - val_loss: 0.4462 - val_accuracy: 0.7939\n",
            "Epoch 168/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.4317 - accuracy: 0.8008 - val_loss: 0.4465 - val_accuracy: 0.7950\n",
            "Epoch 169/200\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.4317 - accuracy: 0.8012 - val_loss: 0.4465 - val_accuracy: 0.7933\n",
            "Epoch 170/200\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.4316 - accuracy: 0.8001 - val_loss: 0.4467 - val_accuracy: 0.7944\n",
            "Epoch 171/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.4317 - accuracy: 0.8010 - val_loss: 0.4462 - val_accuracy: 0.7944\n",
            "Epoch 172/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.4315 - accuracy: 0.8011 - val_loss: 0.4465 - val_accuracy: 0.7961\n",
            "Epoch 173/200\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.4314 - accuracy: 0.8004 - val_loss: 0.4465 - val_accuracy: 0.7944\n",
            "Epoch 174/200\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.4314 - accuracy: 0.8004 - val_loss: 0.4465 - val_accuracy: 0.7944\n",
            "Epoch 175/200\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.4315 - accuracy: 0.8009 - val_loss: 0.4465 - val_accuracy: 0.7939\n",
            "Epoch 176/200\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.4314 - accuracy: 0.8006 - val_loss: 0.4463 - val_accuracy: 0.7939\n",
            "Epoch 177/200\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.4314 - accuracy: 0.8004 - val_loss: 0.4465 - val_accuracy: 0.7939\n",
            "Epoch 178/200\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.4313 - accuracy: 0.8018 - val_loss: 0.4466 - val_accuracy: 0.7939\n",
            "Epoch 179/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.4312 - accuracy: 0.8001 - val_loss: 0.4465 - val_accuracy: 0.7933\n",
            "Epoch 180/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.4314 - accuracy: 0.8006 - val_loss: 0.4465 - val_accuracy: 0.7944\n",
            "Epoch 181/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.4311 - accuracy: 0.8004 - val_loss: 0.4468 - val_accuracy: 0.7933\n",
            "Epoch 182/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.4311 - accuracy: 0.8008 - val_loss: 0.4467 - val_accuracy: 0.7933\n",
            "Epoch 183/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.4310 - accuracy: 0.8006 - val_loss: 0.4470 - val_accuracy: 0.7944\n",
            "Epoch 184/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.4311 - accuracy: 0.8013 - val_loss: 0.4474 - val_accuracy: 0.7928\n",
            "Epoch 185/200\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.4309 - accuracy: 0.8012 - val_loss: 0.4471 - val_accuracy: 0.7939\n",
            "Epoch 186/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.4310 - accuracy: 0.8017 - val_loss: 0.4474 - val_accuracy: 0.7928\n",
            "Epoch 187/200\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.4309 - accuracy: 0.8006 - val_loss: 0.4472 - val_accuracy: 0.7939\n",
            "Epoch 188/200\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.4309 - accuracy: 0.8008 - val_loss: 0.4473 - val_accuracy: 0.7917\n",
            "Epoch 189/200\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.4307 - accuracy: 0.8010 - val_loss: 0.4471 - val_accuracy: 0.7917\n",
            "Epoch 190/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.4307 - accuracy: 0.8005 - val_loss: 0.4477 - val_accuracy: 0.7917\n",
            "Epoch 191/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.4305 - accuracy: 0.8011 - val_loss: 0.4474 - val_accuracy: 0.7939\n",
            "Epoch 192/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.4305 - accuracy: 0.8009 - val_loss: 0.4473 - val_accuracy: 0.7950\n",
            "Epoch 193/200\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.4305 - accuracy: 0.8009 - val_loss: 0.4473 - val_accuracy: 0.7939\n",
            "Epoch 194/200\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.4305 - accuracy: 0.8004 - val_loss: 0.4474 - val_accuracy: 0.7933\n",
            "Epoch 195/200\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.4303 - accuracy: 0.8009 - val_loss: 0.4475 - val_accuracy: 0.7911\n",
            "Epoch 196/200\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.4304 - accuracy: 0.8009 - val_loss: 0.4477 - val_accuracy: 0.7922\n",
            "Epoch 197/200\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.4303 - accuracy: 0.8014 - val_loss: 0.4475 - val_accuracy: 0.7933\n",
            "Epoch 198/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.4303 - accuracy: 0.8006 - val_loss: 0.4476 - val_accuracy: 0.7939\n",
            "Epoch 199/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.4303 - accuracy: 0.8006 - val_loss: 0.4476 - val_accuracy: 0.7944\n",
            "Epoch 200/200\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.4301 - accuracy: 0.8004 - val_loss: 0.4479 - val_accuracy: 0.7939\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nlSladpspALa",
        "outputId": "f466fad2-1200-46a7-fbfb-d8eff7081ee7"
      },
      "source": [
        "model_histories['Small'] = compile_and_fit(small_model, max_epochs = EPOCHS)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_27\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_125 (Dense)           (None, 16)                304       \n",
            "                                                                 \n",
            " dense_126 (Dense)           (None, 16)                272       \n",
            "                                                                 \n",
            " dense_127 (Dense)           (None, 1)                 17        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 593\n",
            "Trainable params: 593\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/200\n",
            "33/33 [==============================] - 1s 7ms/step - loss: 0.6642 - accuracy: 0.5872 - val_loss: 0.6042 - val_accuracy: 0.6750\n",
            "Epoch 2/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.5749 - accuracy: 0.7190 - val_loss: 0.5473 - val_accuracy: 0.7278\n",
            "Epoch 3/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.5290 - accuracy: 0.7477 - val_loss: 0.5149 - val_accuracy: 0.7383\n",
            "Epoch 4/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.5003 - accuracy: 0.7638 - val_loss: 0.4957 - val_accuracy: 0.7556\n",
            "Epoch 5/200\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.4838 - accuracy: 0.7746 - val_loss: 0.4857 - val_accuracy: 0.7606\n",
            "Epoch 6/200\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.4746 - accuracy: 0.7799 - val_loss: 0.4811 - val_accuracy: 0.7661\n",
            "Epoch 7/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.4693 - accuracy: 0.7827 - val_loss: 0.4779 - val_accuracy: 0.7678\n",
            "Epoch 8/200\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.4653 - accuracy: 0.7841 - val_loss: 0.4761 - val_accuracy: 0.7700\n",
            "Epoch 9/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.4622 - accuracy: 0.7860 - val_loss: 0.4740 - val_accuracy: 0.7739\n",
            "Epoch 10/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.4599 - accuracy: 0.7862 - val_loss: 0.4721 - val_accuracy: 0.7717\n",
            "Epoch 11/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.4575 - accuracy: 0.7877 - val_loss: 0.4710 - val_accuracy: 0.7744\n",
            "Epoch 12/200\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.4550 - accuracy: 0.7896 - val_loss: 0.4697 - val_accuracy: 0.7772\n",
            "Epoch 13/200\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.4531 - accuracy: 0.7896 - val_loss: 0.4690 - val_accuracy: 0.7789\n",
            "Epoch 14/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.4516 - accuracy: 0.7907 - val_loss: 0.4672 - val_accuracy: 0.7811\n",
            "Epoch 15/200\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.4498 - accuracy: 0.7905 - val_loss: 0.4663 - val_accuracy: 0.7833\n",
            "Epoch 16/200\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.4480 - accuracy: 0.7922 - val_loss: 0.4654 - val_accuracy: 0.7817\n",
            "Epoch 17/200\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.4470 - accuracy: 0.7928 - val_loss: 0.4643 - val_accuracy: 0.7822\n",
            "Epoch 18/200\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.4455 - accuracy: 0.7928 - val_loss: 0.4642 - val_accuracy: 0.7833\n",
            "Epoch 19/200\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.4445 - accuracy: 0.7944 - val_loss: 0.4627 - val_accuracy: 0.7878\n",
            "Epoch 20/200\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.4434 - accuracy: 0.7951 - val_loss: 0.4621 - val_accuracy: 0.7911\n",
            "Epoch 21/200\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.4427 - accuracy: 0.7960 - val_loss: 0.4617 - val_accuracy: 0.7894\n",
            "Epoch 22/200\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 0.4416 - accuracy: 0.7961 - val_loss: 0.4618 - val_accuracy: 0.7878\n",
            "Epoch 23/200\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.4412 - accuracy: 0.7960 - val_loss: 0.4604 - val_accuracy: 0.7878\n",
            "Epoch 24/200\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.4402 - accuracy: 0.7966 - val_loss: 0.4595 - val_accuracy: 0.7889\n",
            "Epoch 25/200\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.4392 - accuracy: 0.7967 - val_loss: 0.4598 - val_accuracy: 0.7872\n",
            "Epoch 26/200\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.4389 - accuracy: 0.7971 - val_loss: 0.4595 - val_accuracy: 0.7872\n",
            "Epoch 27/200\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 0.4378 - accuracy: 0.7985 - val_loss: 0.4582 - val_accuracy: 0.7878\n",
            "Epoch 28/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4373 - accuracy: 0.7980 - val_loss: 0.4579 - val_accuracy: 0.7894\n",
            "Epoch 29/200\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.4369 - accuracy: 0.7977 - val_loss: 0.4577 - val_accuracy: 0.7878\n",
            "Epoch 30/200\n",
            "33/33 [==============================] - 0s 9ms/step - loss: 0.4365 - accuracy: 0.7985 - val_loss: 0.4571 - val_accuracy: 0.7878\n",
            "Epoch 31/200\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.4358 - accuracy: 0.7992 - val_loss: 0.4574 - val_accuracy: 0.7889\n",
            "Epoch 32/200\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.4352 - accuracy: 0.7990 - val_loss: 0.4573 - val_accuracy: 0.7894\n",
            "Epoch 33/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4349 - accuracy: 0.7989 - val_loss: 0.4573 - val_accuracy: 0.7872\n",
            "Epoch 34/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4346 - accuracy: 0.8002 - val_loss: 0.4568 - val_accuracy: 0.7867\n",
            "Epoch 35/200\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.4340 - accuracy: 0.7993 - val_loss: 0.4575 - val_accuracy: 0.7889\n",
            "Epoch 36/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4344 - accuracy: 0.7990 - val_loss: 0.4570 - val_accuracy: 0.7900\n",
            "Epoch 37/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4333 - accuracy: 0.7990 - val_loss: 0.4569 - val_accuracy: 0.7889\n",
            "Epoch 38/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4328 - accuracy: 0.8010 - val_loss: 0.4567 - val_accuracy: 0.7894\n",
            "Epoch 39/200\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.4332 - accuracy: 0.7999 - val_loss: 0.4565 - val_accuracy: 0.7894\n",
            "Epoch 40/200\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.4321 - accuracy: 0.7996 - val_loss: 0.4565 - val_accuracy: 0.7900\n",
            "Epoch 41/200\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.4323 - accuracy: 0.8004 - val_loss: 0.4565 - val_accuracy: 0.7900\n",
            "Epoch 42/200\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.4317 - accuracy: 0.7991 - val_loss: 0.4561 - val_accuracy: 0.7878\n",
            "Epoch 43/200\n",
            "33/33 [==============================] - 0s 10ms/step - loss: 0.4313 - accuracy: 0.8010 - val_loss: 0.4564 - val_accuracy: 0.7889\n",
            "Epoch 44/200\n",
            "33/33 [==============================] - 0s 9ms/step - loss: 0.4310 - accuracy: 0.8012 - val_loss: 0.4561 - val_accuracy: 0.7883\n",
            "Epoch 45/200\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.4306 - accuracy: 0.8010 - val_loss: 0.4564 - val_accuracy: 0.7872\n",
            "Epoch 46/200\n",
            "33/33 [==============================] - 0s 10ms/step - loss: 0.4304 - accuracy: 0.8017 - val_loss: 0.4558 - val_accuracy: 0.7889\n",
            "Epoch 47/200\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.4304 - accuracy: 0.8000 - val_loss: 0.4559 - val_accuracy: 0.7900\n",
            "Epoch 48/200\n",
            "33/33 [==============================] - 0s 9ms/step - loss: 0.4299 - accuracy: 0.8023 - val_loss: 0.4560 - val_accuracy: 0.7889\n",
            "Epoch 49/200\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.4297 - accuracy: 0.8005 - val_loss: 0.4560 - val_accuracy: 0.7922\n",
            "Epoch 50/200\n",
            "33/33 [==============================] - 0s 9ms/step - loss: 0.4298 - accuracy: 0.8009 - val_loss: 0.4561 - val_accuracy: 0.7911\n",
            "Epoch 51/200\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 0.4293 - accuracy: 0.8016 - val_loss: 0.4558 - val_accuracy: 0.7889\n",
            "Epoch 52/200\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.4291 - accuracy: 0.8011 - val_loss: 0.4560 - val_accuracy: 0.7883\n",
            "Epoch 53/200\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.4288 - accuracy: 0.8030 - val_loss: 0.4564 - val_accuracy: 0.7878\n",
            "Epoch 54/200\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.4285 - accuracy: 0.8022 - val_loss: 0.4560 - val_accuracy: 0.7878\n",
            "Epoch 55/200\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.4284 - accuracy: 0.8020 - val_loss: 0.4562 - val_accuracy: 0.7889\n",
            "Epoch 56/200\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 0.4282 - accuracy: 0.8016 - val_loss: 0.4559 - val_accuracy: 0.7917\n",
            "Epoch 57/200\n",
            "33/33 [==============================] - 0s 9ms/step - loss: 0.4279 - accuracy: 0.8028 - val_loss: 0.4560 - val_accuracy: 0.7894\n",
            "Epoch 58/200\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.4279 - accuracy: 0.8020 - val_loss: 0.4559 - val_accuracy: 0.7883\n",
            "Epoch 59/200\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.4276 - accuracy: 0.8032 - val_loss: 0.4560 - val_accuracy: 0.7872\n",
            "Epoch 60/200\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.4275 - accuracy: 0.8030 - val_loss: 0.4559 - val_accuracy: 0.7878\n",
            "Epoch 61/200\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.4271 - accuracy: 0.8031 - val_loss: 0.4559 - val_accuracy: 0.7894\n",
            "Epoch 62/200\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.4270 - accuracy: 0.8020 - val_loss: 0.4561 - val_accuracy: 0.7900\n",
            "Epoch 63/200\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.4270 - accuracy: 0.8035 - val_loss: 0.4563 - val_accuracy: 0.7883\n",
            "Epoch 64/200\n",
            "33/33 [==============================] - 0s 9ms/step - loss: 0.4271 - accuracy: 0.8024 - val_loss: 0.4565 - val_accuracy: 0.7878\n",
            "Epoch 65/200\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.4268 - accuracy: 0.8031 - val_loss: 0.4562 - val_accuracy: 0.7900\n",
            "Epoch 66/200\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.4264 - accuracy: 0.8034 - val_loss: 0.4567 - val_accuracy: 0.7906\n",
            "Epoch 67/200\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.4265 - accuracy: 0.8030 - val_loss: 0.4566 - val_accuracy: 0.7900\n",
            "Epoch 68/200\n",
            "33/33 [==============================] - 0s 9ms/step - loss: 0.4262 - accuracy: 0.8031 - val_loss: 0.4565 - val_accuracy: 0.7894\n",
            "Epoch 69/200\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.4261 - accuracy: 0.8029 - val_loss: 0.4564 - val_accuracy: 0.7906\n",
            "Epoch 70/200\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 0.4263 - accuracy: 0.8036 - val_loss: 0.4566 - val_accuracy: 0.7878\n",
            "Epoch 71/200\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.4259 - accuracy: 0.8036 - val_loss: 0.4570 - val_accuracy: 0.7911\n",
            "Epoch 72/200\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.4257 - accuracy: 0.8030 - val_loss: 0.4572 - val_accuracy: 0.7894\n",
            "Epoch 73/200\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 0.4260 - accuracy: 0.8036 - val_loss: 0.4573 - val_accuracy: 0.7872\n",
            "Epoch 74/200\n",
            "33/33 [==============================] - 0s 10ms/step - loss: 0.4256 - accuracy: 0.8029 - val_loss: 0.4569 - val_accuracy: 0.7883\n",
            "Epoch 75/200\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.4255 - accuracy: 0.8041 - val_loss: 0.4570 - val_accuracy: 0.7872\n",
            "Epoch 76/200\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.4255 - accuracy: 0.8040 - val_loss: 0.4573 - val_accuracy: 0.7883\n",
            "Epoch 77/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.4253 - accuracy: 0.8041 - val_loss: 0.4565 - val_accuracy: 0.7906\n",
            "Epoch 78/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.4251 - accuracy: 0.8049 - val_loss: 0.4576 - val_accuracy: 0.7900\n",
            "Epoch 79/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.4252 - accuracy: 0.8044 - val_loss: 0.4575 - val_accuracy: 0.7900\n",
            "Epoch 80/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.4250 - accuracy: 0.8040 - val_loss: 0.4574 - val_accuracy: 0.7889\n",
            "Epoch 81/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.4248 - accuracy: 0.8039 - val_loss: 0.4578 - val_accuracy: 0.7900\n",
            "Epoch 82/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.4245 - accuracy: 0.8036 - val_loss: 0.4576 - val_accuracy: 0.7900\n",
            "Epoch 83/200\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.4248 - accuracy: 0.8050 - val_loss: 0.4577 - val_accuracy: 0.7906\n",
            "Epoch 84/200\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.4245 - accuracy: 0.8046 - val_loss: 0.4581 - val_accuracy: 0.7878\n",
            "Epoch 85/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.4246 - accuracy: 0.8031 - val_loss: 0.4578 - val_accuracy: 0.7889\n",
            "Epoch 86/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.4243 - accuracy: 0.8051 - val_loss: 0.4588 - val_accuracy: 0.7867\n",
            "Epoch 87/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.4248 - accuracy: 0.8036 - val_loss: 0.4579 - val_accuracy: 0.7878\n",
            "Epoch 88/200\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.4243 - accuracy: 0.8038 - val_loss: 0.4579 - val_accuracy: 0.7883\n",
            "Epoch 89/200\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.4240 - accuracy: 0.8043 - val_loss: 0.4578 - val_accuracy: 0.7906\n",
            "Epoch 90/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.4244 - accuracy: 0.8051 - val_loss: 0.4578 - val_accuracy: 0.7894\n",
            "Epoch 91/200\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.4238 - accuracy: 0.8043 - val_loss: 0.4581 - val_accuracy: 0.7900\n",
            "Epoch 92/200\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.4235 - accuracy: 0.8047 - val_loss: 0.4585 - val_accuracy: 0.7894\n",
            "Epoch 93/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.4237 - accuracy: 0.8056 - val_loss: 0.4583 - val_accuracy: 0.7889\n",
            "Epoch 94/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.4235 - accuracy: 0.8049 - val_loss: 0.4584 - val_accuracy: 0.7872\n",
            "Epoch 95/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.4233 - accuracy: 0.8047 - val_loss: 0.4586 - val_accuracy: 0.7878\n",
            "Epoch 96/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.4233 - accuracy: 0.8036 - val_loss: 0.4583 - val_accuracy: 0.7900\n",
            "Epoch 97/200\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.4234 - accuracy: 0.8047 - val_loss: 0.4588 - val_accuracy: 0.7883\n",
            "Epoch 98/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.4233 - accuracy: 0.8046 - val_loss: 0.4584 - val_accuracy: 0.7911\n",
            "Epoch 99/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.4232 - accuracy: 0.8049 - val_loss: 0.4590 - val_accuracy: 0.7900\n",
            "Epoch 100/200\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.4234 - accuracy: 0.8034 - val_loss: 0.4585 - val_accuracy: 0.7878\n",
            "Epoch 101/200\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.4229 - accuracy: 0.8041 - val_loss: 0.4583 - val_accuracy: 0.7889\n",
            "Epoch 102/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.4231 - accuracy: 0.8054 - val_loss: 0.4588 - val_accuracy: 0.7883\n",
            "Epoch 103/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.4232 - accuracy: 0.8052 - val_loss: 0.4588 - val_accuracy: 0.7878\n",
            "Epoch 104/200\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.4229 - accuracy: 0.8046 - val_loss: 0.4587 - val_accuracy: 0.7894\n",
            "Epoch 105/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.4227 - accuracy: 0.8049 - val_loss: 0.4587 - val_accuracy: 0.7900\n",
            "Epoch 106/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.4228 - accuracy: 0.8046 - val_loss: 0.4592 - val_accuracy: 0.7878\n",
            "Epoch 107/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.4227 - accuracy: 0.8051 - val_loss: 0.4583 - val_accuracy: 0.7900\n",
            "Epoch 108/200\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.4223 - accuracy: 0.8056 - val_loss: 0.4593 - val_accuracy: 0.7917\n",
            "Epoch 109/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.4231 - accuracy: 0.8059 - val_loss: 0.4590 - val_accuracy: 0.7894\n",
            "Epoch 110/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.4224 - accuracy: 0.8062 - val_loss: 0.4586 - val_accuracy: 0.7883\n",
            "Epoch 111/200\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.4222 - accuracy: 0.8062 - val_loss: 0.4597 - val_accuracy: 0.7911\n",
            "Epoch 112/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.4223 - accuracy: 0.8056 - val_loss: 0.4589 - val_accuracy: 0.7883\n",
            "Epoch 113/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.4222 - accuracy: 0.8059 - val_loss: 0.4591 - val_accuracy: 0.7906\n",
            "Epoch 114/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.4222 - accuracy: 0.8060 - val_loss: 0.4592 - val_accuracy: 0.7911\n",
            "Epoch 115/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.4218 - accuracy: 0.8058 - val_loss: 0.4594 - val_accuracy: 0.7883\n",
            "Epoch 116/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.4219 - accuracy: 0.8047 - val_loss: 0.4591 - val_accuracy: 0.7889\n",
            "Epoch 117/200\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.4218 - accuracy: 0.8063 - val_loss: 0.4588 - val_accuracy: 0.7878\n",
            "Epoch 118/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.4220 - accuracy: 0.8057 - val_loss: 0.4591 - val_accuracy: 0.7889\n",
            "Epoch 119/200\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.4220 - accuracy: 0.8067 - val_loss: 0.4590 - val_accuracy: 0.7900\n",
            "Epoch 120/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.4220 - accuracy: 0.8051 - val_loss: 0.4594 - val_accuracy: 0.7878\n",
            "Epoch 121/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.4218 - accuracy: 0.8050 - val_loss: 0.4590 - val_accuracy: 0.7889\n",
            "Epoch 122/200\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.4217 - accuracy: 0.8063 - val_loss: 0.4595 - val_accuracy: 0.7883\n",
            "Epoch 123/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.4220 - accuracy: 0.8068 - val_loss: 0.4597 - val_accuracy: 0.7883\n",
            "Epoch 124/200\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.4217 - accuracy: 0.8068 - val_loss: 0.4600 - val_accuracy: 0.7889\n",
            "Epoch 125/200\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.4221 - accuracy: 0.8060 - val_loss: 0.4592 - val_accuracy: 0.7872\n",
            "Epoch 126/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.4214 - accuracy: 0.8054 - val_loss: 0.4596 - val_accuracy: 0.7894\n",
            "Epoch 127/200\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.4214 - accuracy: 0.8062 - val_loss: 0.4595 - val_accuracy: 0.7867\n",
            "Epoch 128/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.4212 - accuracy: 0.8065 - val_loss: 0.4593 - val_accuracy: 0.7889\n",
            "Epoch 129/200\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.4217 - accuracy: 0.8062 - val_loss: 0.4601 - val_accuracy: 0.7889\n",
            "Epoch 130/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.4211 - accuracy: 0.8063 - val_loss: 0.4596 - val_accuracy: 0.7878\n",
            "Epoch 131/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.4214 - accuracy: 0.8058 - val_loss: 0.4594 - val_accuracy: 0.7883\n",
            "Epoch 132/200\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.4211 - accuracy: 0.8064 - val_loss: 0.4593 - val_accuracy: 0.7872\n",
            "Epoch 133/200\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.4212 - accuracy: 0.8071 - val_loss: 0.4597 - val_accuracy: 0.7883\n",
            "Epoch 134/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.4211 - accuracy: 0.8073 - val_loss: 0.4602 - val_accuracy: 0.7894\n",
            "Epoch 135/200\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.4207 - accuracy: 0.8060 - val_loss: 0.4595 - val_accuracy: 0.7906\n",
            "Epoch 136/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.4212 - accuracy: 0.8077 - val_loss: 0.4601 - val_accuracy: 0.7867\n",
            "Epoch 137/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.4212 - accuracy: 0.8071 - val_loss: 0.4598 - val_accuracy: 0.7883\n",
            "Epoch 138/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.4209 - accuracy: 0.8075 - val_loss: 0.4611 - val_accuracy: 0.7900\n",
            "Epoch 139/200\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.4210 - accuracy: 0.8069 - val_loss: 0.4599 - val_accuracy: 0.7883\n",
            "Epoch 140/200\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.4206 - accuracy: 0.8056 - val_loss: 0.4596 - val_accuracy: 0.7894\n",
            "Epoch 141/200\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.4206 - accuracy: 0.8065 - val_loss: 0.4607 - val_accuracy: 0.7894\n",
            "Epoch 142/200\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.4207 - accuracy: 0.8067 - val_loss: 0.4601 - val_accuracy: 0.7894\n",
            "Epoch 143/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.4206 - accuracy: 0.8064 - val_loss: 0.4597 - val_accuracy: 0.7883\n",
            "Epoch 144/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.4206 - accuracy: 0.8070 - val_loss: 0.4605 - val_accuracy: 0.7856\n",
            "Epoch 145/200\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.4205 - accuracy: 0.8065 - val_loss: 0.4605 - val_accuracy: 0.7883\n",
            "Epoch 146/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.4207 - accuracy: 0.8066 - val_loss: 0.4613 - val_accuracy: 0.7894\n",
            "Epoch 147/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.4209 - accuracy: 0.8059 - val_loss: 0.4608 - val_accuracy: 0.7906\n",
            "Epoch 148/200\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.4207 - accuracy: 0.8070 - val_loss: 0.4601 - val_accuracy: 0.7894\n",
            "Epoch 149/200\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.4200 - accuracy: 0.8059 - val_loss: 0.4603 - val_accuracy: 0.7889\n",
            "Epoch 150/200\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.4205 - accuracy: 0.8072 - val_loss: 0.4607 - val_accuracy: 0.7889\n",
            "Epoch 151/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.4201 - accuracy: 0.8071 - val_loss: 0.4606 - val_accuracy: 0.7889\n",
            "Epoch 152/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.4201 - accuracy: 0.8062 - val_loss: 0.4606 - val_accuracy: 0.7883\n",
            "Epoch 153/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.4199 - accuracy: 0.8064 - val_loss: 0.4609 - val_accuracy: 0.7889\n",
            "Epoch 154/200\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.4202 - accuracy: 0.8070 - val_loss: 0.4603 - val_accuracy: 0.7889\n",
            "Epoch 155/200\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.4201 - accuracy: 0.8061 - val_loss: 0.4610 - val_accuracy: 0.7883\n",
            "Epoch 156/200\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.4202 - accuracy: 0.8069 - val_loss: 0.4603 - val_accuracy: 0.7883\n",
            "Epoch 157/200\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.4202 - accuracy: 0.8067 - val_loss: 0.4605 - val_accuracy: 0.7867\n",
            "Epoch 158/200\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.4197 - accuracy: 0.8072 - val_loss: 0.4610 - val_accuracy: 0.7894\n",
            "Epoch 159/200\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.4201 - accuracy: 0.8060 - val_loss: 0.4605 - val_accuracy: 0.7889\n",
            "Epoch 160/200\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.4198 - accuracy: 0.8060 - val_loss: 0.4609 - val_accuracy: 0.7900\n",
            "Epoch 161/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.4196 - accuracy: 0.8077 - val_loss: 0.4608 - val_accuracy: 0.7900\n",
            "Epoch 162/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.4197 - accuracy: 0.8070 - val_loss: 0.4608 - val_accuracy: 0.7867\n",
            "Epoch 163/200\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.4198 - accuracy: 0.8064 - val_loss: 0.4610 - val_accuracy: 0.7883\n",
            "Epoch 164/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.4195 - accuracy: 0.8073 - val_loss: 0.4609 - val_accuracy: 0.7889\n",
            "Epoch 165/200\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.4197 - accuracy: 0.8067 - val_loss: 0.4610 - val_accuracy: 0.7872\n",
            "Epoch 166/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.4197 - accuracy: 0.8073 - val_loss: 0.4611 - val_accuracy: 0.7872\n",
            "Epoch 167/200\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.4196 - accuracy: 0.8069 - val_loss: 0.4612 - val_accuracy: 0.7883\n",
            "Epoch 168/200\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.4195 - accuracy: 0.8062 - val_loss: 0.4609 - val_accuracy: 0.7883\n",
            "Epoch 169/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.4194 - accuracy: 0.8069 - val_loss: 0.4610 - val_accuracy: 0.7883\n",
            "Epoch 170/200\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.4192 - accuracy: 0.8065 - val_loss: 0.4611 - val_accuracy: 0.7889\n",
            "Epoch 171/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.4194 - accuracy: 0.8069 - val_loss: 0.4608 - val_accuracy: 0.7889\n",
            "Epoch 172/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.4194 - accuracy: 0.8077 - val_loss: 0.4607 - val_accuracy: 0.7889\n",
            "Epoch 173/200\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.4198 - accuracy: 0.8053 - val_loss: 0.4614 - val_accuracy: 0.7917\n",
            "Epoch 174/200\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.4191 - accuracy: 0.8067 - val_loss: 0.4616 - val_accuracy: 0.7911\n",
            "Epoch 175/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.4191 - accuracy: 0.8072 - val_loss: 0.4612 - val_accuracy: 0.7872\n",
            "Epoch 176/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.4190 - accuracy: 0.8073 - val_loss: 0.4622 - val_accuracy: 0.7883\n",
            "Epoch 177/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.4190 - accuracy: 0.8077 - val_loss: 0.4613 - val_accuracy: 0.7906\n",
            "Epoch 178/200\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.4192 - accuracy: 0.8072 - val_loss: 0.4628 - val_accuracy: 0.7883\n",
            "Epoch 179/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.4189 - accuracy: 0.8081 - val_loss: 0.4616 - val_accuracy: 0.7889\n",
            "Epoch 180/200\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.4187 - accuracy: 0.8080 - val_loss: 0.4617 - val_accuracy: 0.7878\n",
            "Epoch 181/200\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.4187 - accuracy: 0.8077 - val_loss: 0.4616 - val_accuracy: 0.7906\n",
            "Epoch 182/200\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.4190 - accuracy: 0.8062 - val_loss: 0.4623 - val_accuracy: 0.7867\n",
            "Epoch 183/200\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.4188 - accuracy: 0.8075 - val_loss: 0.4612 - val_accuracy: 0.7906\n",
            "Epoch 184/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.4187 - accuracy: 0.8081 - val_loss: 0.4618 - val_accuracy: 0.7900\n",
            "Epoch 185/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.4188 - accuracy: 0.8070 - val_loss: 0.4612 - val_accuracy: 0.7911\n",
            "Epoch 186/200\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.4184 - accuracy: 0.8081 - val_loss: 0.4625 - val_accuracy: 0.7900\n",
            "Epoch 187/200\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.4186 - accuracy: 0.8087 - val_loss: 0.4619 - val_accuracy: 0.7883\n",
            "Epoch 188/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.4181 - accuracy: 0.8071 - val_loss: 0.4623 - val_accuracy: 0.7894\n",
            "Epoch 189/200\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.4188 - accuracy: 0.8068 - val_loss: 0.4619 - val_accuracy: 0.7872\n",
            "Epoch 190/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.4185 - accuracy: 0.8064 - val_loss: 0.4619 - val_accuracy: 0.7861\n",
            "Epoch 191/200\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.4192 - accuracy: 0.8069 - val_loss: 0.4619 - val_accuracy: 0.7878\n",
            "Epoch 192/200\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.4184 - accuracy: 0.8075 - val_loss: 0.4619 - val_accuracy: 0.7883\n",
            "Epoch 193/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.4181 - accuracy: 0.8072 - val_loss: 0.4623 - val_accuracy: 0.7889\n",
            "Epoch 194/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.4185 - accuracy: 0.8073 - val_loss: 0.4621 - val_accuracy: 0.7889\n",
            "Epoch 195/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.4182 - accuracy: 0.8075 - val_loss: 0.4620 - val_accuracy: 0.7894\n",
            "Epoch 196/200\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.4187 - accuracy: 0.8072 - val_loss: 0.4626 - val_accuracy: 0.7883\n",
            "Epoch 197/200\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.4183 - accuracy: 0.8077 - val_loss: 0.4629 - val_accuracy: 0.7900\n",
            "Epoch 198/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.4182 - accuracy: 0.8082 - val_loss: 0.4619 - val_accuracy: 0.7861\n",
            "Epoch 199/200\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.4182 - accuracy: 0.8085 - val_loss: 0.4625 - val_accuracy: 0.7906\n",
            "Epoch 200/200\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.4181 - accuracy: 0.8077 - val_loss: 0.4626 - val_accuracy: 0.7906\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nDv5iuM-pOgY",
        "outputId": "7eb6cac7-7225-47d2-80e3-9108322ea617"
      },
      "source": [
        "model_histories['Medium']  = compile_and_fit(medium_model, max_epochs = EPOCHS)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_28\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_128 (Dense)           (None, 64)                1216      \n",
            "                                                                 \n",
            " dense_129 (Dense)           (None, 64)                4160      \n",
            "                                                                 \n",
            " dense_130 (Dense)           (None, 64)                4160      \n",
            "                                                                 \n",
            " dense_131 (Dense)           (None, 1)                 65        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 9,601\n",
            "Trainable params: 9,601\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/200\n",
            "33/33 [==============================] - 1s 8ms/step - loss: 0.5669 - accuracy: 0.7113 - val_loss: 0.4963 - val_accuracy: 0.7522\n",
            "Epoch 2/200\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.4707 - accuracy: 0.7809 - val_loss: 0.4639 - val_accuracy: 0.7672\n",
            "Epoch 3/200\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.4532 - accuracy: 0.7912 - val_loss: 0.4576 - val_accuracy: 0.7789\n",
            "Epoch 4/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.4458 - accuracy: 0.7916 - val_loss: 0.4560 - val_accuracy: 0.7789\n",
            "Epoch 5/200\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.4414 - accuracy: 0.7959 - val_loss: 0.4553 - val_accuracy: 0.7806\n",
            "Epoch 6/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.4390 - accuracy: 0.7971 - val_loss: 0.4535 - val_accuracy: 0.7817\n",
            "Epoch 7/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.4362 - accuracy: 0.7975 - val_loss: 0.4524 - val_accuracy: 0.7822\n",
            "Epoch 8/200\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.4331 - accuracy: 0.8014 - val_loss: 0.4516 - val_accuracy: 0.7817\n",
            "Epoch 9/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.4316 - accuracy: 0.8010 - val_loss: 0.4553 - val_accuracy: 0.7750\n",
            "Epoch 10/200\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.4297 - accuracy: 0.8023 - val_loss: 0.4541 - val_accuracy: 0.7878\n",
            "Epoch 11/200\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.4281 - accuracy: 0.8015 - val_loss: 0.4530 - val_accuracy: 0.7806\n",
            "Epoch 12/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.4258 - accuracy: 0.8040 - val_loss: 0.4520 - val_accuracy: 0.7844\n",
            "Epoch 13/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.4251 - accuracy: 0.8044 - val_loss: 0.4502 - val_accuracy: 0.7867\n",
            "Epoch 14/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.4233 - accuracy: 0.8048 - val_loss: 0.4537 - val_accuracy: 0.7856\n",
            "Epoch 15/200\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.4220 - accuracy: 0.8051 - val_loss: 0.4552 - val_accuracy: 0.7850\n",
            "Epoch 16/200\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.4215 - accuracy: 0.8051 - val_loss: 0.4555 - val_accuracy: 0.7856\n",
            "Epoch 17/200\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.4198 - accuracy: 0.8061 - val_loss: 0.4536 - val_accuracy: 0.7861\n",
            "Epoch 18/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.4183 - accuracy: 0.8077 - val_loss: 0.4526 - val_accuracy: 0.7900\n",
            "Epoch 19/200\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.4166 - accuracy: 0.8061 - val_loss: 0.4562 - val_accuracy: 0.7922\n",
            "Epoch 20/200\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.4172 - accuracy: 0.8074 - val_loss: 0.4559 - val_accuracy: 0.7861\n",
            "Epoch 21/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.4165 - accuracy: 0.8074 - val_loss: 0.4534 - val_accuracy: 0.7872\n",
            "Epoch 22/200\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.4156 - accuracy: 0.8071 - val_loss: 0.4589 - val_accuracy: 0.7872\n",
            "Epoch 23/200\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.4132 - accuracy: 0.8073 - val_loss: 0.4546 - val_accuracy: 0.7917\n",
            "Epoch 24/200\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.4120 - accuracy: 0.8081 - val_loss: 0.4552 - val_accuracy: 0.7900\n",
            "Epoch 25/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.4102 - accuracy: 0.8126 - val_loss: 0.4604 - val_accuracy: 0.7878\n",
            "Epoch 26/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.4092 - accuracy: 0.8114 - val_loss: 0.4620 - val_accuracy: 0.7817\n",
            "Epoch 27/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.4097 - accuracy: 0.8110 - val_loss: 0.4567 - val_accuracy: 0.7844\n",
            "Epoch 28/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.4081 - accuracy: 0.8123 - val_loss: 0.4604 - val_accuracy: 0.7867\n",
            "Epoch 29/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.4066 - accuracy: 0.8127 - val_loss: 0.4574 - val_accuracy: 0.7861\n",
            "Epoch 30/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.4054 - accuracy: 0.8123 - val_loss: 0.4605 - val_accuracy: 0.7867\n",
            "Epoch 31/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.4040 - accuracy: 0.8138 - val_loss: 0.4626 - val_accuracy: 0.7922\n",
            "Epoch 32/200\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.4035 - accuracy: 0.8136 - val_loss: 0.4605 - val_accuracy: 0.7856\n",
            "Epoch 33/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.4027 - accuracy: 0.8155 - val_loss: 0.4646 - val_accuracy: 0.7844\n",
            "Epoch 34/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.4033 - accuracy: 0.8160 - val_loss: 0.4584 - val_accuracy: 0.7900\n",
            "Epoch 35/200\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.4011 - accuracy: 0.8146 - val_loss: 0.4610 - val_accuracy: 0.7861\n",
            "Epoch 36/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.4010 - accuracy: 0.8145 - val_loss: 0.4637 - val_accuracy: 0.7844\n",
            "Epoch 37/200\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.3981 - accuracy: 0.8170 - val_loss: 0.4619 - val_accuracy: 0.7844\n",
            "Epoch 38/200\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.3968 - accuracy: 0.8196 - val_loss: 0.4651 - val_accuracy: 0.7883\n",
            "Epoch 39/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.3972 - accuracy: 0.8176 - val_loss: 0.4653 - val_accuracy: 0.7867\n",
            "Epoch 40/200\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.3954 - accuracy: 0.8180 - val_loss: 0.4647 - val_accuracy: 0.7850\n",
            "Epoch 41/200\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.3945 - accuracy: 0.8191 - val_loss: 0.4642 - val_accuracy: 0.7856\n",
            "Epoch 42/200\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.3969 - accuracy: 0.8184 - val_loss: 0.4706 - val_accuracy: 0.7783\n",
            "Epoch 43/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.3950 - accuracy: 0.8197 - val_loss: 0.4720 - val_accuracy: 0.7822\n",
            "Epoch 44/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.3916 - accuracy: 0.8217 - val_loss: 0.4698 - val_accuracy: 0.7844\n",
            "Epoch 45/200\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.3933 - accuracy: 0.8203 - val_loss: 0.4681 - val_accuracy: 0.7894\n",
            "Epoch 46/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.3905 - accuracy: 0.8228 - val_loss: 0.4726 - val_accuracy: 0.7861\n",
            "Epoch 47/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.3895 - accuracy: 0.8229 - val_loss: 0.4679 - val_accuracy: 0.7872\n",
            "Epoch 48/200\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.3875 - accuracy: 0.8249 - val_loss: 0.4755 - val_accuracy: 0.7828\n",
            "Epoch 49/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.3878 - accuracy: 0.8236 - val_loss: 0.4715 - val_accuracy: 0.7789\n",
            "Epoch 50/200\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.3864 - accuracy: 0.8248 - val_loss: 0.4717 - val_accuracy: 0.7833\n",
            "Epoch 51/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.3855 - accuracy: 0.8246 - val_loss: 0.4731 - val_accuracy: 0.7806\n",
            "Epoch 52/200\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.3830 - accuracy: 0.8264 - val_loss: 0.4739 - val_accuracy: 0.7850\n",
            "Epoch 53/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.3835 - accuracy: 0.8254 - val_loss: 0.4783 - val_accuracy: 0.7833\n",
            "Epoch 54/200\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.3839 - accuracy: 0.8263 - val_loss: 0.4774 - val_accuracy: 0.7778\n",
            "Epoch 55/200\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.3813 - accuracy: 0.8272 - val_loss: 0.4772 - val_accuracy: 0.7800\n",
            "Epoch 56/200\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.3819 - accuracy: 0.8287 - val_loss: 0.4769 - val_accuracy: 0.7833\n",
            "Epoch 57/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.3786 - accuracy: 0.8293 - val_loss: 0.4847 - val_accuracy: 0.7778\n",
            "Epoch 58/200\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.3805 - accuracy: 0.8279 - val_loss: 0.4822 - val_accuracy: 0.7783\n",
            "Epoch 59/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.3777 - accuracy: 0.8290 - val_loss: 0.4815 - val_accuracy: 0.7750\n",
            "Epoch 60/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.3778 - accuracy: 0.8303 - val_loss: 0.4815 - val_accuracy: 0.7794\n",
            "Epoch 61/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.3751 - accuracy: 0.8311 - val_loss: 0.4823 - val_accuracy: 0.7783\n",
            "Epoch 62/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.3753 - accuracy: 0.8312 - val_loss: 0.4831 - val_accuracy: 0.7817\n",
            "Epoch 63/200\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.3731 - accuracy: 0.8339 - val_loss: 0.4855 - val_accuracy: 0.7772\n",
            "Epoch 64/200\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.3732 - accuracy: 0.8328 - val_loss: 0.4866 - val_accuracy: 0.7794\n",
            "Epoch 65/200\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.3707 - accuracy: 0.8338 - val_loss: 0.4875 - val_accuracy: 0.7800\n",
            "Epoch 66/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.3695 - accuracy: 0.8339 - val_loss: 0.4888 - val_accuracy: 0.7772\n",
            "Epoch 67/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.3695 - accuracy: 0.8343 - val_loss: 0.4869 - val_accuracy: 0.7811\n",
            "Epoch 68/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.3682 - accuracy: 0.8367 - val_loss: 0.4896 - val_accuracy: 0.7733\n",
            "Epoch 69/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.3704 - accuracy: 0.8319 - val_loss: 0.4976 - val_accuracy: 0.7811\n",
            "Epoch 70/200\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.3692 - accuracy: 0.8335 - val_loss: 0.4902 - val_accuracy: 0.7828\n",
            "Epoch 71/200\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.3678 - accuracy: 0.8346 - val_loss: 0.4937 - val_accuracy: 0.7756\n",
            "Epoch 72/200\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.3646 - accuracy: 0.8371 - val_loss: 0.4978 - val_accuracy: 0.7733\n",
            "Epoch 73/200\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.3686 - accuracy: 0.8348 - val_loss: 0.4931 - val_accuracy: 0.7772\n",
            "Epoch 74/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.3625 - accuracy: 0.8380 - val_loss: 0.4958 - val_accuracy: 0.7728\n",
            "Epoch 75/200\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.3625 - accuracy: 0.8361 - val_loss: 0.4981 - val_accuracy: 0.7744\n",
            "Epoch 76/200\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.3604 - accuracy: 0.8383 - val_loss: 0.4999 - val_accuracy: 0.7783\n",
            "Epoch 77/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.3595 - accuracy: 0.8412 - val_loss: 0.5016 - val_accuracy: 0.7761\n",
            "Epoch 78/200\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.3575 - accuracy: 0.8412 - val_loss: 0.5076 - val_accuracy: 0.7772\n",
            "Epoch 79/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.3584 - accuracy: 0.8389 - val_loss: 0.5019 - val_accuracy: 0.7733\n",
            "Epoch 80/200\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.3573 - accuracy: 0.8376 - val_loss: 0.5091 - val_accuracy: 0.7756\n",
            "Epoch 81/200\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.3589 - accuracy: 0.8400 - val_loss: 0.5038 - val_accuracy: 0.7844\n",
            "Epoch 82/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.3538 - accuracy: 0.8407 - val_loss: 0.5055 - val_accuracy: 0.7683\n",
            "Epoch 83/200\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.3544 - accuracy: 0.8421 - val_loss: 0.5059 - val_accuracy: 0.7761\n",
            "Epoch 84/200\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.3512 - accuracy: 0.8440 - val_loss: 0.5060 - val_accuracy: 0.7700\n",
            "Epoch 85/200\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.3506 - accuracy: 0.8433 - val_loss: 0.5127 - val_accuracy: 0.7672\n",
            "Epoch 86/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.3497 - accuracy: 0.8455 - val_loss: 0.5193 - val_accuracy: 0.7689\n",
            "Epoch 87/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.3514 - accuracy: 0.8451 - val_loss: 0.5150 - val_accuracy: 0.7672\n",
            "Epoch 88/200\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.3496 - accuracy: 0.8456 - val_loss: 0.5131 - val_accuracy: 0.7694\n",
            "Epoch 89/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.3477 - accuracy: 0.8449 - val_loss: 0.5218 - val_accuracy: 0.7672\n",
            "Epoch 90/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.3469 - accuracy: 0.8448 - val_loss: 0.5143 - val_accuracy: 0.7678\n",
            "Epoch 91/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.3462 - accuracy: 0.8468 - val_loss: 0.5184 - val_accuracy: 0.7644\n",
            "Epoch 92/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.3451 - accuracy: 0.8449 - val_loss: 0.5215 - val_accuracy: 0.7706\n",
            "Epoch 93/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.3443 - accuracy: 0.8472 - val_loss: 0.5216 - val_accuracy: 0.7756\n",
            "Epoch 94/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.3460 - accuracy: 0.8464 - val_loss: 0.5238 - val_accuracy: 0.7667\n",
            "Epoch 95/200\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.3483 - accuracy: 0.8454 - val_loss: 0.5402 - val_accuracy: 0.7706\n",
            "Epoch 96/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.3419 - accuracy: 0.8488 - val_loss: 0.5232 - val_accuracy: 0.7744\n",
            "Epoch 97/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.3397 - accuracy: 0.8501 - val_loss: 0.5242 - val_accuracy: 0.7650\n",
            "Epoch 98/200\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.3391 - accuracy: 0.8501 - val_loss: 0.5289 - val_accuracy: 0.7711\n",
            "Epoch 99/200\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.3410 - accuracy: 0.8502 - val_loss: 0.5249 - val_accuracy: 0.7661\n",
            "Epoch 100/200\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.3375 - accuracy: 0.8504 - val_loss: 0.5364 - val_accuracy: 0.7589\n",
            "Epoch 101/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.3364 - accuracy: 0.8504 - val_loss: 0.5338 - val_accuracy: 0.7556\n",
            "Epoch 102/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.3368 - accuracy: 0.8511 - val_loss: 0.5300 - val_accuracy: 0.7672\n",
            "Epoch 103/200\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.3340 - accuracy: 0.8539 - val_loss: 0.5370 - val_accuracy: 0.7639\n",
            "Epoch 104/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.3358 - accuracy: 0.8530 - val_loss: 0.5270 - val_accuracy: 0.7683\n",
            "Epoch 105/200\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.3329 - accuracy: 0.8520 - val_loss: 0.5385 - val_accuracy: 0.7667\n",
            "Epoch 106/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.3361 - accuracy: 0.8499 - val_loss: 0.5501 - val_accuracy: 0.7578\n",
            "Epoch 107/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.3347 - accuracy: 0.8530 - val_loss: 0.5359 - val_accuracy: 0.7661\n",
            "Epoch 108/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.3309 - accuracy: 0.8558 - val_loss: 0.5466 - val_accuracy: 0.7506\n",
            "Epoch 109/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.3323 - accuracy: 0.8520 - val_loss: 0.5456 - val_accuracy: 0.7644\n",
            "Epoch 110/200\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.3333 - accuracy: 0.8548 - val_loss: 0.5404 - val_accuracy: 0.7644\n",
            "Epoch 111/200\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.3304 - accuracy: 0.8528 - val_loss: 0.5441 - val_accuracy: 0.7700\n",
            "Epoch 112/200\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.3295 - accuracy: 0.8522 - val_loss: 0.5416 - val_accuracy: 0.7633\n",
            "Epoch 113/200\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.3284 - accuracy: 0.8557 - val_loss: 0.5390 - val_accuracy: 0.7678\n",
            "Epoch 114/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.3292 - accuracy: 0.8548 - val_loss: 0.5400 - val_accuracy: 0.7628\n",
            "Epoch 115/200\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.3256 - accuracy: 0.8548 - val_loss: 0.5488 - val_accuracy: 0.7728\n",
            "Epoch 116/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.3242 - accuracy: 0.8581 - val_loss: 0.5501 - val_accuracy: 0.7556\n",
            "Epoch 117/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.3229 - accuracy: 0.8568 - val_loss: 0.5482 - val_accuracy: 0.7583\n",
            "Epoch 118/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.3219 - accuracy: 0.8585 - val_loss: 0.5489 - val_accuracy: 0.7550\n",
            "Epoch 119/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.3225 - accuracy: 0.8609 - val_loss: 0.5505 - val_accuracy: 0.7644\n",
            "Epoch 120/200\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.3223 - accuracy: 0.8577 - val_loss: 0.5563 - val_accuracy: 0.7600\n",
            "Epoch 121/200\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.3228 - accuracy: 0.8570 - val_loss: 0.5552 - val_accuracy: 0.7633\n",
            "Epoch 122/200\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.3187 - accuracy: 0.8610 - val_loss: 0.5587 - val_accuracy: 0.7622\n",
            "Epoch 123/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.3187 - accuracy: 0.8613 - val_loss: 0.5551 - val_accuracy: 0.7667\n",
            "Epoch 124/200\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.3183 - accuracy: 0.8623 - val_loss: 0.5568 - val_accuracy: 0.7533\n",
            "Epoch 125/200\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.3196 - accuracy: 0.8593 - val_loss: 0.5624 - val_accuracy: 0.7689\n",
            "Epoch 126/200\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.3169 - accuracy: 0.8628 - val_loss: 0.5595 - val_accuracy: 0.7556\n",
            "Epoch 127/200\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.3137 - accuracy: 0.8639 - val_loss: 0.5590 - val_accuracy: 0.7561\n",
            "Epoch 128/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.3189 - accuracy: 0.8601 - val_loss: 0.5648 - val_accuracy: 0.7550\n",
            "Epoch 129/200\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.3132 - accuracy: 0.8628 - val_loss: 0.5609 - val_accuracy: 0.7667\n",
            "Epoch 130/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.3119 - accuracy: 0.8641 - val_loss: 0.5652 - val_accuracy: 0.7567\n",
            "Epoch 131/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.3159 - accuracy: 0.8608 - val_loss: 0.5677 - val_accuracy: 0.7611\n",
            "Epoch 132/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.3130 - accuracy: 0.8623 - val_loss: 0.5656 - val_accuracy: 0.7600\n",
            "Epoch 133/200\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.3093 - accuracy: 0.8666 - val_loss: 0.5668 - val_accuracy: 0.7561\n",
            "Epoch 134/200\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.3092 - accuracy: 0.8641 - val_loss: 0.5701 - val_accuracy: 0.7572\n",
            "Epoch 135/200\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.3099 - accuracy: 0.8641 - val_loss: 0.5698 - val_accuracy: 0.7539\n",
            "Epoch 136/200\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.3106 - accuracy: 0.8641 - val_loss: 0.5760 - val_accuracy: 0.7594\n",
            "Epoch 137/200\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.3127 - accuracy: 0.8630 - val_loss: 0.5789 - val_accuracy: 0.7428\n",
            "Epoch 138/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.3105 - accuracy: 0.8643 - val_loss: 0.5740 - val_accuracy: 0.7667\n",
            "Epoch 139/200\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.3038 - accuracy: 0.8694 - val_loss: 0.5765 - val_accuracy: 0.7578\n",
            "Epoch 140/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.3062 - accuracy: 0.8657 - val_loss: 0.5923 - val_accuracy: 0.7461\n",
            "Epoch 141/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.3125 - accuracy: 0.8638 - val_loss: 0.5812 - val_accuracy: 0.7450\n",
            "Epoch 142/200\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.3041 - accuracy: 0.8673 - val_loss: 0.5774 - val_accuracy: 0.7550\n",
            "Epoch 143/200\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.3024 - accuracy: 0.8697 - val_loss: 0.5789 - val_accuracy: 0.7628\n",
            "Epoch 144/200\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.3032 - accuracy: 0.8685 - val_loss: 0.5864 - val_accuracy: 0.7594\n",
            "Epoch 145/200\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.3033 - accuracy: 0.8707 - val_loss: 0.5822 - val_accuracy: 0.7567\n",
            "Epoch 146/200\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.3005 - accuracy: 0.8705 - val_loss: 0.5890 - val_accuracy: 0.7578\n",
            "Epoch 147/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.3038 - accuracy: 0.8697 - val_loss: 0.5918 - val_accuracy: 0.7572\n",
            "Epoch 148/200\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.3016 - accuracy: 0.8702 - val_loss: 0.5913 - val_accuracy: 0.7517\n",
            "Epoch 149/200\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.3009 - accuracy: 0.8689 - val_loss: 0.5916 - val_accuracy: 0.7522\n",
            "Epoch 150/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.3020 - accuracy: 0.8699 - val_loss: 0.5884 - val_accuracy: 0.7478\n",
            "Epoch 151/200\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.2972 - accuracy: 0.8729 - val_loss: 0.5881 - val_accuracy: 0.7489\n",
            "Epoch 152/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.2974 - accuracy: 0.8717 - val_loss: 0.5906 - val_accuracy: 0.7606\n",
            "Epoch 153/200\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.2990 - accuracy: 0.8707 - val_loss: 0.5962 - val_accuracy: 0.7517\n",
            "Epoch 154/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.2943 - accuracy: 0.8736 - val_loss: 0.5881 - val_accuracy: 0.7589\n",
            "Epoch 155/200\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.2962 - accuracy: 0.8709 - val_loss: 0.6002 - val_accuracy: 0.7506\n",
            "Epoch 156/200\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.2931 - accuracy: 0.8764 - val_loss: 0.6117 - val_accuracy: 0.7506\n",
            "Epoch 157/200\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.2926 - accuracy: 0.8750 - val_loss: 0.6017 - val_accuracy: 0.7506\n",
            "Epoch 158/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.2931 - accuracy: 0.8720 - val_loss: 0.6179 - val_accuracy: 0.7483\n",
            "Epoch 159/200\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.2975 - accuracy: 0.8694 - val_loss: 0.6093 - val_accuracy: 0.7500\n",
            "Epoch 160/200\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.2908 - accuracy: 0.8756 - val_loss: 0.5991 - val_accuracy: 0.7528\n",
            "Epoch 161/200\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.2927 - accuracy: 0.8744 - val_loss: 0.6006 - val_accuracy: 0.7406\n",
            "Epoch 162/200\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.2903 - accuracy: 0.8765 - val_loss: 0.6101 - val_accuracy: 0.7411\n",
            "Epoch 163/200\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.2899 - accuracy: 0.8741 - val_loss: 0.6040 - val_accuracy: 0.7467\n",
            "Epoch 164/200\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.2908 - accuracy: 0.8759 - val_loss: 0.6067 - val_accuracy: 0.7467\n",
            "Epoch 165/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.2908 - accuracy: 0.8726 - val_loss: 0.6147 - val_accuracy: 0.7400\n",
            "Epoch 166/200\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.2911 - accuracy: 0.8767 - val_loss: 0.6054 - val_accuracy: 0.7450\n",
            "Epoch 167/200\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.2872 - accuracy: 0.8760 - val_loss: 0.6069 - val_accuracy: 0.7539\n",
            "Epoch 168/200\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.2862 - accuracy: 0.8768 - val_loss: 0.6100 - val_accuracy: 0.7500\n",
            "Epoch 169/200\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.2863 - accuracy: 0.8775 - val_loss: 0.6195 - val_accuracy: 0.7483\n",
            "Epoch 170/200\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.2840 - accuracy: 0.8775 - val_loss: 0.6267 - val_accuracy: 0.7444\n",
            "Epoch 171/200\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.2869 - accuracy: 0.8754 - val_loss: 0.6156 - val_accuracy: 0.7494\n",
            "Epoch 172/200\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.2842 - accuracy: 0.8773 - val_loss: 0.6168 - val_accuracy: 0.7511\n",
            "Epoch 173/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.2809 - accuracy: 0.8799 - val_loss: 0.6236 - val_accuracy: 0.7511\n",
            "Epoch 174/200\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.2820 - accuracy: 0.8784 - val_loss: 0.6202 - val_accuracy: 0.7489\n",
            "Epoch 175/200\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.2814 - accuracy: 0.8796 - val_loss: 0.6217 - val_accuracy: 0.7539\n",
            "Epoch 176/200\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.2874 - accuracy: 0.8752 - val_loss: 0.6387 - val_accuracy: 0.7589\n",
            "Epoch 177/200\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.2886 - accuracy: 0.8751 - val_loss: 0.6270 - val_accuracy: 0.7544\n",
            "Epoch 178/200\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.2851 - accuracy: 0.8775 - val_loss: 0.6239 - val_accuracy: 0.7456\n",
            "Epoch 179/200\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.2793 - accuracy: 0.8810 - val_loss: 0.6248 - val_accuracy: 0.7567\n",
            "Epoch 180/200\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.2801 - accuracy: 0.8796 - val_loss: 0.6319 - val_accuracy: 0.7456\n",
            "Epoch 181/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.2753 - accuracy: 0.8822 - val_loss: 0.6296 - val_accuracy: 0.7417\n",
            "Epoch 182/200\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.2774 - accuracy: 0.8810 - val_loss: 0.6303 - val_accuracy: 0.7494\n",
            "Epoch 183/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.2757 - accuracy: 0.8829 - val_loss: 0.6294 - val_accuracy: 0.7411\n",
            "Epoch 184/200\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.2761 - accuracy: 0.8823 - val_loss: 0.6356 - val_accuracy: 0.7489\n",
            "Epoch 185/200\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.2783 - accuracy: 0.8803 - val_loss: 0.6382 - val_accuracy: 0.7433\n",
            "Epoch 186/200\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.2747 - accuracy: 0.8820 - val_loss: 0.6405 - val_accuracy: 0.7333\n",
            "Epoch 187/200\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.2783 - accuracy: 0.8806 - val_loss: 0.6362 - val_accuracy: 0.7500\n",
            "Epoch 188/200\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.2746 - accuracy: 0.8831 - val_loss: 0.6367 - val_accuracy: 0.7467\n",
            "Epoch 189/200\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.2707 - accuracy: 0.8873 - val_loss: 0.6584 - val_accuracy: 0.7478\n",
            "Epoch 190/200\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.2759 - accuracy: 0.8822 - val_loss: 0.6453 - val_accuracy: 0.7511\n",
            "Epoch 191/200\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.2734 - accuracy: 0.8816 - val_loss: 0.6419 - val_accuracy: 0.7528\n",
            "Epoch 192/200\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.2715 - accuracy: 0.8849 - val_loss: 0.6467 - val_accuracy: 0.7456\n",
            "Epoch 193/200\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.2747 - accuracy: 0.8809 - val_loss: 0.6392 - val_accuracy: 0.7411\n",
            "Epoch 194/200\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.2687 - accuracy: 0.8873 - val_loss: 0.6493 - val_accuracy: 0.7417\n",
            "Epoch 195/200\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.2700 - accuracy: 0.8861 - val_loss: 0.6520 - val_accuracy: 0.7372\n",
            "Epoch 196/200\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.2742 - accuracy: 0.8829 - val_loss: 0.6487 - val_accuracy: 0.7461\n",
            "Epoch 197/200\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.2724 - accuracy: 0.8838 - val_loss: 0.6491 - val_accuracy: 0.7494\n",
            "Epoch 198/200\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.2656 - accuracy: 0.8878 - val_loss: 0.6502 - val_accuracy: 0.7461\n",
            "Epoch 199/200\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.2659 - accuracy: 0.8864 - val_loss: 0.6589 - val_accuracy: 0.7439\n",
            "Epoch 200/200\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.2692 - accuracy: 0.8853 - val_loss: 0.6605 - val_accuracy: 0.7461\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mclCkooBpYnh",
        "outputId": "7167c7ae-1fb6-4367-fde4-bf5a1082e63a"
      },
      "source": [
        "model_histories['Large'] = compile_and_fit(large_model, max_epochs = EPOCHS)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_29\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_132 (Dense)           (None, 512)               9728      \n",
            "                                                                 \n",
            " dense_133 (Dense)           (None, 512)               262656    \n",
            "                                                                 \n",
            " dense_134 (Dense)           (None, 512)               262656    \n",
            "                                                                 \n",
            " dense_135 (Dense)           (None, 512)               262656    \n",
            "                                                                 \n",
            " dense_136 (Dense)           (None, 1)                 513       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 798,209\n",
            "Trainable params: 798,209\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/200\n",
            "33/33 [==============================] - 1s 9ms/step - loss: 0.4958 - accuracy: 0.7657 - val_loss: 0.4632 - val_accuracy: 0.7694\n",
            "Epoch 2/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4474 - accuracy: 0.7938 - val_loss: 0.4567 - val_accuracy: 0.7794\n",
            "Epoch 3/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4379 - accuracy: 0.7965 - val_loss: 0.4591 - val_accuracy: 0.7800\n",
            "Epoch 4/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4319 - accuracy: 0.8015 - val_loss: 0.4570 - val_accuracy: 0.7839\n",
            "Epoch 5/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4259 - accuracy: 0.8036 - val_loss: 0.4573 - val_accuracy: 0.7867\n",
            "Epoch 6/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4231 - accuracy: 0.8063 - val_loss: 0.4607 - val_accuracy: 0.7817\n",
            "Epoch 7/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4169 - accuracy: 0.8065 - val_loss: 0.4647 - val_accuracy: 0.7850\n",
            "Epoch 8/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4173 - accuracy: 0.8104 - val_loss: 0.4645 - val_accuracy: 0.7922\n",
            "Epoch 9/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4138 - accuracy: 0.8102 - val_loss: 0.4584 - val_accuracy: 0.7878\n",
            "Epoch 10/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4115 - accuracy: 0.8117 - val_loss: 0.4675 - val_accuracy: 0.7794\n",
            "Epoch 11/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4028 - accuracy: 0.8150 - val_loss: 0.4696 - val_accuracy: 0.7844\n",
            "Epoch 12/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.3976 - accuracy: 0.8180 - val_loss: 0.4799 - val_accuracy: 0.7833\n",
            "Epoch 13/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.3954 - accuracy: 0.8189 - val_loss: 0.4743 - val_accuracy: 0.7833\n",
            "Epoch 14/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.3868 - accuracy: 0.8235 - val_loss: 0.4836 - val_accuracy: 0.7783\n",
            "Epoch 15/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.3823 - accuracy: 0.8259 - val_loss: 0.4894 - val_accuracy: 0.7833\n",
            "Epoch 16/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.3759 - accuracy: 0.8286 - val_loss: 0.4998 - val_accuracy: 0.7689\n",
            "Epoch 17/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.3698 - accuracy: 0.8316 - val_loss: 0.4945 - val_accuracy: 0.7711\n",
            "Epoch 18/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.3607 - accuracy: 0.8365 - val_loss: 0.5091 - val_accuracy: 0.7683\n",
            "Epoch 19/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.3518 - accuracy: 0.8380 - val_loss: 0.5521 - val_accuracy: 0.7739\n",
            "Epoch 20/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.3416 - accuracy: 0.8485 - val_loss: 0.5522 - val_accuracy: 0.7694\n",
            "Epoch 21/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.3304 - accuracy: 0.8514 - val_loss: 0.6010 - val_accuracy: 0.7661\n",
            "Epoch 22/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.3185 - accuracy: 0.8606 - val_loss: 0.6154 - val_accuracy: 0.7567\n",
            "Epoch 23/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.3077 - accuracy: 0.8642 - val_loss: 0.6246 - val_accuracy: 0.7567\n",
            "Epoch 24/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.2986 - accuracy: 0.8674 - val_loss: 0.6161 - val_accuracy: 0.7489\n",
            "Epoch 25/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.2899 - accuracy: 0.8759 - val_loss: 0.6681 - val_accuracy: 0.7661\n",
            "Epoch 26/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.2751 - accuracy: 0.8842 - val_loss: 0.7699 - val_accuracy: 0.7572\n",
            "Epoch 27/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.2604 - accuracy: 0.8890 - val_loss: 0.7598 - val_accuracy: 0.7467\n",
            "Epoch 28/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.2467 - accuracy: 0.8952 - val_loss: 0.7991 - val_accuracy: 0.7550\n",
            "Epoch 29/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.2311 - accuracy: 0.9027 - val_loss: 0.8546 - val_accuracy: 0.7550\n",
            "Epoch 30/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.2299 - accuracy: 0.9035 - val_loss: 0.8615 - val_accuracy: 0.7478\n",
            "Epoch 31/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.2093 - accuracy: 0.9151 - val_loss: 0.9402 - val_accuracy: 0.7489\n",
            "Epoch 32/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.1953 - accuracy: 0.9198 - val_loss: 0.9896 - val_accuracy: 0.7550\n",
            "Epoch 33/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.1942 - accuracy: 0.9184 - val_loss: 0.9847 - val_accuracy: 0.7594\n",
            "Epoch 34/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.1824 - accuracy: 0.9244 - val_loss: 1.0379 - val_accuracy: 0.7422\n",
            "Epoch 35/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.1624 - accuracy: 0.9346 - val_loss: 1.1943 - val_accuracy: 0.7367\n",
            "Epoch 36/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.1488 - accuracy: 0.9408 - val_loss: 1.2072 - val_accuracy: 0.7461\n",
            "Epoch 37/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.1455 - accuracy: 0.9414 - val_loss: 1.2425 - val_accuracy: 0.7367\n",
            "Epoch 38/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.1294 - accuracy: 0.9470 - val_loss: 1.3401 - val_accuracy: 0.7372\n",
            "Epoch 39/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.1200 - accuracy: 0.9529 - val_loss: 1.3685 - val_accuracy: 0.7400\n",
            "Epoch 40/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.1074 - accuracy: 0.9585 - val_loss: 1.4348 - val_accuracy: 0.7256\n",
            "Epoch 41/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.1115 - accuracy: 0.9546 - val_loss: 1.4775 - val_accuracy: 0.7550\n",
            "Epoch 42/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.1105 - accuracy: 0.9573 - val_loss: 1.5557 - val_accuracy: 0.7283\n",
            "Epoch 43/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.1002 - accuracy: 0.9617 - val_loss: 1.4932 - val_accuracy: 0.7256\n",
            "Epoch 44/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.0849 - accuracy: 0.9686 - val_loss: 1.6652 - val_accuracy: 0.7283\n",
            "Epoch 45/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.0812 - accuracy: 0.9710 - val_loss: 1.6208 - val_accuracy: 0.7411\n",
            "Epoch 46/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.0680 - accuracy: 0.9751 - val_loss: 1.6740 - val_accuracy: 0.7311\n",
            "Epoch 47/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.0676 - accuracy: 0.9756 - val_loss: 1.8930 - val_accuracy: 0.7456\n",
            "Epoch 48/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.0588 - accuracy: 0.9786 - val_loss: 1.9278 - val_accuracy: 0.7406\n",
            "Epoch 49/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.0665 - accuracy: 0.9762 - val_loss: 1.9175 - val_accuracy: 0.7256\n",
            "Epoch 50/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.0685 - accuracy: 0.9746 - val_loss: 1.8068 - val_accuracy: 0.7306\n",
            "Epoch 51/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.0546 - accuracy: 0.9809 - val_loss: 1.9129 - val_accuracy: 0.7400\n",
            "Epoch 52/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.0564 - accuracy: 0.9802 - val_loss: 1.9076 - val_accuracy: 0.7267\n",
            "Epoch 53/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.0556 - accuracy: 0.9799 - val_loss: 1.9863 - val_accuracy: 0.7372\n",
            "Epoch 54/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.0614 - accuracy: 0.9785 - val_loss: 1.9400 - val_accuracy: 0.7417\n",
            "Epoch 55/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.0464 - accuracy: 0.9839 - val_loss: 2.1657 - val_accuracy: 0.7394\n",
            "Epoch 56/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.0402 - accuracy: 0.9860 - val_loss: 2.0599 - val_accuracy: 0.7333\n",
            "Epoch 57/200\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.0289 - accuracy: 0.9908 - val_loss: 2.1307 - val_accuracy: 0.7328\n",
            "Epoch 58/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.0252 - accuracy: 0.9928 - val_loss: 2.1836 - val_accuracy: 0.7311\n",
            "Epoch 59/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.0249 - accuracy: 0.9926 - val_loss: 2.1779 - val_accuracy: 0.7267\n",
            "Epoch 60/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.0237 - accuracy: 0.9919 - val_loss: 2.2978 - val_accuracy: 0.7317\n",
            "Epoch 61/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.0253 - accuracy: 0.9929 - val_loss: 2.3714 - val_accuracy: 0.7194\n",
            "Epoch 62/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.0428 - accuracy: 0.9860 - val_loss: 2.3319 - val_accuracy: 0.7389\n",
            "Epoch 63/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.0696 - accuracy: 0.9757 - val_loss: 2.0520 - val_accuracy: 0.7278\n",
            "Epoch 64/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.0495 - accuracy: 0.9824 - val_loss: 2.1032 - val_accuracy: 0.7300\n",
            "Epoch 65/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.0288 - accuracy: 0.9903 - val_loss: 2.1556 - val_accuracy: 0.7428\n",
            "Epoch 66/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.0199 - accuracy: 0.9941 - val_loss: 2.2782 - val_accuracy: 0.7228\n",
            "Epoch 67/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.0198 - accuracy: 0.9944 - val_loss: 2.3613 - val_accuracy: 0.7378\n",
            "Epoch 68/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.0280 - accuracy: 0.9910 - val_loss: 2.3262 - val_accuracy: 0.7344\n",
            "Epoch 69/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.0375 - accuracy: 0.9872 - val_loss: 2.1593 - val_accuracy: 0.7333\n",
            "Epoch 70/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.0323 - accuracy: 0.9895 - val_loss: 2.2178 - val_accuracy: 0.7450\n",
            "Epoch 71/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.0328 - accuracy: 0.9902 - val_loss: 2.2451 - val_accuracy: 0.7328\n",
            "Epoch 72/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.0288 - accuracy: 0.9914 - val_loss: 2.2444 - val_accuracy: 0.7378\n",
            "Epoch 73/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.0236 - accuracy: 0.9929 - val_loss: 2.3429 - val_accuracy: 0.7283\n",
            "Epoch 74/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.0156 - accuracy: 0.9952 - val_loss: 2.3495 - val_accuracy: 0.7261\n",
            "Epoch 75/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.0115 - accuracy: 0.9967 - val_loss: 2.5649 - val_accuracy: 0.7289\n",
            "Epoch 76/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.0117 - accuracy: 0.9960 - val_loss: 2.4014 - val_accuracy: 0.7189\n",
            "Epoch 77/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.0142 - accuracy: 0.9954 - val_loss: 2.4616 - val_accuracy: 0.7444\n",
            "Epoch 78/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.0152 - accuracy: 0.9961 - val_loss: 2.3605 - val_accuracy: 0.7289\n",
            "Epoch 79/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.0161 - accuracy: 0.9956 - val_loss: 2.4735 - val_accuracy: 0.7406\n",
            "Epoch 80/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.0137 - accuracy: 0.9961 - val_loss: 2.4930 - val_accuracy: 0.7189\n",
            "Epoch 81/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.0138 - accuracy: 0.9958 - val_loss: 2.5593 - val_accuracy: 0.7433\n",
            "Epoch 82/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.0096 - accuracy: 0.9972 - val_loss: 2.6174 - val_accuracy: 0.7422\n",
            "Epoch 83/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.0047 - accuracy: 0.9988 - val_loss: 2.6460 - val_accuracy: 0.7361\n",
            "Epoch 84/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.0044 - accuracy: 0.9993 - val_loss: 2.7418 - val_accuracy: 0.7478\n",
            "Epoch 85/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.0061 - accuracy: 0.9986 - val_loss: 2.7231 - val_accuracy: 0.7439\n",
            "Epoch 86/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.0167 - accuracy: 0.9944 - val_loss: 2.6815 - val_accuracy: 0.7394\n",
            "Epoch 87/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.0363 - accuracy: 0.9884 - val_loss: 2.3409 - val_accuracy: 0.7272\n",
            "Epoch 88/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.0591 - accuracy: 0.9799 - val_loss: 2.2267 - val_accuracy: 0.7228\n",
            "Epoch 89/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.0468 - accuracy: 0.9842 - val_loss: 2.1901 - val_accuracy: 0.7328\n",
            "Epoch 90/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.0283 - accuracy: 0.9917 - val_loss: 2.3605 - val_accuracy: 0.7411\n",
            "Epoch 91/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.0292 - accuracy: 0.9904 - val_loss: 2.2811 - val_accuracy: 0.7311\n",
            "Epoch 92/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.0221 - accuracy: 0.9931 - val_loss: 2.3965 - val_accuracy: 0.7372\n",
            "Epoch 93/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.0178 - accuracy: 0.9931 - val_loss: 2.4000 - val_accuracy: 0.7322\n",
            "Epoch 94/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.0226 - accuracy: 0.9927 - val_loss: 2.3777 - val_accuracy: 0.7278\n",
            "Epoch 95/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.0224 - accuracy: 0.9938 - val_loss: 2.4098 - val_accuracy: 0.7217\n",
            "Epoch 96/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.0186 - accuracy: 0.9943 - val_loss: 2.5040 - val_accuracy: 0.7444\n",
            "Epoch 97/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.0202 - accuracy: 0.9940 - val_loss: 2.4210 - val_accuracy: 0.7272\n",
            "Epoch 98/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.0254 - accuracy: 0.9933 - val_loss: 2.3595 - val_accuracy: 0.7356\n",
            "Epoch 99/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.0137 - accuracy: 0.9960 - val_loss: 2.4055 - val_accuracy: 0.7300\n",
            "Epoch 100/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.0064 - accuracy: 0.9980 - val_loss: 2.5955 - val_accuracy: 0.7350\n",
            "Epoch 101/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.0089 - accuracy: 0.9977 - val_loss: 2.6238 - val_accuracy: 0.7417\n",
            "Epoch 102/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.0046 - accuracy: 0.9983 - val_loss: 2.6342 - val_accuracy: 0.7361\n",
            "Epoch 103/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.0026 - accuracy: 0.9996 - val_loss: 2.6461 - val_accuracy: 0.7328\n",
            "Epoch 104/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.0023 - accuracy: 0.9994 - val_loss: 2.7910 - val_accuracy: 0.7400\n",
            "Epoch 105/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.0068 - accuracy: 0.9980 - val_loss: 2.7242 - val_accuracy: 0.7378\n",
            "Epoch 106/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.0062 - accuracy: 0.9982 - val_loss: 2.7411 - val_accuracy: 0.7339\n",
            "Epoch 107/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.0060 - accuracy: 0.9978 - val_loss: 2.6614 - val_accuracy: 0.7306\n",
            "Epoch 108/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.0213 - accuracy: 0.9930 - val_loss: 2.5845 - val_accuracy: 0.7378\n",
            "Epoch 109/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.0304 - accuracy: 0.9888 - val_loss: 2.5355 - val_accuracy: 0.7239\n",
            "Epoch 110/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.0499 - accuracy: 0.9823 - val_loss: 2.2585 - val_accuracy: 0.7472\n",
            "Epoch 111/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.0474 - accuracy: 0.9844 - val_loss: 2.1077 - val_accuracy: 0.7428\n",
            "Epoch 112/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.0353 - accuracy: 0.9884 - val_loss: 2.1428 - val_accuracy: 0.7483\n",
            "Epoch 113/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.0242 - accuracy: 0.9920 - val_loss: 2.2530 - val_accuracy: 0.7328\n",
            "Epoch 114/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.0156 - accuracy: 0.9956 - val_loss: 2.4946 - val_accuracy: 0.7422\n",
            "Epoch 115/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.0155 - accuracy: 0.9954 - val_loss: 2.5295 - val_accuracy: 0.7372\n",
            "Epoch 116/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.0115 - accuracy: 0.9963 - val_loss: 2.5067 - val_accuracy: 0.7361\n",
            "Epoch 117/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.0076 - accuracy: 0.9979 - val_loss: 2.6271 - val_accuracy: 0.7322\n",
            "Epoch 118/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.0057 - accuracy: 0.9983 - val_loss: 2.6472 - val_accuracy: 0.7400\n",
            "Epoch 119/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.0077 - accuracy: 0.9977 - val_loss: 2.6065 - val_accuracy: 0.7289\n",
            "Epoch 120/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.0133 - accuracy: 0.9972 - val_loss: 2.5690 - val_accuracy: 0.7239\n",
            "Epoch 121/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.0152 - accuracy: 0.9955 - val_loss: 2.4149 - val_accuracy: 0.7339\n",
            "Epoch 122/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.0120 - accuracy: 0.9964 - val_loss: 2.5912 - val_accuracy: 0.7361\n",
            "Epoch 123/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.0146 - accuracy: 0.9956 - val_loss: 2.4951 - val_accuracy: 0.7294\n",
            "Epoch 124/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.0229 - accuracy: 0.9936 - val_loss: 2.3216 - val_accuracy: 0.7122\n",
            "Epoch 125/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.0192 - accuracy: 0.9943 - val_loss: 2.3869 - val_accuracy: 0.7172\n",
            "Epoch 126/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.0125 - accuracy: 0.9957 - val_loss: 2.4307 - val_accuracy: 0.7289\n",
            "Epoch 127/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.0091 - accuracy: 0.9976 - val_loss: 2.5480 - val_accuracy: 0.7339\n",
            "Epoch 128/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.0069 - accuracy: 0.9981 - val_loss: 2.6094 - val_accuracy: 0.7372\n",
            "Epoch 129/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.0089 - accuracy: 0.9978 - val_loss: 2.5739 - val_accuracy: 0.7289\n",
            "Epoch 130/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.0055 - accuracy: 0.9981 - val_loss: 2.6777 - val_accuracy: 0.7344\n",
            "Epoch 131/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.0068 - accuracy: 0.9982 - val_loss: 2.6671 - val_accuracy: 0.7356\n",
            "Epoch 132/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.0241 - accuracy: 0.9923 - val_loss: 2.3793 - val_accuracy: 0.7322\n",
            "Epoch 133/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.0177 - accuracy: 0.9944 - val_loss: 2.5085 - val_accuracy: 0.7389\n",
            "Epoch 134/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.0170 - accuracy: 0.9951 - val_loss: 2.4043 - val_accuracy: 0.7261\n",
            "Epoch 135/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.0149 - accuracy: 0.9962 - val_loss: 2.4716 - val_accuracy: 0.7394\n",
            "Epoch 136/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.0153 - accuracy: 0.9959 - val_loss: 2.3619 - val_accuracy: 0.7389\n",
            "Epoch 137/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.0093 - accuracy: 0.9974 - val_loss: 2.5125 - val_accuracy: 0.7411\n",
            "Epoch 138/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.0259 - accuracy: 0.9925 - val_loss: 2.3172 - val_accuracy: 0.7217\n",
            "Epoch 139/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.0217 - accuracy: 0.9937 - val_loss: 2.3514 - val_accuracy: 0.7294\n",
            "Epoch 140/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.0166 - accuracy: 0.9939 - val_loss: 2.3142 - val_accuracy: 0.7289\n",
            "Epoch 141/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.0094 - accuracy: 0.9970 - val_loss: 2.5784 - val_accuracy: 0.7183\n",
            "Epoch 142/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.0129 - accuracy: 0.9962 - val_loss: 2.5760 - val_accuracy: 0.7233\n",
            "Epoch 143/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.0236 - accuracy: 0.9927 - val_loss: 2.4384 - val_accuracy: 0.7278\n",
            "Epoch 144/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.0205 - accuracy: 0.9928 - val_loss: 2.4012 - val_accuracy: 0.7306\n",
            "Epoch 145/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.0106 - accuracy: 0.9968 - val_loss: 2.4231 - val_accuracy: 0.7128\n",
            "Epoch 146/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.0064 - accuracy: 0.9981 - val_loss: 2.5802 - val_accuracy: 0.7328\n",
            "Epoch 147/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.0050 - accuracy: 0.9990 - val_loss: 2.6162 - val_accuracy: 0.7294\n",
            "Epoch 148/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.0020 - accuracy: 0.9994 - val_loss: 2.7042 - val_accuracy: 0.7339\n",
            "Epoch 149/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.0019 - accuracy: 0.9994 - val_loss: 2.7396 - val_accuracy: 0.7328\n",
            "Epoch 150/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.0016 - accuracy: 0.9994 - val_loss: 2.7518 - val_accuracy: 0.7311\n",
            "Epoch 151/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 7.5503e-04 - accuracy: 0.9999 - val_loss: 2.7942 - val_accuracy: 0.7367\n",
            "Epoch 152/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 5.7563e-04 - accuracy: 0.9999 - val_loss: 2.8766 - val_accuracy: 0.7411\n",
            "Epoch 153/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 9.3161e-04 - accuracy: 0.9998 - val_loss: 2.8317 - val_accuracy: 0.7317\n",
            "Epoch 154/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.0011 - accuracy: 0.9997 - val_loss: 2.9003 - val_accuracy: 0.7350\n",
            "Epoch 155/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.0033 - accuracy: 0.9990 - val_loss: 2.9460 - val_accuracy: 0.7261\n",
            "Epoch 156/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.0078 - accuracy: 0.9985 - val_loss: 2.8640 - val_accuracy: 0.7389\n",
            "Epoch 157/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.0126 - accuracy: 0.9970 - val_loss: 2.6197 - val_accuracy: 0.7350\n",
            "Epoch 158/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.0141 - accuracy: 0.9959 - val_loss: 2.5897 - val_accuracy: 0.7339\n",
            "Epoch 159/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.0157 - accuracy: 0.9951 - val_loss: 2.5348 - val_accuracy: 0.7494\n",
            "Epoch 160/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.0274 - accuracy: 0.9912 - val_loss: 2.3334 - val_accuracy: 0.7294\n",
            "Epoch 161/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.0254 - accuracy: 0.9922 - val_loss: 2.2736 - val_accuracy: 0.7317\n",
            "Epoch 162/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.0203 - accuracy: 0.9938 - val_loss: 2.4580 - val_accuracy: 0.7339\n",
            "Epoch 163/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.0224 - accuracy: 0.9917 - val_loss: 2.2743 - val_accuracy: 0.7444\n",
            "Epoch 164/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.0192 - accuracy: 0.9933 - val_loss: 2.3903 - val_accuracy: 0.7222\n",
            "Epoch 165/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.0181 - accuracy: 0.9945 - val_loss: 2.4392 - val_accuracy: 0.7356\n",
            "Epoch 166/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.0196 - accuracy: 0.9935 - val_loss: 2.4183 - val_accuracy: 0.7417\n",
            "Epoch 167/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.0274 - accuracy: 0.9919 - val_loss: 2.3480 - val_accuracy: 0.7328\n",
            "Epoch 168/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.0209 - accuracy: 0.9936 - val_loss: 2.3643 - val_accuracy: 0.7378\n",
            "Epoch 169/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.0159 - accuracy: 0.9953 - val_loss: 2.4938 - val_accuracy: 0.7406\n",
            "Epoch 170/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.0124 - accuracy: 0.9964 - val_loss: 2.4628 - val_accuracy: 0.7283\n",
            "Epoch 171/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.0072 - accuracy: 0.9978 - val_loss: 2.5658 - val_accuracy: 0.7383\n",
            "Epoch 172/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.0050 - accuracy: 0.9986 - val_loss: 2.7172 - val_accuracy: 0.7311\n",
            "Epoch 173/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.0181 - accuracy: 0.9951 - val_loss: 2.6487 - val_accuracy: 0.7439\n",
            "Epoch 174/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.0176 - accuracy: 0.9948 - val_loss: 2.3638 - val_accuracy: 0.7350\n",
            "Epoch 175/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.0114 - accuracy: 0.9967 - val_loss: 2.4545 - val_accuracy: 0.7356\n",
            "Epoch 176/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.0080 - accuracy: 0.9973 - val_loss: 2.5415 - val_accuracy: 0.7256\n",
            "Epoch 177/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.0050 - accuracy: 0.9988 - val_loss: 2.7332 - val_accuracy: 0.7283\n",
            "Epoch 178/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.0034 - accuracy: 0.9990 - val_loss: 2.8042 - val_accuracy: 0.7306\n",
            "Epoch 179/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.0033 - accuracy: 0.9993 - val_loss: 2.7528 - val_accuracy: 0.7300\n",
            "Epoch 180/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.0015 - accuracy: 0.9996 - val_loss: 2.7438 - val_accuracy: 0.7367\n",
            "Epoch 181/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.0012 - accuracy: 0.9997 - val_loss: 2.7701 - val_accuracy: 0.7344\n",
            "Epoch 182/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.0016 - accuracy: 0.9997 - val_loss: 2.8396 - val_accuracy: 0.7350\n",
            "Epoch 183/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 5.0673e-04 - accuracy: 0.9998 - val_loss: 2.8926 - val_accuracy: 0.7367\n",
            "Epoch 184/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 8.3166e-04 - accuracy: 0.9997 - val_loss: 2.8494 - val_accuracy: 0.7350\n",
            "Epoch 185/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 7.5850e-04 - accuracy: 0.9996 - val_loss: 2.8913 - val_accuracy: 0.7283\n",
            "Epoch 186/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 9.0463e-04 - accuracy: 0.9999 - val_loss: 2.9234 - val_accuracy: 0.7356\n",
            "Epoch 187/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.0010 - accuracy: 0.9999 - val_loss: 2.8827 - val_accuracy: 0.7300\n",
            "Epoch 188/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 2.0746e-04 - accuracy: 0.9999 - val_loss: 2.9253 - val_accuracy: 0.7339\n",
            "Epoch 189/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.1493e-04 - accuracy: 1.0000 - val_loss: 2.9460 - val_accuracy: 0.7378\n",
            "Epoch 190/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 5.9264e-05 - accuracy: 1.0000 - val_loss: 2.9728 - val_accuracy: 0.7378\n",
            "Epoch 191/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 5.1042e-05 - accuracy: 1.0000 - val_loss: 2.9934 - val_accuracy: 0.7372\n",
            "Epoch 192/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 4.5841e-05 - accuracy: 1.0000 - val_loss: 3.0122 - val_accuracy: 0.7378\n",
            "Epoch 193/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 4.1870e-05 - accuracy: 1.0000 - val_loss: 3.0288 - val_accuracy: 0.7372\n",
            "Epoch 194/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 3.8707e-05 - accuracy: 1.0000 - val_loss: 3.0438 - val_accuracy: 0.7361\n",
            "Epoch 195/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 3.5966e-05 - accuracy: 1.0000 - val_loss: 3.0592 - val_accuracy: 0.7367\n",
            "Epoch 196/200\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 3.3638e-05 - accuracy: 1.0000 - val_loss: 3.0733 - val_accuracy: 0.7367\n",
            "Epoch 197/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 3.1581e-05 - accuracy: 1.0000 - val_loss: 3.0870 - val_accuracy: 0.7356\n",
            "Epoch 198/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 2.9766e-05 - accuracy: 1.0000 - val_loss: 3.0990 - val_accuracy: 0.7356\n",
            "Epoch 199/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 2.8115e-05 - accuracy: 1.0000 - val_loss: 3.1121 - val_accuracy: 0.7356\n",
            "Epoch 200/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 2.6609e-05 - accuracy: 1.0000 - val_loss: 3.1240 - val_accuracy: 0.7361\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iT0kpwL072No"
      },
      "source": [
        "## Plot the training and validation losses\n",
        "\n",
        "Let's build a function to help us generate clean plots."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9DBrKlpg2duG"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "\n",
        "def plotter(history_dict):\n",
        "  \"\"\"\n",
        "  Plots loss, val_loss of multiple models on the same graph.\n",
        "\n",
        "  Input:\n",
        "   - history_dict: dictionary of model names (keys) and history objects (values)\n",
        "  \"\"\"\n",
        "  plt.figure(figsize=(8, 6))\n",
        "  plt.xscale('log')\n",
        "  #plt.xlim([5, max(plt.xlim())])\n",
        "  plt.ylim([0.1, 0.7])\n",
        "  plt.grid(True)\n",
        "  \n",
        "  for model_name in history_dict.keys(): # loop over the passed model names\n",
        "    history = history_dict[model_name] # get history variable from the dictionary\n",
        "    plt.plot(history.history['loss'],\n",
        "             linestyle='-', # solid line\n",
        "             label = model_name + ' Train')\n",
        "    plt.plot(history.history['val_loss'],\n",
        "             linestyle='--', # dashed line\n",
        "             label = model_name + ' Val')\n",
        "    plt.title('Model Loss')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "\n",
        "  plt.legend(loc='lower left')\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kSTbkvsZ-wB4"
      },
      "source": [
        "Now, let's compare the learning curves of our four models. What do you notice?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "id": "m8VUqCtC-0A5",
        "outputId": "6d572758-6f15-4c41-b403-b3f2cccd2d99"
      },
      "source": [
        "plotter(model_histories)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 576x432 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfEAAAGHCAYAAACgSWuhAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nOzdd1RU19rA4d+eGXrvoCAqCgpqRBTFgtixxRJ7ojFqNKaXm3ZTvntvejE9Ro0SS2LXJGrssQcrRhEbFkQsKL332d8fQ4gmaiwMQ9nPWiw5/Z1wMu/Z++wipJQoiqIoilLzaEwdgKIoiqIod0clcUVRFEWpoVQSVxRFUZQaSiVxRVEURamhVBJXFEVRlBpKJXFFURRFqaFUEleUOkwI0VAIIYUQutvYd7wQYldVxKUoyu1RSVxRagghxDkhRLEQwvUv638vT8QNTRPZnT0MKIpSeVQSV5SaJQEY/ceCEKIlYG26cBRFMSWVxBWlZlkAjLtm+WFg/rU7CCEchBDzhRApQohEIcTrQghN+TatEOJjIUSqEOIs0P8Gx84RQlwWQlwUQrwthNDeS8BCiHpCiFVCiHQhxGkhxKPXbAsVQhwQQmQLIa4IIT4pX28phPheCJEmhMgUQuwXQnjcSxyKUhupJK4oNcsewF4I0bw8uY4Cvv/LPl8CDkBjoCuGpP9I+bZHgQFAMNAWGPaXY+cCpUCT8n16A5PuMebFwAWgXvn13hVCdC/f9jnwuZTSHvADlpavf7j8M/gALsBjQME9xqEotY5K4opS8/xRGu8FHAcu/rHhmsT+qpQyR0p5DpgGjC3fZQTwmZQySUqZDrx3zbEeQD/gWSllnpTyKvBp+fnuihDCB+gEvCylLJRSHgJm82dtQgnQRAjhKqXMlVLuuWa9C9BESlkmpYyRUmbfbRyKUlupJK4oNc8CYAwwnr9UpQOugBmQeM26RKB++e/1gKS/bPuDb/mxl8ursDOBmYD7PcRaD0iXUubcJJ6JgD9worzKfED5+gXABmCxEOKSEOJDIYTZPcShKLWSSuKKUsNIKRMxNHDrB6z8y+ZUDKVY32vWNeDP0vplDFXU1277QxJQBLhKKR3Lf+yllEH3EO4lwFkIYXejeKSUp6SUozE8KHwALBdC2EgpS6SU/5VSBgIdMbwCGIeiKNdRSVxRaqaJQHcpZd61K6WUZRjeK78jhLATQvgCz/Pne/OlwNNCCG8hhBPwyjXHXgY2AtOEEPZCCI0Qwk8I0fUO4rIob5RmKYSwxJCso4H3yte1Ko/9ewAhxENCCDcppR7ILD+HXgjRTQjRsvz1QDaGBxP9HcShKHWCSuKKUgNJKc9IKQ/cZPNTQB5wFtgFLASiyrd9i6Ga+jBwkL+X5McB5sAxIANYDnjdQWi5GBqg/fHTHUOXuIYYSuU/Av8npdxcvn8kcFQIkYuhkdsoKWUB4Fl+7WwM7/23Y6hiVxTlGkJKaeoYFEVRFEW5C6okriiKoig1lFGTuBAiUghxsnyAh1dusP1TIcSh8p/48tawiqIoiqLcBqNVp5c3SInH0Jf1ArAfGC2lPHaT/Z8CgqWUE4wSkKIoiqLUMsYsiYcCp6WUZ6WUxRhGbRp0i/1HA4uMGI+iKIqi1CrGTOL1uX5QiQv8OcDDdcq7wTQCthgxHkVRFEWpVarLtIGjgOXlfVz/RggxGZgMYGVlFeLj43Oj3e6KXq9Ho6m8Z5nMIklmkcRaJ3CzFohKO7Oi3Fhl38M1WX5+PkIIrKysTB2KcpvU/fvP4uPjU6WUbjfaZswkfpHrR4by5poxnv9iFPDEzU4kpZwFzAJo27atPHDgZt1j79y2bduIiIiotPMBRO1K4H9rjtG+qSszx4ZgbV5dnpWU2sgY93BN9cUXX1CvXj2GDfvrvC5KdaXu338mhEi82TZjPv7sB5oKIRoJIcwxJOpVNwiuGeAE7DZiLFVqQudGfDSsFb+dTuWh2XvJyi8xdUiKUicUFRVhYWFh6jAUpcoYLYlLKUuBJzGMDnUcWCqlPCqE+J8Q4v5rdh0FLJa1bNSZ4W19mP5gG+IuZjNy1m6u5hSaOiRFqfVUElfqGqO+iJBSrpVS+ksp/aSU75Sve1NKueqaff4jpfxbH/LaILKFF1Hj23E+PZ8RM3aTlJ5v6pAUpdYqLS2ltLRUJXGlTlGtCYysc1NXFkxsT3peMcNn7Ob01Zx/PkhRlDsmhGDUqFEEBgaaOhRFqTIqiVeBEF8nlkwJo1QvGT5jN0cuZJk6JEWpdbRaLc2aNcPN7YaNeBWlVlJJvIo097Jn+WNhWJvrGP3tHvaeTTN1SIpSqxQUFBAfH09+vnptpdQdKolXoYauNqyY2hFPB0vGRe1jy4krpg5JUWqNK1eusHDhQpKTk00diqJUGZXEq5ingyVLp4Th72HH5Pkx/HzoZl3nFUW5E0VFRQCqYZtSp6gkbgLONuYsfLQ9Ib5OPLvkEN/vuWk/fkVRblNhoaEbp6WlpYkjUZSqo5K4idhZmjFvQijdAtx5/ac4pm87beqQFKVGUyVxpS6qeeOBFucZ/k3aD1vf+fv23m+BZ0s4ux1+++zv2/t9DC5+EL8R9n5D85wSaFEfXJsaN+4bsDTTMnNsCP9adpgP158ku6CUlyMDEEKNuK4od0olcaUuqnlJ/A/60j8T+nXry/7cXnSDPtl/bC8rhqIcnNOPwTedoPtrEPYkaLTGi/kGzLQaPh3RGjtLHTO2nyG7sIS3BrVAq1GJXFHuRMuWLfHy8kKnq7lfa4pyp2re3W5uY/jXNwwmbbr5fk16GH5upvkAaD6A/Rt+pGPGctj0Jlw8CCPmVW68t0GjEbw1qAX2lmZM33aG7IISPhnRGnOdetuhKLfL0dERR0dHU4ehKFWq5iXxSlZs4QQjv4e4FWDjalhZWgxCA9qq+88jhOClyGbYW5nx/roT5BaV8s2DIViZV23NgKLUVOfOnaO0tJQmTZqYOhRFqTKqqAcgBLQcBo0jDMvbP4A5veDq8SoP5bGufrw7pCXb41N4OGof2YVqBjRFuR3R0dFs3rzZ1GEoSpVSSfxGPFtAZiLMDIed06CstEovP6Z9A74YFczB8xmMnrWHtNyiKr2+otRE+fn5WFlZmToMRalSKonfSNAQeHwvBPSDX/8Hs3tAyskqDWHgffX49uG2nEnJZfjM3VzKLKjS6ytKTZOVlYWDg4Opw1CUKqWS+M3YuhkauQ2fC7lXoazqq7W7Bbgzf0J7UrKLGD5jNwmpN2iNrygKZWVl5OTkqCSu1Dkqif+ToCHwzGFDFTvA9g8hOa7KLh/ayJlFkztQUFLG8BnRHLuUXWXXVpSaIifH0J1UJXHF1KSUVXo9lcRvh87c8G9eKuybBbO6wrYPqqx03qK+A0unhGGm1TBy1m4OnEuvkusqSk1hZ2fH448/TkBAgKlDUeqYjORLFOblViyv/vQ9opctrLLrqyR+J2xcDe/Kg4bAtnfh225wObZKLt3E3ZZlj4XhamvBQ3P2sj0+pUquqyg1gVarxd3dHRsbG1OHotQxUc9MZuFrL1QsB/cZQOPgtlV2fZXE75SNCzwwG0b+ADlXYMFgKK6a+Yu9naxZOiWMRq62TJq3n7VHLlfJdRWluktISGDfvn1VXpWpKAAZl/+cjdInqBWeTfyr7Noqid+t5gPgib0wYj6YW4NeDynxRr+sm50Fiyd34D5vR55ceJCl+5OMfk1Fqe6OHj3K1q1b1bwDSpVz821E45BQAApzczm5exd5mRlVdn2VxO+FtTM07Gz4/dD3ML0DbHkbSo3br9vByoz5E0Pp1MSVl1bEMnvnWaNeT1Gqu6ysLOzt7U0dhlIHmVtZU1yQz7nDB1n0xr9Y89n7pJyruu9klcQrS7MB0GoE7PgIZkUYxmE3ImtzHbMfbku/lp68/ctxpm08qaoSlTorOztbtUxXqlxRfh4XTxxFo9GSfjGJ9EsXALCyr7p7USXxymLtDENmwJilUJABs3vC7ulGvaSFTsuXo9swsq0PX245zX9WHUWvV4lcqXvUQC+KKRQXGgbhCgjrQv3mLSrWu3g3qLIY6vwEKJXOvw88vgc2vAbuzY1+Oa1G8P4DLbG30vHtzgSyC0v5cFgrzLTq+UypG4qKiigsLFRJXKlypcXFAGyePR0p9bQfMpJWPfugMzevshhUEjcGK0cY/PWfy1veAX0JdH0FzCwr/XJCCP7drzkOVmZ8vDGenMJSvhoTjKWZmgFNqf0sLCx45ZVXTB2GUgf9kcSl1ANgbW+Pvat7lcagimvGJiXkpcCuTw0Tqlw4YJTLCCF4sntT/jcoiM3Hr/Dg7L1k5BUb5VqKUt1YWlpiaVn5D8iKciulxdc3Yra0tavyGFQSNzYhYOBn8NAKKM41THG66U0oKTTK5caFNeTrMW04cjGLB76J5nxa1fRhVxRTSUhIYNOmTRQXq4dWperoy8o4vHEdGu2fFdpVWY3+B5XEq0qTnvD4bggeC3u+gXTjdUHo38qLHya1Jz2/mCHTf+NQUqbRrqUoppaQkEB0dDRarXp9pBhPXmYGhzaurVg+Gb2Do9s3EzJgMAEdwwFDxWtVU0m8Klk6wP1fwFMx4BFoWBe7DEoqf5rRdg2dWTG1I9YWWkbN2s2mY1cq/RqKUh1kZWVhZ2enkrhiVFvnzuLXOdPJTjUMeZ0YdxiAguxsej36BGHDxtA0NKzK41JJ3BQcy7sfXDkKKyfBjM5wfk+lX8bPzZaVUzvh72HHlAUHWLD7XKVfQ1FMTfURV6qCq48vADaOjgBY2tgCcO5wDBqtlo7Dx6AxwYOkSuKm5BEE41ZBWTFERcL6Vyt9HPY/hmntFuDOGz8f5b11x1VfcqVWUaO1KVWhuLAArU6HVmcGQMS4STw+eyFj3/8cMwvTNapUSdzUGneFqbuh3UTYMx3m9qv0FyvW5jpmjg3hoQ4NmLn9LE8v/p3CkrJKvYaimIKUkqKiIlUSV4zuytnTlJWWsuytf7NhxudIKbGys8fawdGkcal+4tWBhS30nwaBgwyjvQlhmFCltNAwuUol0Gk1vDWoBd5O1ry/7gRXc4qYNTYER+uqb02pKJVFCMGLL75IWZl6KFWMS19+j52PM0w/Hbd1ExM+m4mTV31ThqVK4tVKo3BDIgeIiYJvwuDcrko7vRCCx7r68fmo1hw6n8mwGbtJSldd0JSaTzVqU4ytz2PPXLesNTPD1snFRNH8SSXx6sqtfMjWuf1h7YtQlFtppx7Uuj7zJ4ZyNbuQod9Ec+RCVqWdW1GqUmJiIitWrCAnJ8fUoSi1nIOHJ1ozM0L6D+KFJWt49vsfMasGAwypJF5dNewEU6Oh/VTY9y180xESoyvt9B0au7BiakfMtRpGztrN1hNXK+3cimIsx44dY+bMmWRmGsY+uHz5MkeOHEGjUV9lSuVLTUpk2VuvceK37exaNI+ykhJOH9hr6rCuo+786szcBvq+D4+sBY2u0ucpb+phx4+Pd6SRqw2T5h9g4d7zlXp+RalMRUVFrF69msuXL7NixQrKysrIyspCp9NhbV05bUcU5VoZyZc4H3eYX774iH0/LwfAwc29Wk37rJJ4TeDbEZ7YC37dDMu7p8PZbZVyand7S5ZOCaNLU1f+/eMRPtpwolrdoIryBwsLC8aMGUPPnj1JSkpi+/btFVOQCiFMHZ5SC+Wkpl633KxTV4a/8W61ut9U6/SaQmvom0hpERycByknIOQR6PU/sLy3PrI2Fjpmj2vL6z/F8fXWM1zMKODDYfdhrlPPeEr1kJ6ejrOzMz4+Pvj4+JCWlsaePXuwsbHB0dG0XXyU2iE/KxMpJTaOThXrctIMo7PZurgi9XraDx5uqvBuSn1L1zQ6C5i8DcKehJi5MD0MTv9676fVanhvaEte7BPAT4cu8XDUPrIKSu75vIpyr86fP8+XX35JbGxsxbq+ffvy6KOP4ujoiJubmwmjU2qLbyY/xIwpY69bl5eZAUBuWioj3nwP1wYNTRDZrakkXhOZWUGfd2DiRsPvi0ZB9uV7Pq0Qgie6NeGTEfex/1w6w2dEcymz8sd1V5TbVVpayo8//oiDgwP+/v4V683NzXFzc+Phhx+mX79+JoxQqY0KcnM4snXjdaVyqJ6vGVV1ek3mEwqP7YSkfWDvZViXHAeeLe7ptEPbeONhb8ljC2IYMv03osa3I6ieGhFLqXqxsbFkZGTw4IMPqvnClSqzZ/kiDq5bxZQZ82nZvTe56ek41/M2dVg3pEriNZ2ZlWHoVoDTm2FGJ/j5CchPv6fTdmriyrKpYWiEYMSM3WyPT6mEYBXl9un1enbt2oWnpydNmjQxdThKLbZl7kwAQsvfeWt0OrRmZtg4OuFcz5sGLVqZMrxbUkm8NvHtDJ2fg0ML4au2EDPPMHzrXWrmac+Pj3fCx9maCXP3s3R/UiUGqyi3lpKSQl5eHl26dKlWrYGV2uf3davR6nSEDRtD1LNTOLB6JVa2dkQ9O5nYX9ebOrxbUkm8NjGzhJ7/gSk7wNUfVj9teF9+DzwdLFn2WBgd/Vx4aUUsn2yKV13QlCrh4eHBc889R/PmzU0dilLLOXp4YWFjy+cPDSHj8kUAcjPSyUy+jKWtnYmjuzX1Trw28mwJj6yD2KWgKR9TWl8GhVlg7XzHp7OzNCNqfDv+vfIIX/x6ikuZBbw3tCVmWvUMqBhHXl4e1tbW6j24UiUyr9y8YXCDoPuqMJI7Z9RvYSFEpBDipBDitBDilZvsM0IIcUwIcVQIsdCY8dQpQsB9I6HlMMPygSj4so3hX/2dz/hkptXw4bBWPNuzKctjLjBh7n5yClUXNMU4lixZwqJFi0wdhlIHlJWW/m1do+C2AAT3HYilrW1Vh3RHjJbEhRBa4GugLxAIjBZCBP5ln6bAq0AnKWUQ8Kyx4qnzfDuBexCseQ6+7Q4XDtzxKYQQPNvTnw+HtWL3mTSGz9hNclahEYJV6rLExETOnz+Pn5+fqUNR6oCSwr9/h7UfMpIXlqyh+/gpJojozhizJB4KnJZSnpVSFgOLgUF/2edR4GspZQaAlFLNwmEsHoEwfg08MAdykmF2D9j+4V2dakRbH6LGtyMpPZ8h03/jRHJ2JQer1GV79+7F2tqa4OBgU4ei1AGWtrY8/PHXFctBXXtSP6DmtMMQxmqkJIQYBkRKKSeVL48F2kspn7xmn5+AeKAToAX+I6X8W1NAIcRkYDKAh4dHyOLFiystztzcXGyreXVJZdOW5uObuIR05zZkOt2HpqwYvUYL4s7mZE7MLuPTmCKKyiRPBVsS6KLmdDaF2nYP7927F1tbW4KCgkwdilIFqtP9e+XwAcxt7XDyCzB1KNfp1q1bjJSy7Y22mbphmw5oCkQA3sAOIURLKWXmtTtJKWcBswDatm0rIyIiKi2Abdu2UZnnqzn60eCPXze8Bgnbod80aND+js7SK7yAR77bzycxuXzwQCseCKmeAyLUZrXpHi4rK2PHjh2EhITUms+k3Jop71+9voy1X3xMdspV+j/zEg418J4zZnX6RcDnmmXv8nXXugCsklKWSCkTMJTKmxoxJuVGvNtBXhpE9YafHofc2x/YpZ6jFUsfCyO0kTMvLDvMl7+eUl3QlHsyatQoWrWqvoNrKLVH3NZNnNy9k8unT6LR1cyaRGMm8f1AUyFEIyGEOTAKWPWXfX7CUApHCOEK+ANnjRiTciNBg+HJ/dDpWUO3tC9D4MQvt324g5UZcx8JZWhwfaZtimfivANcyVYN3pQ7p9Vq8ff3x93d3dShKLWYvszQQyfzSnLFOivbe5sN0lSMlsSllKXAk8AG4DiwVEp5VAjxPyHE/eW7bQDShBDHgK3Ai1LKNGPFpNyChS30+i88vttQpe5SXiFS9vfuFzdirtMwbcR9vDkgkOgzqfT6ZDvLYy6oUrlyRy5fvsypU6fQ38NIg4pyK+fjDvPpmEFMnzSGy/EnKtbrzM1NGNXdM2o/cSnlWimlv5TST0r5Tvm6N6WUq8p/l1LK56WUgVLKllLKymuxptwd16bw4DJwK58xavkjsHIK5Fz5x0OFEEzo3Ih1z4QT4GnHv5YdZuK8A6obmnLbYmJiWLFihRpmVak0GZcvMv3RBzm1NxqAI1s2AlBaUlKRuO3dPEwW371SQ24pN6fXG5L60ZWGsdh3T4eyfx7gpZGrDUsmh/1ZKv90O8sOJKlSufKP0tLScHFxUUlcqTQFOTkUZGexec50SgoLOR93mOadI3h63jIe+Pf/aNIuDPMaPDKgSuLKzWk00ONNeHyPYdrTDa/CzHC4evw2DjWUytc/E05zT3teXB7LhLn7ValcuaU/kriiVJai/DwAykpL+OLhYeRnZV43rWiXMePp/8xLpgrvnqkkrvwzFz94cDmM/AG0ZmBT3ujoNkrWDV1tWDy5A/83MJDdZ9Po9el2lqpSuXIDxcXFZGdnqySuVKqivNzyf/Mq1gX3vb/id+d69XH18a3yuCqLSuLK7RECmg+AydvBxsUw/vq8gfDbF/9Yxa7RCB7pVF4q97LnpeWxPDJ3P5ezCqooeKUmSE9PB1BJXKlUhdckb4BmnbpiYW1tomgqn0riyp35411lUQ6Y28CmN+CbTnB2+z8e2tDVhsWPduA/AwPZezad3p/sYOl+VSpXDFxdXXnsscfUmOlKpdGXlWHv5nbduuDIASaKxjhUElfujpUjjFkCo5dAWRHMvx+WjYf89FseptEIxndqxPpnu9C8nj0vrYhl/HeqVK6ATqfD09MTKysrU4ei1BLfv/IM+35azoPvfEKrnpH0fuxpXLxrbtX5jZh62FWlpguIhMYR8NvncHy1oXQOhv7l2pvfXr4uhlL5gj2JvL/uBL0/2cEbAwIZ3tZbtUyuo44cOQJAy5YtTRyJUhsU5eeTcv4cAB6Nm+DZxN+0ARlJjUviJ9NPEr44vNLOpy3TcjbuLCP8R2BrXj0G4a9xzCwh4mUI/xdotFCcD9M7QPOB0PEpsPO84WEajeDhjg2JCHDjpeWxvLQiljVHLvP+0JbUc1Slsbpm79696HQ6lcSVSmFhbU238ZPZOncWy999k+Gvv23qkIyixiVxewt7ejfsXWnn+z3xdz6N+ZTZsbMZ2WwkDzZ/EFcr10o7f52iKR97uDgPGnSAPdNh37cQ8jB0egYcbjw5iq+LDYse7cD3exN5b+0J+ny6g9cHNGdEWx9VKq9D0tLSCAwMNHUYSi3SuE0oW+fOIrhP7XoPfq0al8S9bLx4vcPrlXa+bYXbcGvpRtSRKOYcmcP8o/MZ0nQI44PG422nZuS6K7ZuMHQWdH0Zdn0KB6LgwHeG/uauTW54iEYjGBfWkAh/d15cfpiXVxxhTexl3n+gFfVVqbzWy8/Pp6CgQLVMVypFSVEhy99+g0bBbXl+0SqEpvY2/6q9n+wOBLkEMS1iGqsGr2Kg30BWnFrBgB8H8PKOlzmZftLU4dVcLn4w6Ct4+nfo/rphGeDIckg9fcNDGrhYs+jRDvxvUBAxiRn0+XQHi/edVy3Ya7m0NMOUCSqJK5Vh30/LuBR/nIxLF2p1AgeVxK/T0KEh/+n4H9YPXc/YwLFsS9rGsNXDeHzz48RciTF1eDWXYwPo/Kyhe1pJAaz9F3zdDpZPvOHob3+Uytc/E06L+va8svII46L2kZCad4OTK7VBRkYGoJK4cu+klMRt3YTWzIzwhyaYOhyjU0n8BjxsPHih7QtsHLaRp4KfIi41jvHrxzNu3Ti2J21HL9UMS3fNzAqe2AdhT8LJdYYGcEvGQtqZv+3awMWahZM68FZ5qbznJ9t5deURNXRrLdSqVSteeuklnJ2dTR2KUkNdij9OYV4uS//7KrkZ6fSc+Dg2jk6mDsvoatw78arkYOHA5FaTGRs4lh9P/ci8o/N4csuTNHFswoQWE4hsFImZxszUYdY8tu7Q+y3D/OV7psP+b0FfPuVpaTHo/pwSUKMRjA1rSJ8Wnny95TQL951n5cELjO/YkMe6+uFkUzOnD1T+zroWjaKlVJ2y0hJKCotY9MaLAPSY+DhNQ8MIiuhp4siqhiqJ3wYrnRVjmo9hzdA1vNv5XQD+vevfDFg5gIXHF1JQqgYquSs2LtDjDXjhJLgFGNatnAQLhsL5Pdft6m5nyX8HtWDLCxH0b+nFrJ1nCf9wK1/+eoq8otub81ypvjZt2kRsbKypw1BqoKhnH2Puvx6vWDazsKBNv0F1pmeLSuJ3wExjZmj4dv8Kvur+Fe7W7ry37z0iV0QyK3YWWUVZpg6xZjIrb30uJdQPgcuHIaoPzB0ACTuum2jFx9maT0a2Zv0z4XTwc2Hapni6frSVub8lUFRaZqIPoNwLvV7Pvn37uHTpkqlDUWoQvb6Movx8slOukJeRTsiAIQD4BNWtcQZUEr8LGqGhq09XFvRbwLzIebRwbcGXv39J7+W9mXZgGlfzr5o6xJpJCEN/8mdjoc+7kBpvmGRl37d/2zXA045vx7Vl5eMdaeJuy39WH6PHtO2siLlAmV61ZK9JcnJyKCkpUY3alDuyccYXrP3yIwA6PDCarg9NYMqM+di7ups4sqql3onfozYebWjj0YaT6SeJioti/rH5/HD8B+73u5/xQeNp6NDQ1CHWPOY2EPYEtJ0Ivy+AwMGG9We3QUo83DcSLB0AaNPAiUWPdmDnqVQ+3HCCF5YdZuaOM7zQO4DegR51pkqtJlPdy5Q7IaXkly8+4mT0jop19/WMRAiBrVPdaxipSuKVJMA5gA/CP2DNkDUMbTqUNWfXcP9P9/P8tuc5mnbU1OHVTGaWEPqoYfAYgONrYN2LMK05rHraUO0OCCEI93dj1ROd+XpMG0rLJFMWxDBkejTRZ1JN+AGU26GSuHInctJSKhL4/f96jfCHJmDrXHfvHVUSr2Q+dj683uF1HrvvMRYeX8jiE4vZlLiJMK8wJracSKhnqCod3q3+H0PrMXBgDsQuhYPzoOVweGA2YGjJ3r+VF32CPFgec4HPfz3FmG/30qWpKy/1aUZLbwcTfwAFICEhgYSEBAC6d+9OcXEx1tbW2NnZmTgypSa4csYwUFTfJ56nabswE0djeiqJG4mrlStPt3maR1o8wrL4ZSw4trVbDEcAACAASURBVIBJGyfRwqUFE1tOpHuD7miEqgi5Y/XbGH56vw2HF4NVefVZSQFs/wCCx6Jz8WNUaAMGB9fn+z2JfL31NAO/2kW/lp483aMpzTztTfsZ6rjVq1eTnp6Oubk53bt3p1OnTnTs2FE93Cq3lJ16lbzMDBw8PAnpPxj/Dp1NHVK1oJK4kdmZ2zGhxQQebP4gq86s4ru473hu23M0tG/IhBYTGNB4AGZa1df8jlk5QYepfy5fOADRXxrGam8cAe0mYenfl0ldGjOynQ/f7kxgzs6zrD2STLi/G492aUTnJq4qcVSxoqIi0tPT6datG127dq1Yr/4Oyq3kZWbw7ROG0dcmfTmHiHGTTBxR9VHjkrg+NxeAvOhoLr3+94lQ6k+bhnVwMDmbN5P87rt/2+7zzQwsA/zJWr2GlE8/xUVKruzbj31kHyxbtjTal4mF1oLh/sMZ2mQomxI3MSduDm9Gv8lXh77i4cCHGeY/DGszNdjFXWvUBZ47CgcXQMxcWPIQ2HnB5G3Y2XnyfC9/JnRqyA97zzM3+hxj5+yjmacdj3ZpzMD76mGuU7UiVeHKlSsAeHreeHpapW7bMONzriSc4b6efWnUOgR7N0NL8/Nxhyv2sbRVU0Zfq8YlcaEzhKx1csKmfYe/bdc6GN576lxdb7hdY2Nj2O7ujnVoKLnx8aQvWEB6VBRm9erR6MeVFecwBq1GS2SjSPo07EP0pWjmxM3howMfMTN2JmOaj2FMszE4Wdb+oQKNws4Tur4InZ+DUxvhzBaw9TBsOxCFo1NDnugawaQujfj50CVm7zzLC8sO8+GGE4zv2IgxoQ1wsFa1Isak1+vx8fFRSVy5obitmwDYPPtrAKZ8Mw8bRycyky+DEDwZtQRzKzWr4bVETZsdqm3btvLAgQOVdr5t27bRJTiYnK1bKYw7iufrrwFUlOLtIyOxat3aqDPhHE45TNSRKLYkbcFKZ8UDTR9gXOA4vGy9jHbNOkVfBl+0hszz4NQQgsdC6weRdp5sj09h9s4Edp1Oxdpcy4i2Pkzs3Agf55pTK7Jt2zYiIiJMHYai3JU/7l8pJZ+MGlixftCLb7Duq48JjryfnNSrJB2PY/LX35kwUtMRQsRIKdveaFuNK4kbg9bBAcfBg2Hw4Ip1ZekZ5GzYQMb8Bejc3bHr3RuHQYOwatmi0q9/n9t9fN79c85kniEqLorFJxaz+MRi+jXux4QWE/Bz9Kv0a9YpGi08sR+Or4KD82HLW7D1HUT/T4ho+wgRAe4cvZTFnJ0JfL8nkfm7z9G3hReTujQiuIGqFalMer0eTS2fGlK5OwU52dctp19MoriggINrf2bQv14noFO4iSKr3tT/TTdR/+OPaLo7mnoffYRlq5ZkLl1KziZDVY8sKSFv3z5kWeUO8+nn6Mc7nd9h7dC1jGo2ik2Jmxj882Ce3vI0h1MO//MJlJszs4RWI2D8GnjqoGFkOJ/2hm1J+wg6/jmf9LJn58vdeDS8MTtOpTBkejTDZ0Sz4WiyGgWuEpSVlfHhhx8SHR1t6lCUaignNeW65dSkRMMvQpB8Jp7Gwe1MEFX1p0rit6C1tcVh4AAcBg6gLDcPWVIMQN6ePSQ9Ohmtqyt2vXpi3ycS67YhFe/r75WXrRcvh77M5FaTWXRiET8c/4GtSVtp59mOiS0m0rGe6o5zT1z8oOd//lxO2mdo1b5zGl6Nwnk1eBxPvdiXJb+nELUrgSkLYvBxtuLB9r6MaOuDs5o57a6kpaVRWFiITXm7FKXuid/7G3bOrng1DbhufW56GhmXL6I1M6OspASATiMeIuyBUexftYL6zQJNEW6NoN6J38X7RH1+Prk7dpC9fgO527cjCwrQOjvTcPEizBs0qLTY/pBfks/y+OXMOzaPq/lXaebcjIktJtLLtxdajbbSr1cnZV+CQz8YWrdnJoKrPzyxj1K9ZP3RZObvTmRfQjrmOg0DWnkxLqwhrX0cTR01UHPeicfGxrJy5UqmTp2Kh4eHqcNRqtil+BMseuNfeDRuykPvfUpJYSHzXnoSp6Bgzm1Zh4WNDU/MWUzahfPYubhiYW142NPry5B6ibaSCkk1kXonXsk01tbYR0ZiHxmJvqCA3B07ydu1CzNvbwBSvvyK0qtXsOsTiU37UITZvbV4tjazZlzQOEY3G82as2uIiovixR0v4mPnw/ig8QxqMggLrUVlfLS6y74ehL8InV+AczsgNwWEQCckA469yIA23TkV2Zt5v2fy48GLrDx4kVbeDozt4MvA++phaaYepv5JcnIyWq0WV1dXU4eimEBOmmEI5KJ8Qzfh9EsXyLqSjM7xfMU+QghcfXyvO06j0aoXv7eg/tPcI42VFfZ9euP11v8qWrDrc3PI/mUtSZMmcapzFy699hp5e/be87XMtGYMaTqEnwf/zGcRn+Fg7sBbe94ickUkUXFR5Bbn3vM16jyNxjBYTKvhhuWcy5B+Fn55nqbzQ3i7ZBr7R0reGhhAfnEZLy6PpcN7v/Lu2uOcT8s3ZeTVXnJyMu7u7mi16oGnLsrPygD+HNgnO9Uw26O1m6FWxsHdk50L55oktppMlcSNwOPVV3F7/nnydu0ie/0GctZvAAk2HdojpSQvOhqbdu0Q5nf3blUjNPTw7UH3Bt3Zn7yf2Udm82nMp8yOnc3IZiN5sPmDuFqp0k6lcKgPU6Ph8iE4tAiOLMX66I+MfXAFDz3Xgz2nr7Jg3wXm7Erg251nifB3Y2yYL1393dFqVLuFawUEBKiW6XVYXmYmYOihAJCdYkjiBemGCXAyky9xyVL1Ab9TKokbicbCArsePbDr0QN9cTH6nBwAik6cIGniJDR2dth1745dZB9sOnVCcxcJXQhBqFcooV6hHE07StSRKOYcmcOCYwsY7j+cCS0m4GbtVtkfre4RAuoFG356vw2nN0HjCIQQhCXOICx3C9m9h/NDfijf/Z7NhLkHqO9oxYi2Poxo542Xg/piAmjfvr2pQ1BM6I+SeEF2NvtXr2TH91GYW1lhXj4CW3FBATlpKbc6hXID6rG4CmjMzdGVT7No7ueH94xvsOvZk5ytW7kw9XFOdexEQWzsPV0jyCWIaRHTWDV4FX0b9WXRiUX0XdmXD/Z9QEq++h+j0ujMoVl/0JY//7r6g9Rjv+01ph7ox57G37G0ey6N3Wz4dHM8nd7fwsS5+9l07AqlZXrTxm5CeXl55OXlmToMxYSc63nTKLgtY96exo7vowAIjrwfr5AwJn5hmInQ0lbNZHenVOt0E7bslcXF5O3dS87GTXi8+goaa2vS5y+gIDYWuz69se3SBY2l5V2dOyk7iVlHZrH6zGp0Gh0jAkYwocUEVc1uLMlxcHgRxC6Bhl1g+HecT8tn27YNfH3chiu5JXjYWxhK5219KnVEuJrQOn3Hjh1s2bKFV199FQsL1QizLvujlXqzTl3p+8Tz7Ni5k65du7Lvp2UEdAzH0UMNyftXqnV6NSXMzbHt0gXbLl0q1ukLCgzv0tesQVhbYxfRFft+/bDr2fOOzu1j78Nbnd5icsvJzIydycLjC1l2chkjAkbwSItHVDKvbJ4twPMdQ//zAsO7vwb6C4w7Mp6xDj4kNOnHnKy2fLW1iK+2nqZLUzdGt/OhZ6AHZtraXyGWnJyMs7OzSuB1VEFuDiUFBQithp8/fhuAHhOmoilv5CiEoP2QEaYMscaq/d8eNYzrlMk03bWTBlFzcBgwgLzde8hYurRie150NPr8228F7WPvw9ud32bV4FX0btib749/T98Vffl4/8ekFqQa4yPUbVozsC1vh+BQH4bMRLj60/jkt7xz+VFO1n+H/7YXnLqSw9QfDhL23hbeX3eCc6m1u6o5OTlZTXpSh+RnZzFt5ABO79/D8Z1bmTH5IRa8+ixIaNK2Aw+9/7majaySqOr0al4VKUtLKcvMROfqSsmVq5yOiEBYWGAbHo5dn97YRURUzMx2OxKzE5kVO4s1Z9dgrjFnVLNRjA8aj4uVixE/hULuVYhbCcd+gjFLKDO35+iWH/j9xBk+vRRIpt6ajn4ujAptQJ8gDyx0t98Nq7rfw0VFRbz33nt0796d8HA1/nVtdmzHFrRm5ujMzfjpw7dw8qpHQW4uGo2GXo8+SZN2f59Zsrrfv9WBqk6vwYROh658cAydqwsN5s0lZ/0GsjdtJGfjRoSFBfU//wy72/yfwNfel3c6v8PkVpOZeXgm84/NZ8nJJYwMGKmSuTHZukOHxww/gBZolbGZVqk/Ms7KnLOOnYhKaceLiwL5P2sbHmjjzajQBjRxr/mlFTWHeN0gpWT791FIKQnuMwCAjMuXABjzzjS8mgTc6nDlLqnq9BpEaLXYhIbi+eYbNN22Dd/vF+A4YgSWzQ3jCmetXk3S40+QtWoVZeVd2m7G196Xd7u8y8+DfqZHgx7MPzafviv78smBT0gvTK+Kj6MM+w4e3YJoOxG/wqO8U/whuxt9R5ifC3Ojz9Hzk22MmLGbZQeSyC0qNXW0d83Z2ZlBgwZRv359U4eiGFHahfPkZ2VSkJ3Fiegd2Dg54+rji2cTf5XAjUiVxGsoodVi3bYt1m3/rGHRFxZSeOwYuVu2IMzNsenSxTA87ID+N50wpaFDQ97r8p6hZB47k3nH5rH45OKKanZnS+eq+kh1jxBQP8Tw0/ttSNiOs9ac6Y1CSL1yCfPverA2LZTvVoTy5s+NiWzhxdA29eno51qjBpKxtbUlODjY1GEoRqYzt6BNv0Ec37WNoK49aHf/A6SeP0dxQYGpQ6vV1DvxWvY+Rur1FBw+TM769WSv34DWxZnGK1cCkH/wIJYBAbd8h3426ywzD89kXcI6LHWWjG42mvFB43GyVPNqV6m0M7DhNeTpTQh9KVcsGrKkqANzCyMws3djcHB9Hmjjjb+HXbW/h0+ePImrqysuLupVTV2Ql5mBjePtf19U9/u3OrjVO3GVxGvxDST1ekpTUzFzd0efn098x04A2Hbtin3fSGzDw9FY37i/8tnMs8yIncH6hPVY6iwZ02wMDwc9rJJ5VctLg2M/QuwyZNJetkVu4oeTcPZkHOl6K7zr16eVXSHPDwvH1bb6dd8qKyvj3XffJTQ0lD59+pg6HKWSSCkpKylBVz7SpNTrST5zCvdGfnc821ht/g6uLLdK4uqdeC0mNBrM3N0Nv1ta0mD2tzgOHUp+TAwXn32O+E6dyVrzyw2PbezYmA/DP+SnQT8R4R1BVFwUkSsi+fzg52QWZlblx6jbbFyg3SSYuAHxwgm6dWjH7IfbsS5wPQetnuDN7P+RG7+V7u+uYeLc/fwSe5nCkjJTR10hMTGRsrIyvLy8TB2KUgn0ZWVcij/BsR1b+HzsULJTDaNBpl44z8LXX+DEb9tNHGHdo96J1xFCo6l4h+7x2r/JPxBD9vp1WDRtChj6n2cuX4F9v77YdOmCpnxQjsaOjfmw64dMuW8KMw7PYM6ROSw8vpAHmz/IuMBxOFpWjzm16wS7P1t3W/R8Hdz9CY1bQWjJPkqEBUvP9+GJE6Ows9QxoFU9HmhTnxBfp5u2hzA2vV7Phg0bsLe3p1mzZiaJQalcB9b8yO7li3BwN8w8JjSC83GHObZzKwDezVuYMrw6yahJXAgRCXyOoUfNbCnl+3/ZPh74CLhYvuorKeVsY8aklLdybx+KTfvQinUlV66St3s32WvXorGxwbZ7d+z79sW2azhCq8XP0Y+Pun7ElFZTmBE7g9lHZrPwxMKKanYHCwcTfqI6yLOl4afnf/l91QyCzRMY7dQIX9f2rDpwhrBDrzL7QAivOIQxIKQxQ4O9aeBSeUO93o6YmBiuXLnC8OHDMb/LGfuU6uV83GHsXN0oKsjHv0NnNn/7NWcP7gfAzbdRRXJXqo7RkrgQQgt8DfQCLgD7hRCrpJTH/rLrEinlk8aKQ7k9jkMG4zBwgGEs9/Xrydm4ify9e2myzfCEXXT6NOYNGtDEqQkfd/2YU61OMePwDL498i0LT/xZMlfJvIppNGQ5BkLE42iAzkBnu2Rk0nHuz99JYeEMNmxvw1tbOpDn05WBIY3p19ILByszo4eWlZVFo0aNCAwMNPq1FOMrys8j6egR/EJCObUvGp/BrTCzsKBBi9Y0bd/xjhqzKZXHmCXxUOC0lPIsgBBiMTAI+GsSV6oJodNh26kTtp064fnmmxQnJSE0GmRZGecfmYC+qAi7nj2x7xtJkw4dmBYxjfiMeGYensms2FkV1eyjm41Wg8aYkmcLxAsnIXEXlnErGXBsNYMKo3kix5NXV+bx6ao9dGjuy+CQhnRp6nbXY7eXlZUhhLjpHOE9e/as2Eep2aSU7P1pGfqyUkqKiwDwCWyJi7ePiSNTjJnE6wNJ1yxfAG40ofADQohwIB54TkqZdIN9lComzMywaNy4Ytnr7bfIXruOnI0byVq5Eq2DA+4vv4z/0CEVyXzG4RnMjJ3JnLg59GrQi+EBw2nr0VZ9iZuCVgeNI6BxBNr+0yDxN75qGM6US9kU//QM/qc2seF4CM+bd8GjdSSDQ3wJqmd/23+r/Px85s2bR1FREVOnTr1uYpOUlBQKCwvx8fFBq7394WOV6ivxyCH2/7ycoK496f3YU6RfSFIJvJowWhczIcQwIFJKOal8eSzQ/tqqcyGEC5ArpSwSQkwBRkopu9/gXJOByQAeHh4hixcvrrQ4c3NzsVUD8d++khLMjx/HMiaGgrAwSpo1Q3fhAlbbtlMU0obzjRz4LX83e/P2UqAvwNPMk062nQi1DcVaU7XvZOuKO72HHTNicU/egnPKXiz1+WRIW5aUdeN7yzF0rK8jzEuHk+XNS+elpaUcPnyY3NxcfH19adiwIQDFxcWYmZlVbAsLC1NJvAaRej0lebmY29lzOWY3mefO0GzIGENtnJRkJZ7FwbdxpT+Uq+/gf9atW7eq7ycuhAgD/iOl7FO+/CqAlPK9m+yvBdKllLd8qar6iVc/Wb/8QvIbb6LPz0fr5IRNWAd0bVqz+z4Llp7/mSOpR7DSWdG3UV9G+I8gyDXI1CHXKnd9D5cWwZktFB9ewel8a17PH8nB8xn8W7eQPM92NGk/gB73NcLa/M8Ku5KSEhYuXMi5c+cYOXJkRavzS5cuMWfOHPz8/IiPj6dfv36Ehobe7MpKNbTy/f+Qm5bKuI++YvUn7xG/9zdadu9N6z4DcG/Y+J9PcJfUd/A/M9UEKPuBpkKIRhhan48CxvwlMC8p5eXyxfuB40aMRzESh/79sevRg9wdO8jZtJn8vXsp27KVQfv2MrjlSI7+MJ1DZ3ax8soafoxfQaBrC0YGjCSyUSRWOitTh1936SwgoC/mAX0JBFYCSWeO4b5wGxapv1C05h32rQkixSsCn04jCQlqzoEDB0hISGDIkCHXdRtzcHCgdevWxMTE4O7uTkhIiKk+lXIXykpLqN8siF2L5pGbkU6/p1/k0pPHObJlI/nZWQx+8Q1Th6jchNGSuJSyVAjxJLABQxezKCnlUSHE/4ADUspVwNNCiPuBUiAdGG+seBTj0lhaYt+7N/a9eyOlpPRqCqK8W5H9rjjabP2dNkCprRUnG5xhm+/rfNTuI+5vcj8j/EfQ2NF4T/rK7fPxC4RXz6FPjCb9wM80P7ORLsmfMXGRljjbjozyF3RrH0SrFtfXptjY2DBw4EDCwsKwtLRU1eg1zObZ0zm2YwtgKJE/+M40QoeMIGbNj/SYONXE0Sm3YtR+4lLKtcDav6x785rfXwVeNWYMStUTQmDm4V6x7PPNdIovXCR//37yD+yn9f79BOQ2w8LbiyUnl2DzyXx0DXwI6D6Uzt3GYW6hSucmpTNH4xeBl18EAHkXjtPht3jM8izQHp5LV+1ysvY+S5JLZxzuG4B32/4Ia8NEOa7l0+YqNYfU6zkfd5jGbdpxev8eUs6dJS8zg+A+A2jdqx/iJr0PlOpBjdimVAlz7/qYe9fHcchgAPRFRXxoYUFq+gUSvxuJ9aHzsOozjpp/Tm4zb3wmTKFh5AMmjrpuk1Jy8uRJNm3aTFpaGqO7dSN46Ifs2RlG2Yl1NE/dhfOW9eRv+RezOvxK/2Bfmlrng40bqC/+auuPdlBCCE7u3sXxXVvJTrlK55Fj8e/QGSEE9q7lwzWrv2O1p5K4YhJ/DOvq6uyN65bfKL5yhUObFpK4fS0Oxy/w8ab/Q6/7lTE2EfjMXId1aDss/f0x9/XFrEGDiuMV47h48SIbN24kMTERV1dXRo8ejb+/P0IIOgycBAMnkZFTwMbozZw5cZgvtify2bZENtq8iY9IobRBJ2ybdUc06gquTQ3Trioml3TsCL8tWUD7wSNo2DqEYzu3cDZmH/4dOtOsc4TqDloDqSSuVAvmHh6EPvQcoQ89R3JeMvEnl7Hy9I98tncHj53TUH/Pnj93FgLf+fOwbteOwhMnyNuzB3NfX8x9G2LuXb/iXbxy92JjY0lJSaF///60adPmhu+4neys6N1nIPQZyAM5hayPS2btngeol76Hjmf2YXfW8CYtw38kjqNnGhLEpd/BPQh06m9kDDsWzqU4P5/uj0xB85e/2YnoHaz94mNsnJwoLS5GCMH9z7/Kqb3R+IW0Vwm8hlJJXKl2PG08ebLNU0xp/RjbkraxuN0SYhP2UC8dWhS6EFJcj0yrVELKisnfu5er73/w58FaLWb16uH7/QLMPDwoPHmS0qtXDSX4evUQdzhNYl1y9uxZLC0tqVevHt26daNbt25YWlre1rHudpaMC2sIYf/manYhG+IuExN7CKsLv3E5zonTH25lWFMtz8bej9RZIuoFg08o+LSHBmFQ/k5duXtF+fkc/OUnykpLyc/OJHzMIzh6enHheBz7V60g4VAM9QKa88Cr/8Ws/O+q1ZnRrFNXE0eu3Av1jaZUW2YaM3r59qKXby8u5l5kx4Ud7Liwg3eS91MU8xJWh63o4N2ebt+/QrsSH2yvZFOcmEhJYiI6J8M4zpnLlpPx/fflJzTDzMsLc29vfL6dhdBqKTx2DH1REWb166Nzc6uTpREpJfv27WP9+vX4+fnx0EMP3XbyvhF3e0vGdmzE2I6NSM/rz+ZjV1gXd5momAuckM/SRXOGzqkJ+FyYjua3z2HwDGg9GjIS4fRmQ2J3bw4a1cL9Tpw9uI+y0lKad47g+G/bCezSHUdPLwpzc0k5f462A4fSYciIigSu1A5GG+zFWNRgL0pBaQH7k/dXJPXLeYahBgKcAgj3DqeLdxdaubZCq9FSmpFB8ZkzFCcmUnwukZKLFyjLzaXBrFkAXHjqKXI2bQZAWFhgVq8eloGB1J/2MQD5B39H6LToXF3ROjujqYZfgDe6hzMzM3F0NEwTu3v3bpycnPD39//bOOelpaWsXbuWgwcPEhAQwNChQ68bQrUyZReWsPXEVdYdSWbHqRTKigtoY5aIR6MWhAb50790Mw6bnjPsbG4H3iGGhB46GWxUq/dr/TEZiXfzFliWj3aWm5HO6f17uK9nJAW5OZhbWqEzN0evL0Mgqm0jNfUd/M9uNdiLSuLqBqrRpJScyTzDjos72HlhJ79f/Z0yWYaDhQMd63Uk3DucTvU64WR54xmWis+fpzghgeILFyi5eImSixfRWFpS7wPDrLkJw4ZTGBdXsb/G2hqbTh3x/vJLAFK++BJZXITWyRmtszM6ZyfMvL2x8PMz/ocvd+09LKUkJiaGdevWMXjwYAIDA/nmm29ITU3FxcWFjh070qpVK8zMzCgoKGDhwoUkJSURHh5ORETETSczqWxFpWXsPZvOlhNX2Xz8ChcyCgBJD48ChrlfpK32FK4ZhxBXj8OLpw3V7QcXwMUDhsTuHQoufnW2wdzPH7/D6f27ERoNDm4e2Dq7MPD5V7G2r3mzCKrv4H9mqhHbFMXohBA0cWpCE6cmTGgxgezibKIvRbPzwk52XdzFuoR1aISGlq4tCfcOJ8QjhObOzbE2M4zjbt6gAeYNGtz0/PU+eJ/ixPOUpqZQlp5BWUY6Og/Piu25O3ZQdPIksqSkYp1dr154f/kFAAnDRyAszDHz9MLMyxOdpydWrVph1bIlYEi6lVWFX1xczC+//MLhw4fx8/OjcePGaLVapk6dyrFjx4iOjmb16tVs2bKFwYMH07hxY6ytrRk2bBgtWrSolBhul4VOS7i/G+H+bvzfwEBOX83l1xNX2XL8Kk/EWaOXTbG1uJ8uvla0PZhFpyY6ArKSEEd/hJi5hpNYuxgmeRkWZVguzAIL+1qf2Atzc0m/mERw34FYWFmTkXyZ3PQ0VU1eR6mSuHoKrLX0Us+xtGMV1e5H044CoBEaGjs0JtAlkCCXIFq4tiDAOQAL7d1VI0sp0eflUZaeTll6OsLKGssAf6SUXHrpZUouX6L0cjIlV69CSQlOY8fi+dq/0RcXE9++A2bu7ui8vDDz9ETn5Yltl3Cs2wQjy8rQFxSg/YfJIbZt20bLli1ZsmQJV69eJSIigvDw8L+VqqWUJCQkEB0dTd++fXFxcanUh4jKkplfTPSZNH47nUr0mTQSUvMAcLExp2NjJyI9c+hgdhqX9ENQVgQPzDYc+G13SDsDni3Bs5Xh3/oh4OZvwk9z966cPc22BbO5/4XXsLSx5fyRw2j+n733DpPjKvO274qdc08OmqQZWZIly3IOcg6YZLANBtbAh3lZs4ZdlrQv3m+JC+zCBnLYBUxOxgEwzhKyLclyULSyNJrR5NAz0zlUd1W9f1RPj8YKlm1l131NXdVVp6r6dE1X/855zvM8R5Zomn8mpWIRQRCQTgNHTfs3+OU5rczpixcuMR+6d8VRu96mFzdy1RsvxeE69R8Gm8MzkZtg68RWtsS2VNaT+UkAZEGmI9TBgsgCFkQXsCCygLmhuSiictTe3zQMSrEYgigiR6Po6Qyx73yH4sgIpeFhaz0+TvUnPkHk9g+g9fXRfe11CA4HUjBYWSIfMzlYlgAAIABJREFU+P/wXnYZpViMxIMPsnt0DLWrk8e3b+fGG26gc9Gik06YXwtD8Ryr98R4pnuC1d0xRpPWfNaNIRcXtUe4uCPKhe0RqnffY5nbR16E0a1QysO8N8Gtv7IutOJfIdoJjedAqPWE9thX3P1DEmMjXPWBD+OvshKrmIbBpscfpm5uF2vu+RUN8xaw+ne/xBeNIiAQHx3mjEsu54aPfvKE1ftYYIv4y3NaiXhzVZf5Tzd9/6hf1xt2EG3wEmnwEmn0Eqn3EqxxIUonpzOIzWvHNE1Gs6NsjW1ly8QWtsa2snViK0ktCYAqqnSFu5gfmc/8yHy6wl10BDtedY/9iOpULGLqOqLTSWlyksT991OanESfiqPH4+SSScR33EKhvZ2xF19k4vf3sGDbNgA0RUEtFmn4xjfwX38duS1bif3g+0geL6LHYy1eL4E3vwmlvh49ncbM5ZCi0VNG9E3TpHs8w5ruGGv2TPDM3gkSOWsoo7PGywVtEc5tCXPeHD81xUEwdKiZb5na/2s+aGnrQo4A1C2C8/8WzngzGIa1/xj6BJimCaaJIIp0r3uOB7/x74iSyAU3vYuuCy9Fdbq4++N3YJRK5DNp3vZPn6OQy7L5iYdxery0LjmX+cuuRFaOXsPyZMAW8ZfntBLxo90TX//CRuojrUwMZpgYTBMfyWIY1j2RZJFQnZtog5dwg9cS+UYvbr+dqOJ0xTRNBlIDbJ3YWumtb5vYRraUBUASJFoDrXSGOpkXnkdXuIuuUBcRV+So10XTNDZv3sw551jP7v3338+mTZtmHRMOhVhSU8PZLa2UxscpjY/ju+Zq1KYm0qtXM/Zv/46RyVjm/kwGSiXm/ObXuJcsIX7f/QzfdRei34+jvR1HRztqWzvBW25G8nrR05bgiR7PSSvyumGybSjJ6u4Yq/fEWLdviqymA9AUdnFuS7iytEcclqPc0HoY2gjDG+H8O2DxrTC6DX50tSX4NQushDQ1C6BuMTiOzlzXu59bw7P338NbP/XP+MJREmOjLP/x9+jZuA6AN/7DpxEliT//11fxRar44Hd+hPg6CLOzRfzlOa1E/FiPietFg6nRDBMD6YqwxwbTZBNa5RiXT7F67OUl2uglVOdGVk7/B+71iGEaDKQG2DG5g51TO9k5uZMdkzsYzY5WjqlyVVUEvSvcRXuwnRZ/C6r0yht8hmGwadMmVqxYQSqV4jOf+QwOh4Nt27YxOTlJOBwmFAoRCoVwOp1H/CNomiampiFIEoIsU+jpIbNqNYXuPWjdeyl0d6NPTjJ31dPI0Sjj3/4Ose9+F0FVkSIR5HAYKRKm8ZvfRHS5yL7wAsWRUZSGeivOPho94WFMJd1g23CS53uneL5nkhf2TRJLW89u2KNyzpwQ57aEOaclxMKGAMq0pW1yLzz7Q8sMP7oFclPW/nffA53XWqK/7Y+WsEfnQrj9oOJe1AqM7NlFqK4BbyiMXiphGgaCKPKzT96JIIq87+vfmZVNrX/bi+xau4oLb343bn+ANff8ikjjHLouvOSY36+TAVvEX57Tyju9pFkPZKx/H1v++vgB5YuvvYFQbT2je/ewfdXKA8rPvuGt+KNVDO3azq61qxkYHGTt5CgOjwenx0vb2ecSbfThDQk0dDlwepqRVZVcWrNEfSDNxKC1bHlqEL1omeEEAYI17hlxb/QSafDgCztP2l6MzcHp6+tjz549xGIxYrEYExMT6LrO5z//ea5tuZYHH3yQul11IICJiWEa6JLOVudW1g6tZUFsAbIhM+wZRq1WaQm10BZooy3QRnuwndZAKx7Fc8D7mqbJnj17ePzxxxkbG6OhoYGbbrqpErc9f/781/S5BEFA2C8G3NHaiqO1ddYxpakppIAVpuRddimix4M+OUFpYpLS5AT6VLxyjfgDD5D4w70z13c4UFtaaH3gfgRBYPw73yW3YQOCqqLOmYNjXhfOM+bj7Dp2jmayJLKoMciixiC3X9JqOfPFMrzQO8VzvZO80DvJY9usxpdTETmrKcjSOSGWzglx9mVfIuhWwTQhNQyjWylWL2Lb4w+xKBxDWPMtMEozb+arhw88guFvRBjdSrJ/Nw/87jFig4O84c6PM3/ZlQzv2sHvv3gX7kCATHyKt37qXw5Ih9o0/0ya5p9Z2b7olvccs/tjc/pxyom4XrLGv5KxMTYvf/SA8o5zLyBUW098dPig5WdcegX+aBWTgwNsXv4opWKR0Y3PV8pv/+b/4nB72PzEIzz9658CICkKTo8Xh9vDO7/w7yy+qondz61h3+ZtIDgpaTKFnEQuLTDa28yedWOYRh4wUV1uoo3+/cbaPUQavKi2I90JI5fLMTIyUhHp6eVDH/oQHo+HvXv38vTTTxMKhYhGo7S3t6Pul4+9tbUVVVUxDMPq3ZomsizzhWu/QFEv8uCKB9ny3BZa0i2YMZO4L86jzkcZdA1WrlHjrqE92M4c/xyafc00+5upkqu47/f34fV6ufnmm1mwYMFxbwBOZ7oDcC1ejGvx4kMeW/uZzxB5//spDg6iDQ5SHBjEzOcrdTYLBYx0GiOfJ7N6Naamoba20v6wlVN96DN3URodQQwEkPwBpEAAx9wOAm9+MwDJhx9GT6Yw9RJqYyOuRYuQyglsDkc2majESwuCQFuVl7YqL+84twmAsWSeF/ZN8VzPJOv7pvjBk3vRy0NoHdVeLnBO0hGUOe/Cc9j0n//J0K7tVP/rf1B31xBM7IGJPSR7XoSpXvzeGrY+tZwXfvNDsuk0mHBD6zhzdn0b8ivwLLyD89/+TqaGBnAHg7QvPe9V/FdsjiZGLmdZo45wjgVT0zCyWZKPPErywQdp/M63EQMBJn74PyQe/DOO1jYcXV0E3vJmlKYmYt//PsGbbp41HfOxxDanr1zJJRdfjJbNkM+kCVTXIskyY717Gd69g3w6TaFcVshkuP7Of0RRHay997ese/hPFDJpzGmnGOAff/NHSprJI9/7NrufXQ6AIDkAB+DG4X83ALKyE0VN4Av78VcFCdeFiTZW0bpkKWDFgkqKjKw67J78KySfzzM8PEwymSSZTJJKpUgmk1xxxRXU1NSwfv16/vSnPwGgqirRaJRoNMpVV11FIBCgUCggSRLyawjfKZVK9Pb2sm3bNnbs2EF7RztnX3U23VPdbNu+jWHHMN2ZbsYmx6iaqmJHcAcIEM6HcYVcNAYbLXH3NdMSaKE90E69tx7pIGOkp4I50iyV0Hp70ZNJ3GefDcDw5z5PYedO9ETCWpJJPBdcQPOP/heAPVddTXFwcNZ1grfcQt2XvghA8pFHUOrqQBTR02lefG4NO/ZsZ3J8lL/93k9xFHWSxTzhhiYEQbCeU0E44HnqfnEzT/zsx2iX3MrGuIxn9S+Zk+xGRwQBhha/nY7zLqS6ZzXnXnwBib4env7NT2lasJgbP/X/07PhBVb95m6MQpa3XL+AUGEvjGyG7BR83Apr5N4PwsgWK0FN9XxovsAKf3O9fKPkdOdg319T1620yDt3oTQ0IHlnW66MTAbThNymjQiyguf88ygODZFZ+yyuMxei9Q+Q27QJZ1cn/htuYOq3vyX1xHIkv5/0qlXIoRCt992L6PHQ/3d3kl27Frm6mtLEBIKqUveFz+O7+moya9bQ94HbZ723/41vxHv55Qx96lOz9off9z6q/v6j9H/476j+9KdxLVxw1O7RaWVOPxbIioIcCOIOzDxQ1S1tVLe0HfKc8972Ds55682Uyp6k2VSKXCaNIIioToG5l16Eq76aQj5LIZejkMtR1HQuu2ERscE0zy1/kqnMEKNZA3NQgE0iAirtDQaRBg9bt/2CZCGFIEqIioIgy7jdHv7+n/8FgLu/+20mEklMQcBEwAQCfj933HknAD//2U8ZHRtHUZTKUltby1ve8hYAli9fTjqdrpTJskw0GuXMchKSnTt3YhgGsixXjvF4PJVUnvl8HlmWkSTpuDQypnu8oiiSzWbZsWNHRaSnl6uvvprOzk4GBwf5xS9+UTnX6XTi9/vJ5/MAdHR0cNtttxGNRvH7/QfU/2ikHZVlmY6ODjo6OnjTm95EPp/H7XbjKXhY9ewqXKKLNzS9gYGBARDgo9d/lISSoC/VR1+yj4HUAI/ve5x4IT5TL8lBa6C1YpZvD7TTFmxDN/XXXN9jjSDLODo6Zu2r+8LnZ22bpjkrac6cX/0SBBFBFBhfvx6hey/u1hYA9HSGwY9ZKVoNYGtjFf0RP1W+IJe+630YiQTr3/Y2Vnc2Mtcb5swzz2b17hdZeOW1LL75VrrXria5axdjE2NsWfs0vmgVb5vn56PzFqD/zRJWLf8rW59ayXD9UrYUIjz0yFbePfgovY8/gIRBqaad0tI3MjCVpeWspbQuOcjv6/4dpIZzoJCG2C7Y+RCYBtSfDR/6q1X+7A+tRDW1Z570eeMNTbNEtFikODCIoCjoU5Oora2oTU3kd+1C6+1FdLnB0Mlt3YqZy1P9iY8DMPGTu0mvXImRTiO63fhVhamRUUK3vpPciy/S++73QLGI0tBAcXCQ4DveQd0Xv4Cp63Rfdz16MomRSlXub/NPfgxA+ulVjHzuc7PqGrrtNvw33ED66VVkVq0CwHXWWRiZdOUY99Kl5NatQ3C5cJ29BDkardx/taOD6J13UpqcwLVwIaLbjXfZMgBqv/RFnHPn4ly0CEolUBQEQaD5Zz89rh2vU07Ei+WHfGhoiDVr1hxQvmzZMqqrq+nr6+O55547oPzKK68kHA6zd+9e1q9fz+joKCMjI+i6jq7r3Hjjjfj9fjZs2MCaNWsq+0ulErqu85GPfASv18vKlSt56qmnDrj+XXd1oaoq+yamWLtj9wHl7z0zQsuiKIPa2YxtmP2PlkUZp0dm39ZJMnoIw+sAE4yyTBcKJjvWDhNp8JIZGyGfzlihMaaBYJro2owpdGLrJrSCRkEUQRAxRZHi+AiURXzdkysoIM40AgSBqM9TEfF7f/dbNGO2laa5toYP3PFhTNPkP7/+dYq6JR6yJCFJEmd0dnLjzTdj6Drf/OY3mLbyTP+WLVp0Jtdcex2FQp5vfevbgDn9B8AFF1zAsmXLSKfTfOtb3yqfa4m3YRhcc801XHjhhWSz2UpP2uPx4Pf7CYVCKOXQm/r6et773vfi9/vx+/2zTOFAZf/xQhRF3G4rQ1xVVRUf/OAH2b59O3v27GHhwoVceeWVBAIHT5eZKCToSfSwN7GX7ng33YluNoxt4KGehyrHKIJC54OddIW76Ax10hXqojPciV89fp/xSDFNk5HuXSRGR5BUFVlRqWpuwRuOWGP2+/2vBkeHCNXV44tU8fgTf6ZUKnH91ZfjB0SXk7Y//4lCXz+PPHI//X09LL3kSi6+9TaUqiqMbJaOf/w4Y08+zq7MFLufeQLRMCkODACw6cEH6Nm9HUyT1liSM4aTZN53O+n/+k+8F1/M2ZJA9b33AffxbkDweBipqWFDXYTu5ktwb+vmDXfexi5XgM0uDw63C7fHhetzX2L+me0Unn6S1GOPIzhUBFVFVFUE9WIit/8YUTbIPfF7Cj0DCH/8I3oqhfHwf6C6kvjq8gguL5raSbHqUsy2a62ww4kepNo2PJdeDkBq+XK03n0oDfWYJR30Emp7R6X3l9u4kVI8bpmNFQUMA0FVcZcjHeL3P0BpdAStp8e62aKE88yFhN/9bgxNo+fGt6FPWc59cm0teiyG+/zzafiPr5N9/nn6b//gAf/b1gfuB2D0y18h++yzs8rc555beZ167DGMQgFBVdCTSRwDA3Dd9QA4u7pQ5zSj1NYhKApKYyNq2W/DSKdxn38eosOB6PVh6iU8F16E66yzrHMXLKDpf/8XPR5HdLus+Q4iVs796N99mPqvfgUjn0epnm3mjtz+ASK3f+Cg31elupqqj37koGWhW26Z2djve3vch8CO67sdBYyy6TqfzzM0NHRAuVZ2fMvlcgctn24EZLNZhoaGyOVyVuajshBNX9/lchGNRiu9zf0XgLa2NhRFOaBsOkvW0qVL6ejoqJhl9z8X4LrrruPqq6+ulImiOCvDVjZ5XsWBzloyTA5lWP7T7QCI4tW0VruI1Kn4ogLeoECo1lvJwHX9W28kn0qiF4uUikX0UpFQXUPl+ktbGsmn05Q0Db1UpKRpNLWcUSmvSsbQtAIl3UA3dAzdIFxnhVHppRLi4F5UUQJRxBREDFFEC1uiUchmyPT3ztz0soqnw5ZQZSYn0Qamy83KKj1QB0BydARhbND6Ud9vKcYshyQzn2W+W8br8eD0uHG4PThcDoIuqwct6DrOUgEhn6UgAm4PiuPkcDAUBIH47u0Ud27Gva+HZPcWHtn0LNWt7Vzxvv8DwLanVlAqaji9PpweH/VeP3OrrsAz9+2V62SLWXoSPeyJ72HFiyvIqBn+2vdX7tt9X+WYek89naFOWgIttPhbKuuwM3zIe2EaBumpSeKjw8RHh0mMjhAfHeGGj3wCUZJ45g+/YcOjD1LSNEpaAZfPT21HJ2/62D+hqA4MQz9kWFQyNs6f/+srjHTPbtxOO4ElxkbYtXY1XRdeyvbVT7Lqtz9n3kXLeOPff4rL3vtBHvvht/jNZz/N/EsuB0Hg/Le9g/BVVzKPIvNFkYVXXFO5puh2E3nf+3jTe9/Lc3/8A8M7t7Ps7bcSbLTGxd/8ibvYet89eE2BQDZHcWwM0eFErqoCQG1rI/rRj4BhgmmgJ1N4hoa46J8+jTpnDtnuvfT8Moi6tw9tMk4inWVqLMGXf/YCWe8e/s/EOq7Y8DQOo4Ssl6CoYWoa4ffeBp4gqe1TTPzot8BvyzWWEZRqun72GRheR+zup0hsuQe4p/KZJNWg8/0yRNpJPJgk9eJMdASAo72FtvvvBdXNyFe+Sn7z5lnlriVLaPnNrwGsJEODg8h1dQiSZJmvy41gUVXxXHwxZlEDXac4NoZz/hmE3vEO631aWqj553+27lNzk5XXwOWqzBfQ8I3/pjQygpHPg2ni6OhA2q/R3PLb38yq18qVK1lQNqcLqkr7gw8e9PsjBQLUf/nLBy0DDmu+di2wyqTj2Hg/Xthj4qfAeOI0hm6QGM8RG5gR9omBNKnJfOUY1SUTafAQiLrwRV34o078EWvtCTgQxNcuZKZpkksl0UtFdM1qIJSKRVw+P/5oFXqpSN+WzRi6jmHoVvpQXSfa3EK0aQ6FbJYdq1da5bqBoZcwdJ05i5ZQ2z6X9NQk6x/6Y+V8QzcwSkUWXnEN9Z1nMLp3D498778pZLNouSyFXBZMkxs//S+0Lz2f7nXP8cDXvjirzoIgcsu//CtNCxbRs+EFnv7tz1FUB7LDgeJwojgcXHLrbQSqaxnp3k3PxhdQVAeK04msOlAcDloWn43qcpOJT5FLJVEcjkqZ7HDMEq9sIs5YTzejPd2M9XSTz6S45V++AsD9X/siYz3dVLe0IUoy+UyKYE0d193xDwD85GN/y9Tw7LHglsVnc9Nd1mf61V3/SLFQwOn14fL5mEqlOffKa5i/7ErGc+M8u/ohBouj9GqD7Cn2sTffR9HYL7e76qPV31oR9WZ3Iy3BVlpCray7716e+cOvZ+5beYKNW7/4NTzBELvWrqJvyyZkVUVSVNKTEyRjY7zzc9aEMQ9+82sMbN+CLxzBF6nCG4kw58wltC89D0PXufcrn2XueRfRtOBM9FKJkqYRrKnFHQiyefmjPP4/3668d+eFl3L9Hf9QyQmu5bI89au72frUClxePzd85BM0zj++Od8Px0giz/q+Kdbts5atQwmKuvX7Wh9wsqQpyFlNQZbMCTE/IKHkMpiahujzIbpcaP0DFc/9wp49lGLW+KwgGggDz2DG+nD5EzCxB31oDyz7JMW6axAS++B370LXRNzRIrhC5I05mIvei1l/NmYqhjC2HbmhGXXuAhBEjJKA6QwhBQ4+MdDx5FT6DT5R2HHih+F0+AIVciUmyz322GCGyaE0yfEcmf1i2wFEWbAEPeK0BD7ixL+f0Ds88knRW32lmIaBls8jKQqyopBNJhjt3k0hm0HL5ShkMxSyWRZddR3+qmr6t27mhb88QKlQoKgVKOXzFLUCN376s0Qamtjw6IOs+MkPDnifD377RwSqa3n2gXtY9ZufHVB+xw9/gScYYu19v2P172bG5AM1tdS0dvDGv/8UoiRR0jTkw3jGFgt58uk0+XSKfDpFLp3C6fHSvNDyFF9x9w9JTcTIZ1LkUykSkxPMv+hSrv7gnZimyTfecyOGPjNO7vT66Lj6CtyXnkFPooeBF9YzKE+SHB4h2F+kYdzFirPHGa3W6NDr6EhFidQ20tg0l/amBbSG26hyHdlc65uXP8Lw7p2kJmKVxeFy8X++e/cBoVUHIz4yzM61q/AEgiy4/OpT8vs4Tb6os3Uoycb+OBv6ptjQF2cwngNAFgXm1flY3BhkcVnc26u8SK+kkW3o1thtPgH71kAmBpkxiPdZOeQv/7/Qcgnsegx+fcuB5//NvdBxNXSvgOVfsqZ7dUetGeM8UVj8LvDVQnYSCknwN1ipbB2+o3SHLE6H3+BjjS3ih+F0/gKVijqpiTzJiTypWI5kLE9yYmZdyJRmHa84pUqv3R9x4YuWRb4s9orj5HW2OdoYuk6xUKCkFax1IU+ovgFJVpgcGmB8X095f7khUChw7ltuQlZVdj+7hsT4KDWt7VS1tOH0HJ2MX4fipVORjvV0k0slyaWSlml8eIjGBWdyxsWXkZqM8T8ffn/lXHcwRGRhJ/LSOQy7U/QmeulN9rIvuY9cKVc5zqN4aA+2W8lsyglt5obmHjTe/aWUisXTLlXoq2UslWdjX5wN/XE2D8TZ3J8gVbCeQ48qsbAhwFlNlrAvbgpSHzgKw0CFtBUal41Zgmwalhh3Xm+JdM9TsOq/ITsBmQnruFIePvyMlcHu2R/Cw5+euZ47AnoJ7lwL/np49n9gzxNWEhxXCGQHBJpgwY3W8eO7rIlqnEFQ3KC6QXZWctc/tfwxll117Wv7jKc5p5WIn9W+0Fzx1Xtf/sAjZHhshMa5c5B8KqJPRfKrSD5rEZTTO296IVciNS3qLxH5VCxHqWjMOt7lUyqi/tKevDfsRLLzzJ8QXklDVC+VmBzsJz4yjDccobZ97kGzrBmmwVh2jJ5ED73JXnoSPeye2s3OqZ2ktFTluGp3NXWeuspS46mh2ddMR7CDWk/tKd2TPh4YhsneWIZNZVHfOJBg+1ASTbeevahXrfTWFzcFWdwYsBLSHGu0jCW0omT16nufhuQQSKrV01dccN1XrVzza38A638Ok92W+AOoXrirPCR0z/th6/2zr++rh09Y/j3pr5+FtxizxN0oWT40zRfCu8rDOr+7zdoviJYHv16wvPgvsaITePJrlnUgF4diud5LboPQHMs6sXclFFLW+ZF2q1HTeimoHqtRk50AZ8CybIBlhZDKjc5cHFIjVmOnkLSuITnAWwXFPCz/gnXtYtZqwCSH4e3/c9RS9U5zWoWYmQWdwt7EUbueJyeQGuq34lReguCUkfzKjMBPL/79tv0qguP4hFkdbRwuGUejj2jjgeYxa9y7aIn7fsKenMgz2puke/14Jcc8WI1qT8iBP+LCG3bgDTrxhhx4go7K2u1Tj8qYvM2rR5Jlqua0UjWn9bDHiYJIraeWWk8tF9ZfWNlvmiYjmZFK+tn+VD8jmRG2TWxjRd8KNGNmCMejeGgPtNMebKfZ30yDt4FGbyMNvgZCjtAp+cwcbURRoKPaS0e1l5uWNgJQKOnsGE5Zot6fYNNAnBU7xzBN6zlb1BBgUWOQjmovixoDzK/345CPspVM3c/CEmm3lkNxwR3WYuigFy2RTY/NlC/7FCx4G+STlthpGau3Xqav+e3M92esBoBYlqRQy8z5gghj261efD4Bsgo1ZV+IYh7++uWZ40wTMK1Y/NAcGN4E986O8wbgHzZbn3Hd3bB8tv8Mkbnw0bK19+dvtXLs78+ln4Sr/gX2PA5rv2ftk1Srfv56yMePuogfjlOuJ34szOmXLbsMI1NET2noKQ0jqaGnNfSkhpHS0FPlsqQGpQPVXlDE/UReOaBHP10mepTTRsQM3SAdL1R68amJmXV6qkAmXpgl8mD9YHmC+wl7yIG3sm2Jvjug2j36V8jJMiRkmiYT+Qn2JffRHe9mT3yPFRYX72YiPzHrWLfspsFXFnVvA42+Rhq9jTT6Gqn31uOSXSfoU5ycpPJFXhxM8HzPFKv2jLNjJEUqb5nhFUmgq9ZHV42febU+FjUGWNIcQpVPjefoNX9/Dd0Sd1EGU4eSZgm9K2Q1HKZ6wSha1oGpXmvcv3ah1ZAY3WYl5kkOWkMEoggIsOyTUCrAM98BUYFiDvx1ViOh6Twrlt80rfcWhGMe139amdNP5Ji4aZqYeb0i6EZZ6GeJf/m1mT9IAg4RRO8hevT7i79XRThFHsBDYRom2ZRGJl6oiHplHS8L/VThAJM9Arj96gHi7gk6cPkUXF4Vh0fG5VWRVdHuzXHyiPjhyBazDKYHGUwPMpAaYCA9wGBq0FqnB2eNvwNEXdGKuDd4G4g4I/hUH37VT9AZpDXQelLGwR8vTNNkJJlnU7/VW98ymGDnaIrxlDXXuiqLtEY8tFd7aIt6aa/20F5OP+t1nFwG2FPh+3uiOa3M6ScSQRAQXDKiS0apdh/2WLOoz+rBG2Vxr4h+ooA2kMLIFGeyneyH6JZn9ehnmfN9KuL0/pPU2UwQBTwBB56Ag+o5Bz/GNE0K2dJLhD5POm4JfGI8x9DuOIVs6aDnS4qIy6vg8Ci4vApOr4LLo+Dw7r+t4iy/dnoVFPXkvF+nO27FzdzQXOaG5h5QNt2Drwh8yhL2gfQAG0Y38HDPwxjmgRawale1lbmuvEyPw7tkFy7ZhSIqp20jTxAE6gIu6gIurl9YV9k/mdF4vneSdfum2DueZvtwikcPV0YsAAAgAElEQVS3jlZywwNU+Ry0RNzMiXhoibhpCLmoD7hoCLmo9TuRbUvYKYUt4scIQZGQwxJy2HnY40zdxMgcqkdfxEhpFMYT6CkN9APVXlAlq0fvVWYLfsBhbZfX4kkoXoIg4PQoOD3W1K6HoljQycQL5FIauXSRfKZIPm0tuf1ex/rT5NKaJfqHMDCpTmnWOP10b98TnDHtu7ynz7DHqYAgCERdUaKuKIurDpxwpWgUSWmpyjKRm6A70V0x1d+7+94DevJgxcMvjCxkYXQhXeEuqt3VRF1RqlxVOOXDP5enKmGPynULarluQW1ln1Yy6JvMsGcsw95Ymt5Yht6JLE/tGucP5Z77NKIAHodMxKPSWeMj7FEJeVRCboWIx0FblYfWqIeA6/RtIJ1q2CJ+ghEkAcnvQPIfPl+3aZoY2dKsHv1LzfnFoQz55BSmdqApX3DKSIH9xH1a4PfbFj0n54OpOCSCNW6CNYe3fkxjGCaFbFnkywKfzxTJpTSyCc3q6ccLTO2YIhMv8NIRJVG2rAjeoDVu7wk6CERdBKpdBKvdeMNORFvkjxuKqBB2hgk7w5V9lzVdVnltmAZD6SG6493EcjFypRy5Uo6hzBBbYlv4yZafHJBf3qf4iLotQZ9uQFS5qoi6o9S4a2jwNlDtrkYWT/2fSFUW6aj20VF9oANrTtMZSuQYiucYnMoxGM+RypcYSeTpHk+zvi9OPKtReol/iywKBN0KQbcl8NPrkFudee1RCe1XHnQrM/O32xw1Tv1v6OsEQRCQPAqSR0GpPXxsrlHQ0ZMF9IRltteT2sx2skBxOIOR1g7srcrTDYr9RN7vsMR/ett38o/Xi6KAy6vi8qq8XD4qwzDJJbVZ4/X7m/fH+1L0borNGrsXZaEs6m5L2KtmXvtCTrsXf5wRBdFyjPM1HrQ8W8zSn+pnPDfOeHacWC7GeK68zo6zaXwTsVyMgj67VyoJEhFnhIgrUhH66ddhZ5hcKcdkfpKIM8IFdRdQ56076PufzLhUifYqL+1Vh7aEmaZJulBiLFWgZzxD70SGqazGVLZIPKsxlSnSP5ll84C1TzuI8+80PodM0KPgd5YXl0wmXuCp1DZ8ThmfU8apSMiigCgKVHkdqLKIKosokogqidT4HTgVCYcsIktiJdX06xVbxE9DRIeEWOVGqTp0z9XUTcsDPzEj7npyZrs4mCa/fRLzpY5nMGO6r/TiHTO9/IADOeRAUE4+8/3B2N9j/lCYpkk2oREfy5IYy81a92+fRN/vHkmyiD/qxB1QcXrUyti8c/9xeq9qj9EfR9yKm65wF110HfIY0zRJFVPEsjFGsiMMp4cZTA8ykZ8glosRy8XYObWTydwkJfPgPho17hqa/c14FA/D6WFKRolLGy/l7OqzKRgFZEGmLdBGk68JRTp1kt8IgoDPqeBzKocVeyiHphZ1prJFpjIa8WyxLPiW2E9lNeJZjVS+RDJfZN9ElrG4zsZYP+nCwe/r4VAkAQEBr1OmqBt4VBm3QyLiUfE6ZHQTHOVGgEeVcKsyHoe1dikSblXCVd7vViWcioRLkXAqIrIoYpgmEa91rZO1oWCL+OsUQRKQAw7kwOHFy8yVZon7LLGfKqDtS2K81PFMwBLzqMtaIi7kqNN6HXYinGImNUGYEfqGztl9e9MwySQKxMdyJMayxMdyJMdz5NIak0Ppyhj+IcfoXTLekKO8OGfWYQe+kBNPyGEL/XFAEAT8qh+/6qcteOgpiA3TIFFIMJGbwKW4CDvDDKYGeWb4GbZNbKM/1c9UfooGbwOarvHL7b/kp1t/esB1go7grJ591Gm9DjqCOCQHAUeAGncN1Z5qfIqPkllCEU9+4RcEoSyIMg3BIwsTnPZO1w2TjFYip+mUDJOSbjCR0SjpJoWSTlE3SOVLxLNFCiWdnGaQK+oYpklWKyGLIulCiXxRZyieI5a2chZoJYOSYZDVdNKFEllNn+XodyQ4ZBGXalkIwh4VlyrjdUiEPQ48qkSN30ldwInfZVkYAi6FMxsPPjPh0cYWcZtDIggCgltBdB/ehG8WjVnm+lIsRymWoziRJ7txHDO/n8iLIIecM+JeNS3yLqTg0Zmg5XgiiEJZfJ00dh3ceG8YJlq2RC6tzYzTl8foM3HN8sifskz3uVTxgPMdHhlvyIlvP4HfX/DNV/iDZPPqEQWRkDNEyDnzv+4IddAR6jjo8UktSW+iF7fspqAX6E50Wz383EwPf+PYxoOa819KtauatmAbYWeYkDNE0BEk7AwTcUYIu8KVcX2H5MDERBROrcayJAoVM/s0cyIvn9b3lWKaJppukNN0suXFel0iW9TJazqFkkFRNxAEgYl0gcmMRqpQwjRNRpOFSmNgYCpOVtOJpWf71vicMi9+/rqjXveDYYu4zWtGUERLiCMHtrynHfKmhb0Uy1GayFEaz1HoSWBq+5nrJQE54kSOuq2ee1nclagL0a+etOasl0MUhYr5/OUoFS1P/PSkFW6XKsfTT78e3ps4IOe9IMLo6mcJ13kI13sJ13sI13vwR122A94Jxq/6WVS1qLK9IHrw6TJN0yRTzJDQEhT0AvF8nNHsKKOZUdLFNJIo0Z/spzfZy0BqgEQhQaqYOui1BIRKxj2/6qfKXUWjtxGH5EAWZTRdw6N4aA20UuetQ9M1Qo4Q1Z5qDMPAITtO62Q7giDgkCUcskTwyHxlX5ZCSWcirZHMF0nmShRKB8kTcoywRdzmmLK/Q55jzuzkHKZpYqS0srjnKe4n9Pldk1DaL62rIpZD6cqJcbwqkrecIMerIvoUpHLZqTIefzBkRSJQ5SZwGH+GYkGv9N7TU3k2P78Dr+xkpCfJ7hdm0l1Kiog/6sLhklFdEqpTRnXJqE6pbMZ3EqpzE671oLrsn4ITiSAIeFUvXrU85nwEltiiXmSqMMVkfrLipBfLxdAMjZJRYig9REpLMZwZZsPoBopGkaJRRBEVCnoB8xBjPKIgUu2utsa3SznaAm0EHUH8Dn8l4c70ejpqIF1ME3KGcMtuEoUEDslBW7CNfCmPWzlKSnkS45Al6oMu6jn+jR/7ybU5YQjCTHid4yXDkKZhoicKs3rwekrDSBcpjmXRuxOYuYM7wggOyYqX9yoz67LQq3VelHrPKTcuvz+KQyJU6yFUHuIY1XZx+eVWfLWWLzE1nGViKM3kcIZULI+WL5FPF0nG8mi5Elq+REmb7bDoCaiE6jyE6jwEq134wk58ESe+sBOH++Qfi309okgK1e5qqt3Vr/jcgl5gX3IfI5kRVEllKj/FaGYUWZRJakkG09bkJaqkWsdlR9g1tcuK1T+EBeClqKKKZmgsqlpEjbsG0zSp89ahiiqGaVQS+OhpndEdo7gVN3P8VmaokCPEUGaIpTVLT4swv2OJfXdsTkoEUbDGzkNOmHvwsWazZKBnrIQ4enp6rWGkipbnfapIcSSDni7OEnxBEVHn+HG0+FFbAqjNvpMyGc6rQXXK1LT6qWk9fEpSXTdIxfJMjWSYHM4wNZJlajjD9jXDlAqzTYGqU8IXscS80pN3yihOCW/ISbTJS7TRi+q0f05OFRySg85QJ52hzld8rm7opItpklqS8ew4k/lJvKrXMvFrKdyyG83Q2DG5A6fk5PnR59k9tRtBEFg9tJqSUUISrEmjdEO3Js2ZOPh7uWQXTsmJLMq0BFpQRIV8KU/JLBFxRsgWsxSNIk2+JuaG5jKWHaPeW0/QEcSn+ijoBdyymzpvHbXuWnRTR9M1ElqCOb45SMc45/nxwH7qbE5ZBFm0vOsP42E/jVky0JMa2kAKrTdJoSdBcnmf5TUuCigNXhwtfhwtAdQWP5Ln9O59SpJYSaDTuriqsn969rrURJ7UZH5mPZmnkC2Snsqj5XWK+RJaTkefjgkWIFDlItroo6rZS7TRR7TJi+cI/jc2pxaSKBFwBAg4AjT5ml7TtYpGkXueuIdrLrmGpJakL9mHIAjsTexFEiSGM1aoXr6UpzveTV7I45SdqIJKX7IPURQp6kU2xzZT6n7lIWrtgXa8qhdN1zBMg7AzTHuwHUmQUCWVgCNA2BlmKD1EjaeGeeF5CAjUemoRBRGP4kEURHoSPTT5mk6I1cAWcZvXBYIsIoedyGEn7kWWaBn5EoV9SbSeJIXeBOk1Q6SftsyIcrUbR+uMqIsuGVM3wTAtb3DdxNQNa/sl+wHL2953HOZ9PsoIgoDbr+L2qy/bmzdNk0xcIzaQItafYrw/zXhfku71M+Pybr9q9dSbfEQbvVQ1+QhUuRBEAdM0KRZ08ukipmni9jtQTtK5AGyODYqoUK/WU+WuospdRXvQmvL08qbLX9F1pv0DHJKDZCFJQS8wmB6sCHR/qp9MMYMoiKiSioDA6sHVZEoZJEEi6AgymZ+kN9nLutF1GKZxyHwAh2LaajDtMPhvl/7bKzr/1WKLuM3rFtEp4+oK4+qy0nmaRQNtIEWhN0GhJ0l24ziZZ0de/fV9Kmq9B6Xei1LnQa33IoVPn4xugiBUYtxbzoxW9hdyJWL9KWL96Yq4D2zvq0xNqzgkVKdELlPEKM12rlIcEm6/iifowB9x4ou68EedNHSG8L3MPAQ2r1+m/QMAAg7LK3D/sL8LufCAc26dd+shr1c0ioiIZEtZRjOjVLmr2Dm5k6nCFLlSjlguBsD2ie0MpgcxTANRECvhfUPpoaP58Q7LEYm4IAgeIGeapiEIQicwD3jYNM0Dg1ptbE5RBEXE0RrA0RqAKyznuuJIBq0vaWWuEwUESbDWojjzurxGEiyBNqE4mqU4nKY4lCa/ewqmrc4OCaXOUxF1udqNHHYiek/OvPWvBodLpqEzNCsxjl40mBzOMF4W95Kmz8pkB4KV2z6pkU0USMcL9O+YIhMvN6IEaOgM0nV+Le1Lqm1veptjynRiHZ/qw6daOefPqzvvRFbpkBzpk/AUcKkgCCHgMeB54J3Ae45VxWxsTjSCKKDWe1HrD59q8mA49xMws2hQHM1QHM6gDaUpDmXIrhsj88zwzHuplrlfCrusWPmIE7n8Wgo7T3mBlxSRqmYfVc0HTsJxOEpFncRYjr0bx9m5doQVP9/Byl/uJFTnoapspm/oChFp8Jzy98jG5tVwpCIumKaZFQThduB7pml+TRCEjceyYjY2pwuCIqI2+lAbfUznnzINE33Sio3XJ3KUJvOUJvKUYtkDYuSVBi/Bt7QfEGf/ekBWJCINXiINXs65oYXRniQ9m2PE+lPs2zbJjrVWT90btkz6jfNCVDX58EVmGj6lol65lo3N6cYRi7ggCBdi9bxvL++znwgbm1eJIAqV3PIvxTRMa3rZiRzF4QypJwcY//4m3EuqCbyh5WWnrT1dEQSB2rYAtW0zmVDSU3n6tk7S+2KMHc8Ms+XJcnyzS8blU8ilimi5EqIsUN8RpGl+mDkLIoTr7Z67zenBkYr4x4DPAPebprlVEIQ24K/Hrlo2Nq9fBHFmchpHWxD3ObWkVvaTemqA3NYYviub8V3ScNJPCXs88IaczL+knvmX1FMq6kwMZMre8tbkM9Oe9oVskb5tkzxzXzfP3NeNJ6DSND9M84IITfPCR5QS18bmZOSIRNw0zSeBJwEEQRCBmGmaf/9y5wmCcD3wTaxe+49M0zyoz70gCDcBfwDONU3zhSOsu43N6wLRIRG4rgXPOTXE/9JD8pFess+PEHhzO6554RNdvZMGWZEOm+jmYso9922T9G+bpGdTjB3PjIAA1XP8NJdFvabFh3gKZ/SzeX1xpN7pvwbuAHQspza/IAjfNE3z64c5RwK+C1wDDADPC4LwJ9M0t73kOB/wD8Czr+4j2Ni8PpAjLqLvnU9+1xTxP3cz8dOtOLtCqJETXbNTB2/IyfyL65l/cT2GYTLWm6Rv6wR92yZZ93AvLzzUiygKuANWmFuk3kO0yYfqkomPZdGyJeZfUk+k4ZU7O9rYHAuO1Jw+3zTNpCAI7wEeBv4vsA44pIgD5wF7TNPcCyAIwm+BtwLbXnLcl4B/Bz71SipuY/N6xdkZouZjZ5NeM0zyiX0075QY7duA59xa3IurEO30p0eEKM6MsZ/35jbymSL92yeJDaTJxgukpvLs3Rhj22orikAQQJRENq8coPPcGuraA4iySF17oJLH3sbmeHOkT7siCIIC3Ah8xzTNoiAILzeJcQPQv9/2AHD+/gcIgnA20GSa5l8EQbBF3MbmCBEkEd+lDbjPrmbzPWuomzKI37+HxIN7cS2qwnNuDeocv+289QpwehTmnlPD3HNqKvtM0yQ9VaCk6fgjLoqazobH9rF5xQC7nhsFQJJFLnx7O4uuaLTvt81x50hF/IdAL7AJeEoQhDlA8rW8cXls/b+A9x/BsR8CPgRQU1PDypUrX8tbzyKdTh/V69nYHG/S0TSJOSaOBPgHDPSNI2TXjaJ5TFJ1JgW/ieaDkhOwNea1E4K5bzUxSgK6BiMbDVb9fjfrV+wm2CbgqYJSwZrn3RlilrAbugkmiLL9j5jG/g1+bQim+XId6kOcKAiyaR46uWw5JO3zpmleV97+DIBpml8tbweAbiBdPqUWmATecjjntnPOOcd84YWj5/u2cuVKLr/88qN2PRub481Lv8OGppPbHCPz/Ajavpm2tuCUUGqsbHFKrRvX/MjrNlztaGKaJlufGmTD430kY/lZZbVtAZZc20zjvBCx/jTLf7aNQrbEuW9sZeFlDUh2hIH9G3wECIKwzjTNcw5WdqSObQHgc8Cy8q4ngS8CicOc9jwwVxCEVmAQuBV493ShaZoJoJJwWRCElcAnbe90G5vXhqhKeM6pwXNODUa+VE4Bm6E4Yi3ZjWOYeZ3kY/sIvbOrkjve5tUhCAILL2tkwbIGRnuSTA5n8AQcJGM51j+6j4d/8GJlwhd/xElVs49V9+yme/0Yb/roYhSHRCau4Q3ZDSqbV86RmtN/AmwB3lHevg24G3j7oU4wTbMkCMJHgEexQsx+Uo4x/yLwgmmaf3r11baxsTkSRKeMY45/VrY30zQpjWaZ/N1OJu7eiu+KJvxXz7FywNu8ag6WjGb+JfUM7YoztCcOAiy5phnFIbH7+VGe+Ol2/vTNjQgCjOxNsuTaZi68sf20mSDH5vhwpCLebprmTfttf+FI0q6apvkQ8NBL9n32EMdefoR1sbGxeQ0IgoBS66H67xYT/9NeUn/tR9uXJPyueafk9KknM5Is0jQ/TNP82daOzvNqESWRx368FU9ApXVxlA2P9REfzbJwWQMN80JIdqy6zRFwpCKeEwThEtM0VwEIgnAxkDt21bKxsTnWCIpE6Ka5qC1+4g/sYfRb6wnfOg9ne/BEV+11QcfSampa/bh8CpIssuGxPp5/qJeeTTFCtW4u/5t5eEMOFFXCZTeubA7BkYr4HcDPy2PjAFPA+45NlWxsbI4nnqU1qA1eJn61ndiPXsS7rBHfxfW209txYP850s++bg6Lrmik98UJVv9hN/f/x3rAimfvPL+GMy6qp7bNb2eTs5nFkaZd3QQsFgTBX95OCoLwMWDzsaycjY3N8UGp9VD9kbOI/7Gb9FMDpJ8exHVmFO/F9TiaX3+zp50oZFWiY2k1zQvC7HpuFFESmBhIs23VEDueGcETdPDGOxdR1WRN6aoXDSTFFvXXM68otZNpmvvHhn8c+MbRrY6Njc2JQnTIhN/Rhf+qZtLPDJN5foTcpnGUJh++i+pxLYoi2L3A44LqlFm4rKGyfd5b2ujfNsnqP+zmgf9cT217kMmhNOmpAl0X1HLlbfPsHvrrlNeSn9F2obSxOQ2RIy6Cb2rDf80csutHSa8ZYvJ3O1GeHiB0cydqvZ03/HjjcMmVMfTHf7KVTDxPXUcQSRHZsWaY0Z4kuZTG5e+ZR9tZUTIJbZap3ub05bWI+KvLEmNjY3NKIDokvBfW4zm/jtzWGPE/djP2nY1WSNoVTfZUqCcAX9jJ2z+5dNa+aKOX7vVjCAIs//l2NjzuYWxfkjf87Zm0nVV1gmpqc7w4rIgLgpDi4GItAK5jUiMbG5uTCkEUcJ9ZhaMtSOLP3aSW95HfOkHolk5UezavE87iK5tYfGUT6akCv//Kc0yNZAjVenj8x1upafMzZ2GUJdc0n+hq2hwjDivipmn6jldFbGxsTm4kj0L41nm4FlUxdf8exr67Ad9lTfivarZ75ScB3pCDd9x1LoIgIIgCK36xndREnjX37qGQLRKq9dC2pApFlU50VW2OIvachTY2Nq8I1/wIjhY/8b/0kPprP7ltE4Rv7kRtstv8JxpvaGYc/E13LsYwTB7+/mbWPbwPgIY1Ic59YwvhOo8de36aYIu4jY3NK0Z0K4Rv6cS1KEr83t2MfW8jvmWNVvpWO+TppEEUBd7w4UVMDWcY7Uny11/tYHDnFC6/ys2fXoo/ao+KnurYIm5jY/OqcXWFcXx8KYm/9JB6csDqlb+zC7XR7pWfLIiiQKTBS6TBS02rn/hYlr/+Ygd//MYGzntzG/6oi+pmH8880I07oLL4qiY75esphC3iNjY2rwnRKRO6aS6uM6NM3bub8f99kaoPLbKd3k5CpsXcE3Cw/GfbeeLubQA4PDKFjDWz9O7nR7nyvWdQyJboXj/Gsls7Z82JbnNyYYu4jY3NUcHZGaLqw4sZ/8EmYj/ZQtUdi1Cq3Ce6WjYHobYtwLs+ex5jfSmmhrNseGwfS69rIVDl4snf7OSP39iAy6sSH80y/+J6qppty8rJii3iNjY2Rw056CB6+0LGf7iZ2I+2UPXhRchBO+nIyYgoidS2BqhtDXDGRXWV/YEaF7//1+cpZLIA7N04bov4SYw98GFjY3NUUarcRD+wEKNQIvajLehp7URXyeYVEKn3ctFNHbScGaF+bpCdz46wc+0wpmHn9zoZsUXcxsbmqKPWe4m+fwF6okDsx1sw8qUTXSWbV8Diq5p4452LmX9xHamJPE/8dDur79uDYQv5SYct4jY2NscER0uAyG3zKY5lmfj5NsyicaKrZPMK6bqgjju+fTlnXtbApif6+f2XnyMZy5FJFEhN5tmzbgzTtIX9RGKPidvY2BwznJ0hwrd0MvnbnUz+fifhd81DEG1P51MJSRG59NZO6uYGWfmrnfzys2tnmdZv+cw5VM+xp6s9UdgibmNjc0xxn1WNniqS+Mte4p5ugm9tt0OWTjEEQWDuOTX4wk7WP7oPf9RF7+YYifEcu18Yo6rJZzfOThC2iNvY2BxzfJc2oKc00k8NIPlU/FfZE3KcitS2Bbjhw4sAuOSWuTz43U1sfLyPPetGeddnz0d12pJyvLHHxG1sbI4LgetbcC+pJvn4vv/X3p3HRXWeDR//3TMDDPsquIDgiogoKGqiWYhLNMa4xSjE2BibptGaxSbN9mTBPM3b1Cdt9reJaRPNm4UYfbSapUlsRU3UKAiuaDSKiiIKCoLscN4/0AkoyjbDYWau7+fDx5mzXgP3x2vu+5xzX1zYfkrvcIQVDL41nPABgZScrWDHv46i1WrkHTlP8dlyvUNzGvK1SQjRLpRB4T+9DzUllZxbfQiXLp4yPaud69rHj659/PjXkt2k/+soB9NPU5xfhqefG3c+MaRBQRZhG9ITF0K0G2U0EJDYD6OXCwWf7JdHzxzEmDn9GT0nCoNB0bmnLxVl1ax9cycl5yos21RX1ZD6yQHyc0p0jNTxSBIXQrQro6cLAXdHUVNYwbnPf5JHlByAydVIv+u6cHfycKY+PpjbHoyh8HQp/+/Zzaz/aD+l5yvZtuYIezeeYMc3R/UO16HIcLoQot25hfvgOz6Coq+OULL5JN4ju+kdkrCCS08dhPULYFbydWR8e4x9P5zkUPppKsvqRl0K80r1DNHhSBIXQujC64ZuVBwuouirI7h198E1TK6POxKfIHduvjuSmIRQNn72EyYXAwFdPMn893Eqy6vlTnYrkeF0IYQulEERMKMvRm9XCj7dT21ljd4hCRsI6OrJlIVxTFwwiLD+AWi1Gj9ty2PrP3/mcOYZvcOze5LEhRC6MXi4EDCjLzVnyzn/nVwrdXShkf50i/RjwycHSP/6KF+/s1sKq7SRJHEhhK7cevrhOawzJd+foDKnWO9whA0pg2Ls3Gj639AVn6C6x89y0zSO7i0AQNM0KbLSQnJRQgihO9/belCWdZZzKw8SvCAWZZT+haPy9HXjlnv6UVtTy1fv7Obo7gK+eHMnZi8XPP3cqLhQxeRH4/AL8dA7VLvgEEm8qqqKnJwcystbPkuQr68vWVlZNohKmM1mQkNDcXFx0TsU0cEZ3E34T+5FwUdZFG86gU9CmN4hCRszGA1MmDeQr5ancjRVo7ykivKSKgA2LT9I9I1dMRgVETFBOkfasTlEEs/JycHb25uIiIgWF1YoLi7G21vuirU2TdMoKCggJyeHHj166B2OsAPuA4JwHxDI+XVHcY8OxKWT9MQcncGg8OqsuOOhgRzZmc+ejSfoFunPsb0FHLs4xP6bV2/C1d0hUpVNOMSYVXl5OYGBgVIZqQNRShEYGNiq0RHhvPwm9UaZjBSuOiSTwDiR7tGB3Hx3JL/+y43cOKMPAB4+rgCs/3g/Jw8VWp4zFw05zNcbSeAdj/xNREsZfVzxHR9O4eqfKduTj0dMJ71DEu3I7OmC2dOFX/2fEXj5ubH9q2y2f3GEQ2mniRrZhVGzo/QOscNxiJ643goKCoiNjSU2NpbOnTvTrVs3YmNj8fLyYv78+TY5dmxsLJWVlU3un5aWxsMPP9ymGIRoT57DuuDS2ZOiL4+gVcmz487IO8CMMiiGTezBpEdjATiYdppqaQ9XcJieuJ4CAwPJzMwEIDk5GS8vLx5//PF2O3Z1dTUmU+N/yvj4eOLj460SixDtQRkUvnf0JP+93RRvPCG1x51cWL8AJj8ayz9fy2RTyk8k3NNPRvnqkZ64DaWmpjJx4kSgLgHPnTuXhIQEelO2Ra0AACAASURBVPbsyRtvvAHA888/z2uvvWbZ57/+6794/fXXmzz2nDlzePDBBxk+fDhPPPEE27Zt4/rrrycuLo4RI0Zw4MCBZscgREdj7uWHe0wQxanHqS6saHoH4dBC+wUw5LZw9v2Qy8rF6fxryW5Wv5pBRWmV3qHpTnri7Wj//v2sX7+e4uJiIiMjmTdvHnPnzmXatGk8+uij1NbWkpKSwrZt25p1vJycHDZv3ozRaOT8+fNs2rQJk8nEunXreOaZZ1i5cmWzYpBHwERHdOnZ8aKvjxCY1E/vcITOhk/qSW21RsZ3x8g7Urcse3cBvYcEgwKjk84t4HBJfNHavew7eb7Z29fU1GA0Gq+5Tf+uPrxwR3RbQ+P222/Hzc0NNzc3goODycvLIyIigsDAQDIyMsjLyyMuLo7AwMBmHe+uu+6yxF5UVMS9997LwYMHUUpRVdX4N9TGYggNDW3zZxPC2kwBZrxv6kbxf45TcX0X3CJ89Q5J6EgpxYg7exN/ewQurkY+ePJ71n2wj3Uf7KPP0BA8fFzpPSSYzj2dq50451cXnbi5uVleG41GqqvrHpm4//77Wbp0KR988AFz585t9vE8PT0tr5977jluueUW9uzZw9q1a6/6aNfVYhCiI/JOCMPo60bh6p/RauSRMwGuZhPKoIhJCMXsVTeKeHB7Hjv/fZy1b2RSU1Orc4Tty+F64i3tMXeEyV6mTp3K888/T1VVFZ988kmrjlFUVES3bnU1mZcuXWrF6ITQj8HViN8dPSn4KIuSLSfxvkHqjos6Q2/vQfyECM6dKuXTRT8CUFlewzu/S2XigkGED2jeiKa9k554B+Dq6sott9zCjBkzmhzav5onnniCp59+mri4OOldC4dijg7Era8/5787Ss15uclN/EIpRUAXzyuW7/imriJe4elSCk+XUl1VwwUHvUFS2dusSPHx8VpaWlqDZVlZWURFtW4SgI7QE6+trWXw4MF8/vnn9OnTR9dYrK0tfxvRPKmpqSQkJOgdhk1V55dx6rV03KOD5CY3B2ON9puXfZ6sH06yd9NJAEwuBu5bfAPvLdwIQPiAQI7uKWD+/70FZbC/x9OUUumapjX6rLD0xHW2b98+evfuzejRox0ugQthLaYgd7xvDqNs5xnKD53TOxzRwYRE+JAwqx+33h9N3NjuVFfVWhI4wNE9dfOwlxY3PUGWvZEkrrP+/ftz+PBh/vKXv+gdihAdmk9CKMZAc91NblXOdfOSaJ4+8SEMu6MHXgFumL1cuCmxb4P1xQWOV8tBkrgQwi4oFyP+k3tTnV9G0XfZeocjOiiTq5FfvTSCuf9zAzEJobiYf7nPqPisJPEWUUqNV0odUEodUko91cj6B5VSu5VSmUqp75VS/W0ZjxDCvpn7+uM5vDMlm05QcaRI73BEB6WUskzNWlX+y3zruQcLOXHgHBnfHqPGQUZzbJbElVJG4G3gNqA/kNRIkv5E07QYTdNigcXAX20VjxDCMfhO6InR38zZz3+itkKexBDXdv3UXnTt44d/Zw92bzjB6lcz2Py/h1j71k6OZ53VO7w2s2VPfBhwSNO0w5qmVQIpwOT6G2iaVn9qNU/Avm6VF0K0O4ObkYAZfak5V07Rl0f0Dkd0cIPHhTP1scGMnRtNr7hfStueOHCONa9n6hiZddhyspduwPF673OA4ZdvpJT6HfB7wBUY1diBlFIPAA8AhISEkJqa2mC9r68vxcXFrQqypqam1fteUlBQwKRJkwDIy8vDaDQSFBTE4cOHSUxM5NVXX23T8W+//XYWLlzImDFjLMvefvttDh06dNVjT5gwgT/+8Y8MHjy4Teduq/Ly8iv+XsK6SkpKnPJ3HBihYNspDlafoDRY72hEa7Vn+zVHQnCNwmCC07s1aqvgvcf/Q/ebFG4+itoaDYPRvh5B033GNk3T3gbeVkrdDTwL3NvINkuAJVD3nPjlzxRmZWW1+llvazwn7u3tza5duwDrlyIFuOeee1izZg1Tp061LFu9ejWLFy++auxGoxFPT0/dn4E3m83ExcXpGoOjc4bnxBuj3VBL3psZhB6sIuT2IRg9pZCPPWr39nvxVIczz/D1O7upLIGyQ75ET4jgn69lMv3JIYT08Gm/eNrIlsPpJ4Cweu9DLy67mhRgig3jaXfWKkU6ffp0vvzySyor655xzM7O5uTJk9x4443MmzeP+Ph4oqOjeeGFF9rpkwmhP2UyEDAzktrSagpXH8LeJq4S+gofEMj1U3sBcOKnQsvQ+smDhXqG1WK2TOLbgT5KqR5KKVcgEVhTfwOlVP3ZTW4HDtowHt3t37+fb775hm3btrFo0SKqqqqYO3cuH374IYClFOk999zTYL+AgACGDRvG119/DUBKSgozZsxAKcVLL71EWloau3btYsOGDZYRASGcgWtXL3zGhFO2O5+ynWf0DkfYEaPJwOBx4XSPrptj/dJ3wF3rj1NrR0VUbDacrmlatVJqAfANYATe1zRtr1LqRSBN07Q1wAKl1BigCjhHI0PprfLB7Vcui54Cw34DlaXw8V2Wxe411WA0QezdEDcLLhTA8l813Pe+L60SVltKkSYlJZGSksLkyZNJSUnhH//4BwDLly9nyZIlVFdXk5uby759+xg4cKBV4hXCHnjfFEp5VgHnVv+MWw9fjL5uTe8kxEW3z48hc91xtn95hOrKWkrOVfDdB/vw9jfTa3Bwhx9at+k1cU3TvgK+umzZ8/VeP2LL83c0TZUiPXXq1FVLkU6ePJmFCxeyY8cOSktLGTJkCEeOHOGVV15h+/bt+Pv7M2fOnKuWIBXCUSmjwn9GJKdf30Hhmp8JnC3TTYjmMxjreuSDx4Xz7d/3cDDtNIfSTgNwPr+M8b+NsWx7vqAMD29XTK6tK1RlC7rf2GYT1+o5u3o0WF92+Y1tnoFW63k3V3NKkXp5eXHLLbcwd+5ckpKSADh//jyenp74+vqSl5fH119/7ZQ3OAnhEuSO18huFG84Ts35Cow+0hsXLXfr/QMYNqmUo7sLyDlwjtzDRZSer6SyrJraWo1PF/3IoFFh3DCj49S5cMwkbmculSL18/O7ZinSpKQkpk6dSkpKCgCDBg0iLi6Ofv36ERYWxsiRI9srZCE6HM/4EIpTj3Nhx2l8EsKa3kGIRvgFe+A32gM3TxPZu/L54InvARg+uScAR3adkSTuyJKTky2vExISLD3j+ssB9uzZY3ldW1vL1q1b+fzzz6957ClTplxxB+7SpUsb3dYZnxsWzs0U5I5rDx9K0/LwvjnUMu2mEK0RObwzJw8WkvVDLgBHMutunLxQWEltrYahg5Q0lQIoOpNSpEJYj2d8Z6rzy6jMPt/0xkJcg1KKm5MiCY+pu9H49NG6ScFqqmu5UFihZ2gNSBLXmZQiFcJ63GOCUG5GLqTl6R2KcABGk4GJvxuEwVTX6w4K8wJgy6qf9QyrAUniQgiHYXA14jGoE2W7zkhxFGE1M54eyoxnhjJhXt3juwe353FsbwErF6dxUOcvjJLEhRAOxSM+BK2qlrJd+XqHIhxEYDcvOnX3xjvATOR1nQFY++ZOTh0+z7d/38vpo/pdvpEkLoRwKK5h3piC3WVIXdhE9/4BVyz7/E9p/GvJbg78eIqqippG9rIduTtdCOFQlFJ4xnem6KsjVJ0uxSXYQ++QhAPpHR+CMih8O7mz5vVMKkrrLtv8vOMMP+84Q8TAIG6f336zZkpP3EpeeukloqOjGThwILGxsfz4449WOa6XV92NFNnZ2QwYMKDBut27dxMbG0tsbCwBAQH06NGD2NjYBiVLr2XNmjW8/PLLVolTiI7EIy4YDHAhXXrjwroMBkWf+BCCw32Ytei6K9Yf21vQrvFIT9wKtmzZwhdffMGOHTtwc3MjPz/fUnHMlmJiYsjMrKu8M2fOHCZOnMj06dMbbFNdXY3J1PifedKkSZY66EI4EqO3K+Z+gZSm5+F7azjKKP0VYX3u3q506eWLh48rP2fUPUdeW6Pxzd/3MPa+/hjaod1Jy7aC3NxcgoKCLHOjBwUF0bVrVwAiIiJ4+umniY2NJT4+nh07djBu3Dh69erFO++8A0BJSQmjR49m8ODBxMTE8M9//rNN8SQkJPDoo48SHx/P66+/ztq1axk+fDhxcXGMGTOGvLy63snSpUtZsGABUPcl4OGHH2bEiBH07NmTFStWtCkGIfTmGR9CbUkV5QfO6R2KcGDT/jCE8b+NsTx+BpB35Hy7JHCQJG4Vt956K8ePH6dv377Mnz+fDRs2NFjfvXt3MjMzufHGG5kzZw4rVqxg69atlvrfZrOZVatWsWPHDtavX89jjz3W5trIlZWVpKWl8dhjj3HDDTewdetWMjIySExMZPHixY3uk5uby/fff88XX3zBU0891abzC6E3c6Q/Bi8XucFNtIsZzwwlYVYkAH7B7u12XocbTv/ztj+z/+z+Zm9fU1NzzfnKAfoF9OPJYU9edb2Xlxfp6els2rSJ9evXM3PmTF5++WXmzJkDYBmyjomJoaSkBG9vb7y9vXFzc6OwsBBPT0+eeeYZNm7ciMFg4MSJE+Tl5dG5c+dmf47LzZw50/I6JyeHmTNnkpubS2VlJT169Gh0nylTpmAwGOjfv7+lty6EvVJGAx6DQyj5/gQ1xZUYvV31Dkk4MKUU4QMC6Ts8hPjbItrtvNITtxKj0UhCQgKLFi3irbfeYuXKlZZ1l4bZDQZDg3KkBoOB6upqPv74Y86cOUN6ejqZmZmEhIS0uaSop6en5fVDDz3EggUL2L17N+++++5Vj10/traOBAjREXjGh0CtRmnGab1DEU7Ay9/M2Pui8e/s2fTGVuJwPfFr9ZgbU3x5KdJWOHDgAAaDwTL3eWZmJuHh4c3ev6ioiODgYFxcXFi/fj1Hjx5tUzyNHb9bt24ALFu2zKrHFqIjcwn2wLW7NxfSTuF1YzcpiiIcjsMlcT2UlJTw0EMPUVhYiMlkonfv3ixZsqTZ+8+aNYs77riDmJgY4uPj6devn1XjS05O5q677sLf359Ro0Zx5MgRqx5fiI7Mc2hnzq08SOWxYtzCffQORwirUvY2bBofH6+lpaU1WJaVlUVUVFSrjmeNnri4urb8bUTzpKamWkreiivVVlST+9KPuA/sRMD0vnqHIy4j7bdpSql0TdPiG1sn18SFEA7N4GbCfaAURRGOSZK4EMLheQ7tjFYpRVGE45EkLoRweK7dvTEFe3Bh+ym9QxHCqiSJCyEcnlIKz6EhVB4rpirvgt7hCGE1ksSFEE7BIy4YjIoL22UiI+E4JIkLIZyC0csV96gASjNPo9Xa11M5QlyNJHEr0aMUKUDPnj05cOBAg2WPPvoof/7zn696zIiICPLz5QYf4XzcB3aitqSKyqPn9Q5FCKuQJG4F9UuR7tq1i3Xr1hEWFtYu505MTCQlJcXyvra2lhUrVpCYmNgu5xfCnpgjA8BkoGy3fIkVjkGSuBXoWYo0KSmJzz77zPJ+48aNhIeHEx4ezpQpUxgyZAjR0dEtmkFOCEdlcDNijvSndE++DKkLhyBJ3Ar0LEUaExODwWBg586dAKSkpJCUlATA+++/T3p6OmlpabzxxhsUFBRY8VMLYZ88YoKoPV9J5fFivUMRos0ccu70o7N/dcUy79vGE3D33dSWlXH8gd9allfX1HDWaMR36lT8pk2l+tw5Tjz8SIN9w//fh9c8n96lSJOSkkhJSSE6OprVq1ezaNEiAN544w1WrVoFwPHjxzl48CCBgYHNOqYQjsrcLwCMirLd+TKXurB7DpnE9XCpFGlCQgIxMTEsW7bMksRbUorUxcWFiIiIFpUiTUxM5NZbb+Xmm29m4MCBhISEkJqayrp169iyZQseHh4kJCS0ubypEI7AYDZh7utP2e58fG/vIZXNhF1zyCR+rZ6zwd29wfrLC6CY/P2b7HlfTu9SpL169SIoKIinnnqKRx55xHJMf39/PDw82L9/P1u3bm3RMYVwZO4DgijPOktVTgmuYVIASdgvh0zi7a0jlCJNSkriqaeeYtq0aQCMHz+ed955h6ioKCIjI7nuuutafEwhHJV7/0DOGRX5H+7DrZcvbj198RwSgjLKbULCvkgpUilFalNSitT2pJRj65RlFVC64zQV2eepLa4k8J4o3AcE6R2W05H22zQpRSqEEJdxjwokcFYUXZ4cCiYDFdkyAYywP5LEhRBOTZkMuIZ5UZFdpHcoQrSYJHEhhNNzC/el6uQFaitr9A5FiBaRJC6EcHquET5Qq8kEMMLuSBIXQjg9t+51N7dWynVxYWckiQshnJ7BwwVTiAcVUt1M2BlJ4lailOKee+6xvK+urqZTp05MnDixRcdJSEjg0iN0EyZMoLCw0KpxDh8+nNjYWLp3706nTp2IjY0lNjaW7OzsJvc9efIk06dPt2o8QnQUbhE+VB49L4VRhF2RyV6sxNPTkz179lBWVoa7uzvfffcd3bp1a9Mxv/rqKytF94tLdc6XLl1KWloab731VoP11dXVmEyNN4uuXbuyYsUKq8ckREfgGuHLhR9PUXXqAq5dvfQOR4hmkZ64FU2YMIEvv/wSgE8//dRSTQzgwoULzJ07l2HDhhEXF2cpN1pWVkZiYiJRUVFMnTqVsrIyyz4RERHk5+eTnZ3NgAEDLMtfeeUVkpOTgbqe+8KFC4mPjycqKort27czbdo0+vTpw7PPPtusuJOTk5k9ezYjR45k9uzZZGdnc+ONNzJ48GAGDx7M5s2bARrEsXTpUqZNm8b48ePp06cPTzzxROt/cUJ0AJeKoVTKkLqwI9ITt6LExERefPFFJk6cyK5du5g7dy6bNm0C4KWXXmLUqFG8//77FBYWMmzYMMaMGcO7776Lh4cHWVlZ7Nq1i8GDB7f4vK6urqSlpfH6668zefJk0tPTCQgIoFevXixcuLBZlcv27dvH999/j7u7O6WlpXz33XeYzWYOHjxIUlISl8+SB3VzxGdkZODm5kZkZCQPPfQQYWFhLY5fiI7A6O+GwceViuzzeF3fVe9whGgWh0vim5b/RP7xkmZvX1NTg9FovOY2QWFe3Dijb5PHGjhwINnZ2Xz66adMmDChwbpvv/2WNWvW8MorrwBQXl7OsWPH2LhxIw8//LBl/4EDBzY79kvqlzqNjo6mS5cuAPTs2ZPjx483K4lPmjQJd3d3AKqqqliwYAGZmZkYjUZ++umnRvcZPXo0vr6+APTv35+jR49KEhd2SymFW4QPFYeL0Go1lEGqm4mOz+GSuN4mTZrE448/TmpqKgUFBZblmqaxcuVKIiMjW3xMk8lEbW2t5f3lJUWbKnXaHJ6enpbXr776KiEhIezcuZPa2lrMZnOj+9Q/l9FobPa5hOiozFGBlO3Kp/J4sdQaF3bBpklcKTUeeB0wAn/XNO3ly9b/HrgfqAbOAHM1TWtZHc7LNKfHXJ+1C6DMnTsXPz8/YmJiSE1NtSwfN24cb775Jm+++SZKKTIyMoiLi+Omm27ik08+YdSoUezZs4ddu3ZdccyQkBBOnz5NQUEBXl5efPHFF4wfP95qMV+uqKiI0NBQDAYDy5Yto6ZGZrESzsE9KoBzRkXZ7nxJ4sIu2OzGNqWUEXgbuA3oDyQppfpftlkGEK9p2kBgBbDYVvG0l9DQUMvweH3PPfccVVVVDBw4kOjoaJ577jkA5s2bR0lJCVFRUTz//PMMGTLkin1dXFx4/vnnGTZsGGPHjm1VqdKWmD9/PsuWLWPQoEHs37+/QS9dCEdmMJsw9/GnbHc+9lbhUTgnm5UiVUpdDyRrmjbu4vunATRN+9NVto8D3tI0beS1jiulSO2LlCK1PSnlaF0X0vM49/lPdJo/CLfu0hu3NWm/TdOrFGk34Hi99zkXl13Nr4GvbRiPEEI0yb1/IBgVZXvy9Q5FiCZ1iBvblFL3APHAzVdZ/wDwANRdH65/rRnA19eX4uLWFS6oqalp9b6iaeXl5Vf8vYR1lZSUyO/YyroEGKjalkOm+zGQm9RtStpv29gyiZ8A6j9vFHpxWQNKqTHAfwE3a5pW0diBNE1bAiyBuuH0y4desrKyWj0kLsPptmU2m4mLi9M7DIcmw5HWd8HrFOdWHGSIsTfu0YEYA8woJdncFqT9to0th9O3A32UUj2UUq5AIrCm/gYXr4O/C0zSNO20DWMRQohmc48OwtTJnaKvjnDqf9I4+8l+vUMSolE264lrmlatlFoAfEPdI2bva5q2Vyn1IpCmadoa4H8AL+Dzi99yj2maNslWMQkhRHMY3E2E/H4I1WfKKN6YQ2laHtUFZZgC3fUOTYgGbHpNXNO0r4CvLlv2fL3XY2x5fiGEaC2lFC7BHviMDac0PY8LO07jOzZc77CEaEAKoFiJvZQive+++3j33XcbLFu9ejW33XbbVfeZM2eOVC8TTsvk64ZbLz9KM07Ls+Oiw5EkbiX1S5ECVitF6ufnZ43wLJKSkkhJSWmwLCUlpUHFNSFEQx5xwdScLZcKZ6LDkSRuRfZQinT06NHs37+f3NxcS1zr1q1jypQpvPjiiwwdOpQBAwbwwAMPSK9DiIvcBwShXAyUZsj9t6JjkSRuRYmJiaSkpFBeXs6uXbsYPny4Zd2lUqTbtm1j/fr1/OEPf+DChQv87W9/s5QiXbRoEenp6S0+76VSpA8++CCTJ0/m7bffZs+ePSxdurRBERaoK1Ry5513snz5cgDWrl1LQkICPj4+LFiwgO3bt1tGFL744ou2/UKEcBAGNyPuA4Io3ZmPVlXb9A5CtJMOMdmLtX226KkrlkVedyOx426nqqKc/3052bK8proGo8lI9M1jGJAwhtLzRax9teHMsDNfeJnmsJdSpElJSTz++OM88sgjpKSkMHv2bADWr1/P4sWLKS0t5ezZs0RHR3PHHXe0OB4hHJH7wCBKM05TcfQ85t7WvcwlRGs5ZBLXkz2UIh0xYgS5ubns3LmTzZs3W0YP5s+fT1paGmFhYSQnJ19xHiGcmevFedQrc4oliYsOwyGT+LV6zi5u5gbrL5+xzcPHt9k978bYQylSpRQzZ87k3nvv5bbbbsNsNlvugg8KCqKkpIQVK1Ywffr0Vp9DCEdj9HTBGGimKkemaRYdh1wTtzJ7KUWalJTEzp07LTff+fn58Zvf/IYBAwYwbtw4hg4d2uZzCOFoXEO9qTxeoncYQljYrBSprUgpUvsipUhtT+aebj/Fm05Q9OVhujwzHKOPq97hOARpv03TqxSpEEI4FNcwL6DuurgQHYEkcSGEaCaXrl5ggMrjksRFxyBJXAghmsngasQlxFN64qLDkCQuhBAt4BpWd3Obvd1PJByTJHEhhGgB1zBvtPJqqgtkHgWhP0niQgjRAq5hdU+zyHVx0RFIErcSLy8vXc47depUYmNj6d27N76+vsTGxhIbG8vmzZubtf+IESNsHKEQjsUU7IEym6g4ZN0ywUK0hkPO2GZPqqurMZla/2dYtWoVUPes5SuvvHJF0ZKmjt/cZC+EqKMMCnOkP+X7z6LVaiiD0jsk4cSkJ25Da9euZfjw4cTFxTFmzBjy8vIASE5OZvbs2YwcOZLZs2dz5swZxo4dS3R0NPfffz/h4eHk5+cD8NFHHzFs2DBiY2P57W9/S01NTZPnXbp0KZMmTWLUqFGMHj2akpISRo8ezeDBg4mJibGUQYVfRhAuTbgwffp0+vXrx6xZs+TGHSGuwj0qgNoLVTKkLnQnSdyGbrjhBrZu3UpGRgaJiYksXrzYsm7fvn2sW7eOTz/9lEWLFjFq1Cj27t3L9OnTOXbsGFA329lnn33GDz/8QGZmJkajkY8//rhZ596xYwcrVqxgw4YNmM1mVq1axY4dO1i/fj2PPfZYowk6IyOD1157jX379nH48GF++OEH6/wihHAw5r7+YIDyrLN6hyKcnMMNpxeu/ZnKkxeavX1NTTVlxmv/Gly7euJ3R68Wx5KTk8PMmTPJzc2lsrKSHj16WNZNmjQJd3d3AL7//nvLsPj48ePx9/cH4N///jfp6emWeczLysoIDg5u1rnHjh1LQEAAUFdB7ZlnnmHjxo0YDAZOnDhBXl4enTt3brDPsGHDCA0NBSA2Npbs7GxuuOGGFn9uIRydwcMFtwhfyrIK8B0foXc4wok5XBLvSB566CF+//vfM2nSJFJTU0lOTras8/T0bHJ/TdO49957+dOf/tTktperf/yPP/6YM2fOkJ6ejouLCxEREY2WGa1fxtRoNDZaxlQIUcccFUDRl0eoPluOKcCsdzjCSTlcEm9pj9mWBVCKioro1q0bAMuWLbvqdiNHjmT58uU8+eSTfPvtt5w7dw6A0aNHM3nyZBYuXEhwcDBnz56luLiY8PDwFscRHByMi4sL69ev5+jRo63/UEIIAMxRgRR9eYTyrAK8RnbTOxzhpOSauJWUlpYSGhpq+fnrX/9KcnIyd911F0OGDCEoKOiq+77wwgt8++23DBgwgM8//5zOnTvj7e1N//79+eMf/8itt97KwIEDGTt2LLm5uS2ObdasWaSlpRETE8OHH35olVKmQjg7lyB3TCEenP/PcSqOntc7HOGkpBRpByhFWlFRgdFoxGQysWXLFubNm0dmZqauMVmLlCK1PSnlqJ+q06UULNtLdWEFATP64jGoefesiF9I+23atUqROtxwuj06duwYM2bMoLa2FldXV9577z29QxJCNINLsAfBv4sl/4O9FP7zZ9xjOslz46JdSRLvAPr06UNGRobeYQghWsHg4YLn8M6cW3GQ6jOluIQ0fdOqENYi18SFEKKN3CJ8Aag4ItfGRfuSJC6EEG1kDDRj8HahIrtI71CEk5EkLoQQbaSUwq2HL5VHimS6YtGuJIkLIYQVuEX4UlNUSc25Cr1DEU5EkriV6FWKdNGiRTz99NMNlmVmZl7zsa7k5GReeeUVW4cmhFNxjfABkCF1JFkyUAAACH9JREFU0a4kieusrVObJiUl8dlnnzVYlpKSQlJSUpuOK4RoGZfOniizicpsublNtB9J4jbUHqVI+/bti7+/Pz/++KNl2fLly0lKSuK9995j6NChDBo0iDvvvJPS0tL2+/BCOBllULhF+FBxRHriov1IEreh9ipFmpSUREpKCgBbt24lICCAPn36MG3aNLZv387OnTuJioriH//4R/t8cCGclLmvP9Vnyqg8UaJ3KMJJOORkLx988MEVy6Kjoxk2bBiVlZUNEmFNTQ1Go5HY2Fji4uK4cOECy5cvb7Dvfffd16o42qsU6cyZMxkxYgR/+ctfGgyl79mzh2effZbCwkJKSkoYN25cqz6HEKJ5PGI7UfjVES5sy8V1ah+9wxFOwCGTeEfRXqVIw8LC6NGjBxs2bGDlypVs2bIFgDlz5rB69WoGDRrE0qVLSU1NbcvHEUI0weDhgsfAIEozz+A7oScGN6PeIQkH55BJ/Fo9Z1dX1wbrLy+A4unp2eqe9+XasxRpUlISCxcupGfPnoSGhlo+W5cuXaiqquLjjz+2xCKEsB3P4V0o3XGasp1n8BzWWe9whIOTa+JWoncp0rvuuou9e/c2uCv9v//7vxk+fDgjR46U8qNCtBPX7t6YQjwo2dbyssFCtJSUIpVSpDYlpUhtT0o5djzFm3Io+vIInZ8YiinArHc4HZq036ZJKdIOTkqRCuFYzFGBFH15hPIDZ/G6vqve4QgHJkm8A5BSpEI4Fpcgd0yBZsr3SxIXtiXXxIUQwgbM/QIo/7mI2sqapjcWopUcpieuaRpKKb3DEPXY2/0WQliTOTKAkh9OUnG4iIqD5yjdeQa3nr54XdcVt56+eocnHIRD9MTNZjMFBQWSNDoQTdMoKCjAbJabeoRzcuvpi3IxULj6ECU/nMQU5E7Fz0UUfLQPraZW7/CEg3CInnhoaCg5OTmcOXOmxfuWl5dLorERs9lseWZdCGejTAbcevtRnnUW90GdCJgZSfm+Ago+yqLicBHmPv56hygcgE2TuFJqPPA6YAT+rmnay5etvwl4DRgIJGqatqI153FxcWkwpWlLpKamEhcX16p9hRDiWrxvCcPkb8b3th51BVL6+qNcDJTtyZckLqzCZsPpSikj8DZwG9AfSFJK9b9ss2PAHOATW8UhhBB6cevug9+kXiiXuv9qDa5GzP0CKNtbgFYrl/9E29nymvgw4JCmaYc1TasEUoDJ9TfQNC1b07RdgFwgEkI4BfeYIGpLqqjMlpKlou1smcS7Acfrvc+5uEwIIZyWOTIATAbK9hToHYpwAHZxY5tS6gHggYtvS5RSBy6+9gWa+jrb1DZBQH7bIuyQmvO7sbdzW+O4rT1GS/dr7vbShhsn7de6x5D22/6s2Y6urHp1iaZpNvkBrge+qff+aeDpq2y7FJjeinMsaes2QJqtfgd6/jTnd2Nv57bGcVt7jJbu19ztpQ3b7m/d0c4t7feq6x2u/Vrr792cH1sOp28H+iileiilXIFEYI2Vz7HWSts4Ij0/t63ObY3jtvYYLd2vudtLG26ctF/rHkPab/trl89t0ypmSqkJ1D1CZgTe1zTtJaXUi9R981qjlBoKrAL8gXLglKZp0TYLqPEY07SrVIcRwh5IGxb2TNpv29hdKVJrU0o9oGnaEr3jEKK1pA0Leybtt22cPokLIYQQ9soh5k4XQgghnJEkcSGEEMJOSRIXQggh7JQk8csopTyVUsuUUu8ppWbpHY8QLaWU6qmU+odSqlUFhYTQk1JqysX/fz9TSt2qdzwdnVMkcaXU+0qp00qpPZctH6+UOqCUOqSUeuri4mnACk3TfgNMavdghWhES9qwVlev4Nf6RCrElVrYfldf/P/3QWCmHvHaE6dI4tTNCDe+/oJrVFkL5Zc532vaMUYhrmUpzW/DQnQ0S2l5+3324npxDU6RxDVN2wicvWzx1aqs5VCXyMFJfj+i42thGxaiQ2lJ+1V1/gx8rWnajvaO1d44c5K6WpW1/wXuVEr9DeedLlDYh0bbsFIqUCn1DhCnlHpan9CEaNLV/g9+CBgDTFdKPahHYPbELqqYtSdN0y4A9+kdhxCtpWlaAXXXE4WwO5qmvQG8oXcc9sKZe+IngLB670MvLhPCXkgbFvZM2q8VOHMSb48qa0LYkrRhYc+k/VqBUyRxpdSnwBYgUimVo5T6taZp1cAC4BsgC1iuadpePeMU4mqkDQt7Ju3XdqQAihBCCGGnnKInLoQQQjgiSeJCCCGEnZIkLoQQQtgpSeJCCCGEnZIkLoQQQtgpSeJCCCGEnZIkLoSTUUrVKKUy6/081fRezT52xOXlJoUQtiNzpwvhfMo0TYvVOwghRNtJT1wIAYBSKlsptVgptVsptU0p1fvi8gil1H+UUruUUv9WSnW/uDxEKbVKKbXz4s+Ii4cyKqXeU0rtVUp9q5Ry1+1DCeHgJIkL4XzcLxtOn1lvXZGmaTHAW8BrF5e9CSzTNG0g8DG/VJh6A9igadogYDBwacrMPsDbmqZFA4XAnTb+PEI4LZl2VQgno5Qq0TTNq5Hl2cAoTdMOK6VcgFOapgUqpfKBLpqmVV1cnqtpWpBS6gwQqmlaRb1jRADfaZrW5+L7JwEXTdP+aPtPJoTzkZ64EKI+7SqvW6Ki3usa5N4bIWxGkrgQor6Z9f7dcvH1ZurKRALMAjZdfP1vYB6AUsqolPJtryCFEHXkG7IQzsddKZVZ7/2/NE279JiZv1JqF3W96aSLyx4CPlBK/QE4A9x3cfkjwBKl1K+p63HPA3JtHr0QwkKuiQshAMs18XhN0/L1jkUI0TwynC6EEELYKemJCyGEEHZKeuJCCCGEnZIkLoQQQtgpSeJCCCGEnZIkLoQQQtgpSeJCCCGEnZIkLoQQQtip/w+xC/2pydlNswAAAABJRU5ErkJggg==\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5PsEdLZgaDyT"
      },
      "source": [
        "# Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sV0p4_yBaF92",
        "outputId": "5c8e4e8e-f173-438b-8903-a30d1e9f1e7d"
      },
      "source": [
        "tiny_loss, tiny_acc = tiny_model.evaluate(preprocessing_input.transform(X_test), y_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "63/63 [==============================] - 0s 2ms/step - loss: 0.4547 - accuracy: 0.7935\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oqY8jjYzPUgS",
        "outputId": "657db5c2-d613-43ec-a5a0-42dc79a49672"
      },
      "source": [
        "small_loss, small_acc = small_model.evaluate(preprocessing_input.transform(X_test), y_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "63/63 [==============================] - 0s 2ms/step - loss: 0.4534 - accuracy: 0.7915\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AYKfKUGVPYuF",
        "outputId": "b2f6216e-ca5b-4142-a2be-27f42d6c496b"
      },
      "source": [
        "medium_loss, medium_acc = medium_model.evaluate(preprocessing_input.transform(X_test), y_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "63/63 [==============================] - 0s 2ms/step - loss: 0.6601 - accuracy: 0.7550\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9aoiZXW9PalG",
        "outputId": "ab90b426-5831-4699-e40e-755b0b008084"
      },
      "source": [
        "large_loss, large_acc = large_model.evaluate(preprocessing_input.transform(X_test), y_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "63/63 [==============================] - 0s 2ms/step - loss: 3.1414 - accuracy: 0.7370\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QwWNwsd3Pw9n"
      },
      "source": [
        "# Preventing Overfitting\n",
        "\n",
        "As we have seen, overfitting happens when the model is too complex for the data. That is, when a simpler model exists that can accurately model the data, the more complex model is likely to overfit to the training samples.\n",
        "\n",
        "To prevent this from happening, we can apply one or more regularization techniques. **Regularization** refers to modifications to the learning algorithm such that the model generalizes better. The idea is to penalize complexity so the model is forced to learn the simplest possible representation of the training data.\n",
        "\n",
        "Multiple regularization techniques exist, such as:\n",
        "1. Weight regularization\n",
        "    1. L1\n",
        "    2. L2\n",
        "2. Dropout\n",
        "3. Early stopping\n",
        "4. Data augmentation\n",
        "5. Noise injection\n",
        "\n",
        "In this lab, we will explore the first three."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W9fzCoUbS6Ue"
      },
      "source": [
        "## Weight Regularization\n",
        "\n",
        "One way to penalize complexity is to penalize large weights. This is done by adding all our parameters (weights) to our loss function. The new loss function thus becomes\n",
        "\n",
        "$ J(w) + \\frac{\\lambda}{2} \\sum_{j=1}^{M}\\left|w_{j}\\right|^{q}$\n",
        "\n",
        ">- When $q=1$, we call it **L1 regularization** (we are adding what is called the \"L1 norm\" of the weights).\n",
        "- When $q=2$, we call it **L2 regularization** (we are adding what is called the \"L2 norm\" of the weights).\n",
        "- (Q) What happens when the parameter $\\lambda$ is increased?"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "![image.png](data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAacAAACoCAYAAABEziH5AAAABHNCSVQICAgIfAhkiAAAIABJREFUeF7svQecHMWZ/13KoIhyBO1KKCGByAokITA52WDAGNsYc87nfGcb2+f0vzt/fHe+w8Zn+7XPCRsTTM45JyFACREkUBYKKOeE3t/36aremt6Z3dmkXYkuaXZmerorPFX15OepVrvf273b5SWHQA6BHAI5BHIItCAItG5Bfcm7kkMgh0AOgRwCOQQMAjlxyhdCDoEcAjkEcgi0OAjkxKnFTUneoRwCOQRyCOQQyIlTvgZyCOQQyCGQQ6DFQSAnTi1uSvIO5RDIIZBDIIdATpzyNZBDIIdADoEcAi0OAjlxanFTkncoh0AOgRwCOQRy4pSvgRwCOQRyCOQQaHEQyIlTi5uSvEM5BHII5BDIIZATp3wN5BDIIZBDIIdAi4NATpxa3JTkHcohkEMgh0AOgZw45Wsgh0AOgRwCOQRaHARy4tTipiTvUA6BHAI5BHII5MQpXwM5BHII5BDIIdDiIJATpxY3JXmHcgjkEMghkEMgJ075GsghkEMgh0AOgRYHgZw4tbgpyTuUQyCHQA6BHAI5ccrXQA6BHAI5BHIItDgI5MSpxU1J3qEcAjkEcgjkEMiJU74GcgjkEMghkEOgxUEgJ04tbkryDuUQyCGQQyCHQE6c8jWQQyCHQA6BHAItDgI5cWpxU5J3KIdADoEcAjkEcuKUr4EcAjkEcgjkEGhxEMiJU4ubkrxDOQRyCOQQyCGQE6d8DeQQyCGQQyCHQIuDQE6cWtyU5B3KIZBDIIdADoGcOOVrIIdADoEcAjkEWhwEcuLU4qYk71AOgRwCOQRyCOTEKV8DOQRyCOQQyCHQ4iDQtsX1KO9Q+RBoVcutu2v5fV/7uTZ4MN4cJoWz/n6Dx7625vfh8eTEaW+Y3CzSBaFkrxUbR7iH+8MzXNtXEFI5MMjCJfvMvgKL2saZ/T18r2k9lLvOStWdX88h0AAItNr93u59dXs2ACwt8NEiSHXlipXuzTffdEuWLHEbNmxwu3btcvvtt5/r3bu3GzJkiDvwwANdx84dqwbznv+YrasFDresLmXGsWvHLoPF3Llz3fLly93mTZtcq9atXefOnd2Bgwa5g4cNM9i4oMwO8Ajf94WdEMFk+9ZtbvHiJe6tt95yK1YIHpu3uFb6vWvXbm7gwIG2Rvr06ePatGtTCO6YKO0LMClrMeU3tTQI5JJTS5uRUv3xCGPnth3ulWnT3AMPPOCmTJkipLPCvffee4Z0gjjF944dO7oRI0a4yZMnu0mTJrl+/fuVJ22Var8FX9+wboN7/vnn3UMPPeRmzJjh1q1bZzAJZdeuna5t27auf/8B7thjj3VnnnmGO+ywsa51W1El4LqvIGBPmN5d8a576qmn3COPPOLeeON1MS4bDRS7xYe2ShaKrZcePXq6I444wp1xxhnumGOOdh32368KFvsKTFrwus27VjMEcsmpZvi0nF+FTNatWed+9atfuZtvvsntv39Hd/bZZ7vx48e7AQMGmHQA4tm2bZtbs2aNkNIb7v7773evvPKKO/TQQ90Xv/hFN2HihIRA7QuIxyPiuW/Odf/7v/9riBhJ4KyzzhLhOcz16tXLCDQIeePGjW7p0qXu2WeecQ88+KDbsWOHu+yyy9zll1/uDuhxwD5FnF556RWDx/PPPedGHXKIER7mH3ggVVOQspctW+ZeeuklY3LefXelO/fc89xnP/tZ17d/3yqCnZFMW85myHvyfoBATpyaY5bZ9OUSCI8g5rwxx/34xz9206dPd5deeqn75Cc/Was0tHP7TkPav/71r92qVavcVVdd5S666KJE1Ydg0ZzIpy4wyM6RlyKffeZZ97Of/ZeQ6yp3xRWfMATbs3fP7N0F39+a85b705/+5O677z43ceJE99WvftVVDKlwrqWoPOM5KXeNaISo8O64806b6/bt2xuhOf2009x+nfavER4rl68Us3Oz+/Of/+wGSdX3zW99yx0z7hgPD3XAS1o1VpL/mEOgCSDQ5gff/8EPmqDevMpiEBDikY1PdhB9AAmFV4l77bLuWbp4qfve977nVq5c6b4l5HHZZR9x3Xt2r5KCgmoqvHsE17pNa3fw0IPdUUcdZdzxI4886jp06OBGjRjpWmftDMX60NTXPPI1tWSbCCZxu54QFXRF2rjnnn3O/eQnP9F49nNXX321pMhzXKcunQp7HJB7QPj63qNnD3f0UUe77t0PcE8++ZSbP3++GzN6jOvSrUtTj7b2+tVP7GbMm5V4fRSDg7/nvV3vubvvucf99re/NVXut7/9bXf88ce7dvu1r2KCIhikdetDp86d3FipOA8++GA3Tepi1IGjRo7yEpSfk9p7nt+RQ6DRIZATp0YHaQ0VCsHMmDHd3XzTzW7ggIGua7euyc0xEgqPB2Sk95///Oduzpw33de+9nVT07TrECEd7g/Px++hHl3r2aunGz5shDkKvPzyS5K4+rvBBw1ufsmJPgoP33jDjW7qiy+6vr37ui5dRSSKwSMgV72vWbXGXXvttW7r1q1SV37BELERt7gAvxihR5877NfBDTt4mNmlnnzyCXs/fOzhrk3bjGNAYY1N/01DmPf2PPdnSXZrpcLt1VOqOOxA8dACbBiPp2GzX53t/vjHP8gB5iBT344aPSqxLcXjr6H3jLticIU5R7zwwgtu3rx57thjjnX71yJ11VBl/lMOgQZDICdODQZhGRVEiPXll152n/vc54QEnnerV612feQ9BjdvJdwXkIq+PyXu/je/+Y0755xz3HnnnSejdYfyVYKha6qn2wHdXPt27d1zskVgkxozZozr3KVzGZ1vwlv8eP/0xz9JPfczOTU85zas3+AGiHga4Y7glvZC1+6/9z73xJNPugsvvNCdeuqprm17+fVkCVExeEZDade+nevdq7d7W8QAu9zQoUPcwEEDkzsCAWjCoRetWu3Onzff/cdP/8PdeNNNbtasWQJBK1sjHTvJ6zLAIzys75s2bDI15YIFC90lF18sx4ZjCglT9plsw36sSPMQw82bN7uHH37IHdDtAK2RQ6u3mX0+/55DoIkgEJxom6j6PVhtKYQC0mruEhGbNm3ayGlhq6mUfiCN6qWXfsRd89/XmOrOCuNgVniXHQR7QKdOndwHPvCB+hMTDwOcJ3i9/fbb7vXXX29uqKTt40m3bt1a96CQ4ne+c7WIzkXuV7/8lVu8cHFyT5hbvW/bss09/sQTcgnvZZ5m+3VMjPwFhD07shIIetBBgwyulClTXnTv7RTAuRd4NdO6wamF6I75C+a7G2+80X3pS18yaei+e+5z69eurxqZHxOOLy9K6hw5cqQ7bOzY6urR2sYRjbVz187y7jxZdY1yjz3+uFu7dm0CjwC/EnDMgjv/nkOgMSCw7xCnsAkDIgvQiTdW+K2pNlnY6Fnklmmvdes2ojvvuQ2bNrhXXn7Z7EiXXnqJ+82vfuPmvPFmapxfsGCBERLcn/v1kys49VZ5SNdt/vUcahoQeidx4dSNfcOQcVPDpVRP/ZwlLs7JUly/fr2b8uIU941vfN1deeWV7ne//Z2b++actIbXXnvNLVq0yI0adYipoazUhoCLte+fwdZSUVGheKDF8mKLkH+xZ/bAtdaKy4JAtdZrx64d8jJc4m655Vb36U9/Wmrdr7k7b7/TrVi2Iu3Jq6++6tq1a2seij2DBJ5df+X028NjgNTNY0XkNmgeiBmzEq/rcurK78kh0AgQ2DfinIoRm2LXYoDV9nt9gFtGnSAekHFr/dtt/yQNbN/mnpWBH8RL/A2qqo997HK3evVqt2nTRkMWuIqbRFUfRMxYPBuC23k3qWxCLFAbl7GzlDGG+oCmpmcCQkaFxfB437x1i3v4kYfMSA9BPf300839+5133jHV04EHDjKJ0j+QVF8X2Phx4hgBTBYuXCgb1jbXjZqaAQYBPsBCC4T/VnjbqTitxUsXu+uuu8498cTjYlbGuQvOP9+deNJJ5iTTvXsPN3jwQYktMlRUzzHsJ3vcIKk3W0vCZ43kJYdAc0Fg7ydO2oS7d+22jACblBFg9+5EtLDNHXZ4DF3btLXv3Lp50AqdplWGz9XbgDDRTyNQ+pcg4+T9PfV7lYjRE1JZoabBBZwo/iVLlprU1K5du0ZZI127drW6QMYQw/3331+ZJXYaTOo25oZ1JxBpvO0gwnEBNm1atbG5XLlqpXv00UdSmND3FYJhp06dE5gA5roQpUy3iRfr0qWLBTMjhSC5kWljT8IidKl9+w6SlN9yW6X2TdiWqs6yVrbv3O7m6veFCxe5p59+2uxLxCuNHj3aAmqtNAAWPN6mTVsxL5Do3WabtBKWcgNhnVSW/80hUB4EGoc4sWhD3EyMkxu4UcobgrMgS4zCeBrhSltRUeHgQEFuxehTut9KYqAqztWG4+8L0fXZfiEJUcJ7CoLk4YRIqg6enz17ttu5E2JQVXYLeAFUu3bvMnUfXmTt5MDAM9hj8ChD2qp3oQH1Z/v27Rbz9Oijj5oUgsqQYNX33pOKTyXAqyRo6t2B7IMJpoM4oVIDJoFYJ/1IiDZ37VTf1goGDz/8sJBna5MU8NSDiDTUww54EJRKhgkQ/amnnuL69u1bkGEi2/OGfC+1hqizbdt2pm7duHGT1HqtZXsq1OECHwpEatHihRYesEVwIMAWabIxCmsYCXKVnHUel90JteeIUSOSqvfQfm6MceR17P0QaBzixKIthjcDlub3sLCL3dfARU+MClzk1KlTDfGiFjviiMMVjNjBCEYgGtWnq0TDuhz/UoWoU7JTraoE1fo97NtMbCnJ2PGGAvGDYHGDbrUb6Smh6MV6sUPSDC8Q0owZM90JJ5zourRveCzOciHgLVu2iNPuYZImUgOZJpCoQPbVCGy1kTbOBZA0hn8CRgksRjpKaDbQSCTLRMFXNRc7du7Qb+2U4WG7WybCCoFqH8fy1KVrSTNGmFgzSAswNDt37rJ0T9hegAel9Popv8GaiFKohRg01vA999zt3pKEFKTrbCuBSEFEuAfizgsPzDQlU/ahMr9DrFkjzMsg5SN86KEHbZ0cefSRSQ2BCfXwK7Pa/LYcAnWGQOMQJzVLNgLiI15Uvrc18vLpL3fgCRMmuP4D+1dhF2Fw7iPW55VXpgm5bHFHHnmUm3Cc0uoUw9DlDMdvkhNPPNG4TqSnJ+VmDGIZJ6mgG+lpyikF1Mg/UOxaOXVxT5Fn8T7DbXnneztFyxP7SrGBg3z4h9NEGzlPEBiJ23SDAkUF+x3Kyzdj5kwbwYc//GGzKaBChECdfPLJtWZXKHfoZd/n5460Oq0kKSSligHIqraAFVIeqickUIhK1wN8rFjZjRY2gxptsRwsTjklkZgglKT0Oe20093QYUOTm7OIuD5rtTRfU9BzbIs4zJRTEvVna3NcAB7HHXec69bdrGbllZh59J9RO8+UCzvOJjAtSNeoEGFoyKhRLZ6svJbyu3II1BkCDSdOWtS7duyUQf9Z98tf/tKIA0Snd+8+ZsD++te/7toTNOoJ02233eZ+8YtfWEAoEsk3v/mthDjVuev+ASEKsi4Q60KSU1R8IDAi/3GFJUBzwKAB9as9RijlIKR4s4cWozoYL8SJkuC7ICFU715AzLh+M5ZnlBcOo3d7GazTCqo/VuMV4maeFqFDDUTcFNz2TBEr8s6RzgcCNfBAH+sTOlljjQ34UbB4TwzE448/IeL7pOW7K1WCpADMmFs80+aLEcFmBkxaEzxbzvxkGiB1D1kz1opIf/Tyj5rdBvXpli1bTe2JFEFAK2s3rb8e7RQ8X3qQ1gbrA/ViIjWVujm5zj19+vS1fr/00lQjHhOPn1jY35qrSH71a3Tr5q0WB0ddpLkaOnSoSU9IUVOmvGDMALkLLa6s2Fovp638nhwCZUKgmJKtzEd1mxYozghTp75kySZBctgwMLK/OvtVd8cdd5g0EzYnC/8nP/l3Q4gbN5J8crkbPnx4+e2VuDOoTA466CCL9yD5KbpyVDa33367mzl9ZuI2zfMgl2KvYnXH9xX7PXst3B9fj5BZv379xZGfZr+CWLArBMRbvSqcAlobMoAjxkHiVWUCSJEkD4Ag4lexdv3vZKq+Rylu8AyEiI8aNUqJUiuNUGFjwdEAiWHeW/OqagnjKdVGttN1/A5RIcZphY7+wG06GVLAesUr435gyNEXjMdgkjxYhTCLfY/mAZghwSORksWcNQsTQOJY4IL3Hi8842ZMm2FrPEXG7Jgs3Gv7Xnwo1a+qHsYXCHXc5eo3J1f69e3nzpfnHtLWY489VhgvV6xfpSqSug7p6wElCz5I2UOQHJGSyFBx5JFHStV5soU13H333Rb4a4UONtHaKNXN/Pr7BwINyxChhblVXCZqtDlz5ljmg0suucStW7vOvfDiC26nJKqxYw9zY5QV+SURMBKXvvPOMovZOPHEE+SF1t994hOfqH9wKfMUbQ7sOj1kLIcgoifH7oSRGQSERNVXXGa7Do3j9VanJeKxDDYNiDLpZnYrH9pGuYknQwiKvOQ91N1RnmQf+chHzAaCwZ7UQ32lbhk0cFCSn48bA4LIdijARdeXLV3m/vKXv1hWipNOmmTqGvLQUT9cOt5yZK7eIUkBlU47waxvP2WnjurIVt8o31U/aZyIW5ovlTBzVlvpfkB3S3wLE4I0yXlWByltT5++PuapVAXROtmxbbvZ/ghyhch9/OMfk8Q4yHWQjXKb7Fgg4QED+pvLPa7s5ro+6MCqnHel2mjodfVxu/rG+sUjcasCjmsr9Is1gkcnxBa1HKmpyAhSrQRql51XXWdt/eEPfzTpiKTCRx1zVMrEcd5Tj+7dBY+utnbx9KRdS61Uav1Vazy/kEOgbhBoMHECnaInR9WCWgFd9Zq1a9zjjz1ubrlwoCTa/OlPf2rI5Mtf/rL7zGc+Y7EaJ5xwvLj2fmaIbpSiTbd/x/1dx/32l0H5bTNyj1W/MDTjJgwi7s1RCkp2mXLCjdJwLZWADHzBdRmPs+nTZ7jVa1arG4UEKbktwSJdu3R1F1xwgQjKSSbhPKMs3MRDISkO0NlEjLVgHBEC5jrSwStKl0RCUNStJxx/grtItqZ+A5KAXqSl/by3HNz6OEkPIGJgRRBofzEPTQonDRMDPnndOABv5swZtXqd9ZG6+NxzzxVzc6JJ6EH62V9zDvNhDhIZOKTf1d47S99xt/z9FvfXv/7VdZMTyCeuuMLiqNAxw9x06tjJEDSH9J2kNtoLRi8pUBq4VAjptwpJWWuZ8nr9rH536dzFHBtQpb2tQwJrI9gQIlS0SNdoClBHQrC7CZ7Ao8CbMUuU1EmOYXnwgQe1Rn5nTMpHP/pRs7+1VWCvwc0THzKbsAb7SMrGtgzDR9uWVqnQqbBeQ88fyiGQhUDDiJNqI4My0goxOSAGECceTyANAgfZ7CBjbE3khkO64lgDuC64O1RbjYoAtaHoD8Z+jLk4ZuBejtE/ybgwz+0nDhkOPM2EnYVKU3z3iIHEptvlmIAKbdW775p6r7pqj5uVsFW2hA996EMmJQwePNhOtoXAIqm+LIS5VDFQSK64AQJHJNX169a7+W/Pk6T1grv11lvdbVJrgljPOedsd8EHL3CDVEcKbzUDAcTrDekF7vsQnQG0efMmI54YwZFs0izZTQQX5gKjPvafdRvWFYFHVcOcaIvkN3LUSCNqPXv21Emvc6WCe9LUxat0fAY2UIMpbtGSPlYsVwzTrFftfKublLNumuw6hxwyShLHZe7oo48uOAmWpLBIjtjjIN6HH364MU+zpaZG/YjU2pTSN4wUalakt2dkx127fq2nraiAq5fKikrzTj1k9CGmpoVRRD3H/ntN72tWr3G7tB8DE7RNHn5IZbNffc09IunxBiXdJWwBRuTyj31MdsdJyiTiT08OkpZvFsJ8wAEHuJ7aX4sEH5xHyMdnZ2IFwlesk9W7nV/JIVArBBruEKEmQPIJMuCLk876ILMlvfTKS8ZhgeRQTZEnrHdfHZMdF4SmzCawn7OLvNg9hTWl3+jP4cqqgL1p+vRp5jYNgu8qAjVVB6zdL8JwuJD8BEl6ZtxtypJRe3BqK5wtEhFIhaj/1WsTCaqqG8lgUaNAVEGOSBgTJow3AzVIAQJ11113mV0Pb7ckSHe3eSluk2oId+teSmyKNIs9hfkAsRhcI1hSb2VlpdkLcVLB7jJ+/AQ7ioL8ezgFnHjCCa59fRLOlgNXcd30gXHRR9SvIGRKdaLtLDwAuwxjQDUJDOkzB+dh00QighkCyePVhxs4UiH5DIkbow2yTUB0Bg4cIMLk5z/ARPCBocHRAocLiBhhCswDbeDmfc4551Zl7K7DuqwVHH7Ns37ZQxUVFSY5rZGEjedm8nOy1/hOARamedCPvaXaRIpirSMp44mJ1MfaSOBBYHMCD/YkjGSl2sBz8yhpN2gvJbwRPNJ++7UMg0NOQhwn2GPYAA8Zc0itw8tvyCFQFwg0WHKq1pg2Ce7PIDYyauO5RyJJvPZGHjKyOCGKK2EHJrvQvStvKoIBsY2gViir8Kw2ESofNu1rr71umxCE002palB1gMBBxKhvugthpxmffbtltVPmTUmskyrW/7k6MBCCsl0OGxcrgzSOCR3Vz1kzZ5m0UlWSjgwdMtR9UESMvvM8EgwEBokBG9FocctIHe2kAmKMEBQIMZIiCAP1F+oeiE+1rNbRWFGJIV3BrYPE8NiDI6bg4bdCKXKAmyVZbSwYhXqEV5fLDom3J04s9BtOH6kwG6xMfzif6swzz0wyiKsOkDPqJjzLUIcF6QEHAdYMKY6AH2pkJHcIk+Whk/SeSoQxgVGdIHM8QFkjEOchBw8xKZY19brWE1IDjggFMG0MuPg6lixaYvFOEFKYC5gNpJ0dPlCZ20KXR+geCEXIqB4kL55lnENs7tFeJPDoKFUwqnbO+DpP6+MsSaGEc/Tr3y9RAZZDbNUBMtqT1R0b6hSFjyBtomq29eGJWNV6zj/lEKg7BBpfbNDChCiAaFAxwPkNHTrE1EUlC4s5MjttXL/R1BKoYXr06C5V4Oddn34yeJezccI9qo82V6xYbq7YqB3JQs0hfcRfQZReldoDVRLeWnYaamOVaIPCBRNfBFePxINq7hipkjhTCUPzp3Q67TRJd7feels1jzUCY/eX63QBQVDdEImDKg4Sd32g7CVHmmQaPLyAfYcO7S29j9lfQikFO12nHyD3hQsXmCszsEJVQ04/cq1xjWBMpJTuIbloY8BKY1m0YJHNM2uFs6pQ0yHVEKiMVLhxc+I0EuLCQL68YpgAYzJqD+syzLh/CDLu4HaIoYg2iBkXdBLeFtiMYph4wmDD0mdUa0hkqMioE3iD8IEvEgmegqgXazt5t65gek3qNtY+c3+GCGkXveMtx5leFhC7ZXPBNoAIm+RE8eNBEmX+eA0bdrA7QbYzGKJdggfMEgQdadtiqnRvvYpg1F1OEjjYsNZY3zBYJ046KdnL9IVXDNd6NZQ/9H6FQOMTJy1GuE3URKhSKCAKkOd++ldT4TgEjPGoCtj8cK44TVxxxZaaHiv+mzYGKopDDz3MVGCo0pAqOnbuaCqZQ8VVHqDNhaoGF9xjNx1rOcqMSJZC5MVbKn7Vb0oMzuj031RmbYgiEg/BtOaerGJSnfT9qGM6y1HjXdmh2Ohr168z5JFFxGljPC4DPkiZV9FS0zgC4vD9BDZk+iaImmMYxo0fZ0SQ/uLEgZoIN+JTT/1AYWB10YbLu0gG9nt1NhP2tHHjxiUSouA/XupLvOTwICMLAjAi9x2EKpEASiNU5rxbhwMkJZfoQ4AJ4+YVvgd4+MdYJ7iVY3tijaB2hIgPGzbMEDuSHt5+uHEXxIaVaLacy2++/qblVsT2hyrWMtFrqKhVWRMcOEmYBAHL2B5Jc9VaawD4FJRo3tvq3CqzCdVWalor2WcjuBEEfdxxE22tso9IvYR0CqwMthm4ZqvKv+cQKAWBxiVOWrQQGNx077zzDtPxb9u+MXGNFXIpmuHAL/Tpr0w3goSeH3fYJUIK6zeuL6raKTWYYtfZ4HDA2J6wG6DOgFvE1gRHDPInYwWqCdrFQJ5mHajLhs02LpMAajK4YNSIBLgiidim1W/mjKF38pcdceQR7lK5A7PBUa/dcsstBov+UrWACAtK3KeG9I9K4+eFBCulAsKJBEkTuwXZBohFgvvGewwC9eCDD4phOMFUXSnyoa4Mfkz7HBOAcI/aJX6IzANIkieddGKBRLZo4SKzmV155adMLQeHHuw9qPBQX1aDSVR34Y8lvhWDXXxN9eF5CsOC3RRiQDA3qkDsQUgf9P/++++z1FLDRypeLx5riWarIWue0TpAGuNF3TAxsUT2upgFPAs/9amrzAMVKZD1imceat1U5V2s/cyYqjFexZ4p1ffsdf9sBzk34fGIBAfRRh17qjz+OsomaCX0oSFtZdvOv+/zEKi/zSlGBnzWC2mATUMGCNPViyjMnz/PdPUE8R0kI7Nt4PACvP7z9X+93lzN0bGffvppZueYI2kDThXdeNG4jXKmR0iXUz2XLXtH0tw7Jj1ZXRAIcZ0QLLJZUIjVAjnj7WenxIYx1tZOvPk8HOD8QeSoZ3B7RiUEYQJGRphU8CAD6aLOghigSkKlxme49kMPHSN1mwiD1HR7omA3oOA5B4IZpNgfEAtwAobAjmSjjA07FTFCNhbGXwpW8XX/+TkhMIg2MCGDR6wq3L51u3l2EmJwxhmnm6EeWxL2k4MPHmbEAviAoJu6QIj2E/MC04JEO2z4MBsnkgpejszXWsX0oa4FdthtUjiUgkkWHlqHEH1Up9iPYI56igCHelBxw8DAZOHiTeweEi5wwLbIM8CRddacBQIJPOgHqlC0HsSfVQt3aM5O5m3vVRCoH3GKNxgaFvCTkC4qud/+9v+TpLTQXSVbCogWpwgMuRCdMUK2FBAQLq0clW1Fz2/ZvMVsDRjxhw0AnTpQAAAgAElEQVQbbhsWd2lsIeeee05CUEohwKSWkn9R9XQWB06/UC+iRkuP9tZTqK8w+ENEcTcnjgN3c9LCgJjLKtwmhLR2zVpTweBIwPiRmALHHQgi4129cpVJSCAYEBJxJaaeETw5ohzDP6okkHDah5gLLqtTdbxJzSOpEZ+Gag+jv0m7tKt+QchxDCAe5rXXZnuVZL+U2Ba0FoONz3oRd/X0U08bQWauUV0ZoxDd+6rg9qbaPmnSJMtUwNwAmwCTyspK60fZ81JHEGTHwNH227fvMOKBlNKzlz+aQn1GqoNAAy8cGPaXRIctkfG8pyDran0M4/Tw3KF9AJGeKckMNTProFvwqPT3vjb7NVNpInUj6ROCwb5BmoT5Ax44yeCEFMOxIcOu17PqL5ITcWjABDU6TlE4SRijV4pY16ux/KH3AwTqT5zE8XHQ2csK8twk9184y1/96lciKlMsSSlR6xSM2uS4YyOdesqptoHuv+9+U0kMFxEyzkqFeBqkJHT9eEohdYDECohTfWdEbXbp3NUcB95SYKMhlT7epR0koBfBihazoY2+Tv19Q5uLoMYBOC6Qu41SaoNBoFWWL19maYZA3njSjRs33hww0uKREseB33PvveYscq5sTan6IyAvPQDXDhKqhuCqamuST0gAqKywtaxcuUJEclhVpmv1D9jBIWMLggBvFlNxkFRzBY4G0TgCwiRvG3MK4TlKSJikvPtrru13D1eYFo6vpz6kxpiBYLAQKaTIPQkTEC4EmzlFsh59yOiCuCgL/hU8kDRnzpqpvbDJvCmLSpQBLlovqL8flY0GRogMKkdKLZauAware0kThDYB9SKSdAfc+SmeSQE+xGUVqPWSO/b832hsB8grFiYGVShEHa++NBZqz/csb3EvhUC9bU7YUdiseBHB8eN+C/eICgs36R69ksBcNhYHxj300EOmviLgkwSxqGbaBqQv4MWeZckRBY0oJqgqkEUwcBOsie3CYq5oxiNHy1Ygrz30+tgZUE3QXzhaxpMi0niytSkhNtxLiiEK3mIVFYOFXCUZxgTNEzGkEogkrs3dqbe2ocZ1NPVC03hAtswPxAdJMpudG1hgF0GqA5Z4xfHdXM3jsXiEtV5OIQSUYoMDNqjoigWyoh6DCCBRdcja2hh3bXBqTNgEZCsmjPWAxxwM1SwRoCOPVmqfUHQftjnUk0jeSPxI55Mk+WWJqz3iiQ5MDLDlORiZNKFvNIZXXnnZArWxk2KPTNeqr6cYrKPH9+zHMDf+fXDlYPOOxMHjzjvvtBABCyUBrntyHvcsFPLWGhEC9ZacUMth/CTh68vaRO8qev4EbbTPfu6zpg+Hu20rPTRpYKZowy5dttS4ZhB4F+nryd81bETxpK/EuTz44ANechpiQY/1tjlFwILz7CDDPrnBsImh3jMHhYCI/L1Ic+SwgyPlXuxQ7SRNYHOxe6PXlk1bLGPzzJmzDIGAbCoqK4pnytZzW3X/7bffZuoYEPWelALKXTdIiqizgq2lsqKyQFqgHpgJVKEwJszrMtllkH6NyRBCDzBFffmAJCZiyiZOnGDH0KdIOyAqva9ZtcYyoyNh4yFYFLGXO4DGuC+sCSFS5oi8exs2rBdDNtcNk+0rZNoPyJb+sp5g0lBn4a1Kep9YdU23VgkeeLUhVUCEE0Id2RT92lqxbLllvRgqeCBZVSPmoX+NMdamqEP9I76KjBrMPXYoiBKq4mBzbYpm8zr3HQjUmziRKgeVBNIT8TanyX30yk99yvLo2REGKkTzIzltkQcaB+0hEYF4SPxagJgz3HYgTkhiQxSIips1qoIGF20YDPu4uIM89pNKJrUhZLg5kAxqPgy8qCzRobO5UFFY8KY+r353lYgtLr5zTf2I9GDxWMWK2sYuh2probzRSEuECrNFcpHqK8gYNegbb7xucSxpYlXgBGLUO+o/5hdEjTQIIe8vw72NS2X5O8stholkqicqPyCSa7XAV1/Xs1JfgfxPPnly3c4kKgbrxrzmiUBbn20Cb1JUfQdqzRcUwQNGZ6ACXPEARepkjZEgFdUbZe3qtZb1G2kcJobsE2mGCm7waxC1NoyfSaQi6N1Z+zqcsoAxaswxNmFdMITAAC9UbMg7dWBkv76eKWzCdvOq934I1Futhx4eu8F3v/tdy3IwRMgZA74F9flNBiIiK/nVV1/t5ir/GfEZo0YpJ1pFRWKjCIiuHDjW5d4a6qN/OFzguQc3h3qvwC4UPYsUMPTgoUagsH+hZlkjtdNhMk5jZyM1DHYp1H6oZnCpTYtHuvbdIzjUWujgUXGYmtDDqYbuNs9PIFoxGBBcbE+4ODO3abxM1G9gNGaMOHtJWiDUBx98yOJeKE/orCZUXJPlZZYGOcdj9nNKPjwM/+Mh7iJ2VloYbOD2cXAhuJw5BDYFgeF+vrG9jVbw9/5aC48p+TE5FCcrEwj2RVzPcQ5C9V1RWZGuCxurXyO8I1Xhzm/qZNlutHESeIR7Whhskgkr8tfDhDg8wg8IDidn4w4xtqSESuPz9pbxFBlifqnpIFBvyQlVB7p4NmmlVA/o3U1FlVloEKhevXuZ8wMbG7fhAlVW2HBhjPqeSE6RQ4QkJ/NiakjxG4XNjxGbArGAi0X1UBApz72hX3rvpODYvvJCYmhvvvmGZQbHxsT4jznmaMtEYfYlSjz+qB6M/UhNfeSIQSBrCoMYMSU1tJi/eIah3kNCZmAHKgYnhkv4DOJGosSDDKSK2gqJGtWoed0pu0K1EmCjdzLYg7SRPAmQzq6has82xwX1E4eMzpIisRfClODAYyWsFf8ZAtVDmS6ACRL3M888nTJCIGkLqcg+xzrAJqn3xx59zKQmPBrNRZz6vb0yeXAv+RvBBdhZeiMVgolXrnw3SYkFQxfDby8ZWt7NpodA/ZY8G8kjYVPTsLiiawXdDte5zcf32O/++WrIPDNmbBqtSM8SkFl9YRK3p+qIDSEzAUZpEKqVsEnie/3nLoqER+IjXQxBuyBsCE1FRWWVZ1Z4LttHXYfbxrvxRAVsViOE2ftbwncPb/KwASsIOXahAjjF/RRMUWkh+ZAlHATOs2n2hHh9BDjrfeH8hRY3hcOBIeJSMGxumPh+ceQKRIMs7quV97EaYo3GCVEmPunll18xOxTrzeChcaO6s4J9jo8eJkiQCxbMd6PkkNIz2Di5L9TbUuGTjKbwb9xXfcb+lhzsOMGkwyeeeFy26neTZ8KaKFZPfu19CYH6Eaf6gCreXLVsMAhS4rGXNBR/rk/TxZ6BQOC40UVODMRnkS3cSuhbeGfT6PPihYvljfii2RMuvvgSU80sWrTYVFlIRdWQVNToPB1hgUswsSr9BioOJm4nuq8lfkR6gijvUtwONsCCEiEU8ge+IiKDneW88843VdZs2WdenvpyFfIFIQekrGeJe8IDjsSsSOCpS3RLBISfs9ayK+G0QcZ45t7WCwQmLh4uU6dMNSJG8l689/BGnDVjlj1ToD1gF+oZ3O3x9uMQTgJ+2yo+b18r7DsymGNzW716jWWTsdOXIyeafW3M+XjqB4H6qfXq11bpp7Qxd0uNwSmgvIPgyK83a/YsC3wkvQ2ODAQ2YoxusIebRx5kO0edhFSAGoWzgshXlxZ9BOmSqQKkQSwLbr1wf9hgSHSJI8CqVe+a80QaDR8RNhAOTgE4WHDkuh3itjeUAAa9cyAhBm2cAQiyNOcUxsg9ehGPA+HC4I2qi8MRITacqcXZSTAbeGml9kgPYzJ84zhAVu2C7AotGT4aL7FgEGuCrSGsZj+kABMRmu3yZJ364lSzMeGSf4oS5lZWVpj3IyEKnbTuOM4kXcce1nPlWIMDCkeWDJSqOcA3fW/JcKlj33r1UUyh9vQ87b350l4gNfO9wXu7jv3Ib2+5EGgxxGm5XGfhol98cYoFspKsde2aNWY8RQWyRAQL/T2xUaQXCh6B9QJthHi7yIawRu3gaYYNiWSwARngXWXcrhAoDiC4/g4ZOsSQLBIFXkeJ6/AbltaHE3jNNgYnDKLSizggJLOzzz7L9SLwNxCuenV8Dz/k4QRB5dDI5VLFME7cmy0wWb+T2PaFF54XUn3TbIrYVPDWw0ON9DU4AhAbxDuu5+18pnSkTYg2ueQwjqdefHt4iHVuTmOmr50U44WalkzuI0fgdZfAg9ACCDVq3JEjR0iCPMXc62FccC3HrRoiT4HQGxy1JsiQ8sgjD5vUhNqwmuv43rRuygGq4AiTAwxIl4WaHO9PDjI0JibsUeqKP5dTd37PPgGB5idOfuGB+J5UwN485eJD3Md9dvjwEYa8UOthgGZjEyCK63p6jHR9p8G3204eRCDeZYplWqPMEBaro2u4QRO/NH/+ArMVjFeWbjvePCr0AaM/jgCo+EjQSV9DLMdKxX7dq0wQOEyQI26v3WSCFTnmwI9vvfW2xtvDPByJ2SH9Dq7x2IzGK7N4msVA94J4IVBIpUhVHCRoJ8nK9jBDXlscy476r5S3ZH2ntsmfEzw6tN/PHGtmy0ZkWTP69TUJEnjgLAM8jj/+hIRoeZUV4yb7RUKwZ5kzDusNOL009SUxMbNN6jS3fdbnvkaQikwMJ0NDoGB65s59S4S/jWWXSLOy8ExOnIpAbt+/1GJ0TAQwflCxP7gem+rO7E7a1dqgJoQQKKUCMcCDrMGF6vyi7z9AR7lL/YKUg5REsC0SEwRxrLIZkPcszWARNop1Ci5aLtfDEndz1Di4XZNRe5BiO3A/x46CxLVXFz9OMogHd3gkRmCETYW8iWSlLkCoHrmCkCfKtRzX6ueeSzJooOp66qkn7TgT1KNFkXAWIbU0RC3mfoBcyysrK82ho6e8UGcpkS+Eifk+iiwSgcBEY+koz08IMrZLnuMduCJtEZCLE8n7ChlrXnHJx4P1aRF24MD+GT16TPXDHFvaGtirN3XL73wrqcxaxpRnkVFtsAvEpaG99+3C9ZJiCQJF4koI1OGHj7VM2KmtJPSpCNLhp80bNxtBAkFt2rRRNpf17tJLLnFDhx+cGHyDuq+2sbXU3wVrVJQkrAU+IOSxYw+vOqK71Fx4GL8681Wz00DQ2kg19tWvfFWJZb2HXnb+s9+BSan6mwNevn8L5y1w1/3lL0Z8yW6PSu7QsYcmPQr9DeuFq9G4nnvmOVNVE+SN2vjSSy8RTKsykjfHsJqtTcGF/YOTyQJ5zxI3iX03Pb4m7lhLWgfNBrB9v+E9561XGyxZcOW8qKcJFicqlpAYlvxu48YdqzN6RlQnTDW0j62FWB0kg2nTppstiyPUd+3YldihaoIBSKsYQq7pmdp+a8z6VBfqOSRYHB1wjSb4Fs+rsormbPShoy2w9NVXZ6mu3W7turWJS3U5qzCe82LjKnatrI75m+r6vO8Pb0jYINUCwlRG2xOOm2CefzjbkEWD49it0JfsGg/rI/Szrv0t1p/GqKNYvfW5pvGyf1BrjtYRKW9ofZF9BdVx6m5Pvdl1EOBSW5vxWMuBYUuCTW1j20d/b36bU3MANtroJG1dILsSp66uF9LF+wrHCOJ1iqXsSbub3RRsGl3boeMVkJwgdNQBIsfW0lsccTVPvXiTKLURCBt1ZqMFXGb7WF9YMy55LeKZiCoPVSXxO2TXCGc+GdIotaGjcWJbYYjY6ZYsWWrJY/F0tDHH8NBX3PtxlCGVUgHssuPKPGfDDPdk3/kt29dS/baKihTdj+fo27K/kTcSqYd8kagoq6U1KvK4XfJtvvH6G7ZWUGVv1XtXHQfCWVEFsPT34nyyYf0Gg0eNDiRZ+BTrQ13HXKyOxrzGnOiFYwlwxBuSQO41a1abJx/HpBRbX8SarZWNmsMwq+2v0L94rOFz/B7g1dJg0pjw3Qvrev8Rp2gB4jH2+uuvSc/9kp1We+yx44yzh5gQnIsdrKMyK6clu7D5IbpG7jzUNE/J0/BUuQ9jW0B6wlsNV2y81SxhaECOYVOoGozBeAz2VGaBAmNwsUWV3Uzx9+xvMadZrK5i16J+MT6Od0Cdhz0NR5VTTz3FMnDjdTakckhVNvIAi2ydvj4Cbu+++y651J9tB+cRqEsdJMyFQMWu1UibpP7BoQSGwQ42LJKBxJqK29VnCAdZRnhZsKv+G9GPx5V9LtvnUt+p32ehx/mBRMLklRwk+xPw4RyucAxMqSpCn9+e+7Z56J1xxplujGLuFkrdiecax7ZYQHKQKNUmaue77rrLbKIg77IOwwzjLfZOJ7JEusYON/GPmTmE6SEtGg44uOB3E8E2oh2YGN2/RkSJkBPsVObSLzVztTkO3c7MPXPI+nhP+/497VuDBeuENRb6knmmiSGQV5+BQItxiNijM6NFuE4nmKKaAjnCxcdJWysrKw1J4GFG8GQ1t964s9EG59wqbE4DhTyCQZzDBrHPcPorXO/Rcps2J4CorBH394c//ME24Xe+8x1XObQy2SC1IA+IIZ5fG6VW2rptmxFYCp51XSX94UVWkI2C+upYsAPMnv2qXKOnSVrqa67Rnez47VaWkoeTj09TnJILJ+JSP5s609YuhQRwfMKB8t47TMl/2ysh6AUKToX4EDawTf3HqzE4noBwfv/73yfwFOLnCJLO8uwqIEShLb0T0Is6lgzy2HA2btxghwQikeB0gDSMpycvPseIP9vXoiDyCMvi3sTRo4oDvmeedabZzSCwz+sawdbnnHdO0SrSi6oL4ssYIbqc9MtxI730mczsTz75hMU6DVaaIxLDwhyQEup/ddQMYicxZIzDMq4UgbW1w3X5ExGLt2HDRvMMJE4PIo2UYdKe+mz5IEvVUfMoGv9X+uHhTOUjlRi3ozxpH9dJwE/J2QjGsaKiwjLWsy4fffQR97vf/c7UzcAEFWm63sOYovqIP3tXeRzJTsEaATaEqgCT/QUP9gxZX/qIiYRBeF9lT4/gZBNbD1zR2Avi/UWc/IJd9s4yU09xdDuu6rj9pklNBWGuEUwK4cJ7CltJyQnzkwpyhKvlVNSLLroo3fBsliOOOsI4YTJvIw3gzWUZuvUbiOfmm//urrvuOlMB4vkG8aqJIEKUIGQQV6QPgpYhjHg6EisCoqTfxB1x0i6f06NB6rDo1suhg1RNcK+VItgkdN0fSVJID+QIMeEIFGBlBDVbAoLQ9TcVyLxUBP+SSy81wkQd2Bg4+fixxx61gFUQKHab+TKIc3AlxIxM5c8++4wF6nYOqp0wBg977BIEt0IogUdQk0GYWgvx7PREG4QD3IE/dkE7f6qc4tshoJpTa6dp7fTv389NOmmS68xJwSq4ROOxSMYDvO4sn2A0/mwzMD8c5oiXGoSCgtcaSYEhRBBs1gJzhz2Lo2mmSDKD0AJz4FTNBd/3k/UB8kW9jCTP+mBtARcQMUQVCR0vQeYOxG4nHselDuskO7YGfc/MLXBEygYGSKowMexPPv/f//3e4slQ+eHIRNaJnr39ScV0IoIH8OZeGAIcciDUVV7Bu82Wyu1IpCNGjJTNeZxlkDFGLJ7HGua0QeNuzoc9nBymBcEBuKTMW3OtA8Hj/aPW0wRAQFDXsUAhIiATTiBNJAFBg4nQC9010eqo5MjKTcp/gm6LFiZWiJYgQpAKHn4pMeMBP7moHIbq+A82CfFTqMVQU+C99uMf/1hZ2+e4bdu32UmqBLLa2VFx8QuIwGDURxzgdtttt5m6jQ2LHaid+g0XuULxVbgpv/TSVLPr8GjXzl0sLidVW4QFmR2U33wcNw+cXpN0NEoc7HFClB38YYJsZIgdwdBsdIJRR2pDV4vu921gh0MiACGecOIJttlRt3E/sB5SOdQQJ/1dqSPJ//a3v9nY1q5bY+BDTYdkO4iYII3TCnVoM70tdSinLeNBuHjxIpsrkDoqVaTWY4VkINIQbE4qtjH5IFjy5HXcX8inDIeMzZs2C6avuBdEACHUqDZxC7cOMk69UE2yXsiNd6icRQrgETa52qLfwAOGhQBk3PKtDq0jiESF4uqAK2sKAoNUDaHeun2r2y43a7KmsHaJrQqwsOdVsNNN19yjPgWGwBQCD8JhfeCbC6GCaEHkgAUqZ7KlsB4LVMphbEnVe+5vgJVvsaskU5LGsncSVfBs99e//tX225ZtW2ztkIewACa+7zB/MKJ///vfTUpnT7MeOLLnlFMmm2aE+DxbIyJySFbYn4Eh2WNQxdtBjx6+6fueg0bTtuTXHftuxoyZltiaOFMSHNtRL2HcTduLorXv+5ITi1QIYeP6jWbQR9pArTFB3DPZ1A348Wbwk4Hqh4SncGws7hOVQqng3mjjEhwMwYAzRwpLSzyxup8jAi788IXu4QcfNvUPC+GGG260PoXKqQe1DiodAoSt+P4tkNsyGwwpAwIBYsPdFumoj1RCEFBUWRALVE+oJek7sUnH6t4PiCtnTAWqvqrepghyhRYqzyKdkaqJY9UNgft+BKQLl4kUgjRIGwfjMp9BLFRPH5CGPqXzvgISThG37qc/kyZPMm7/Gp2sDIe7QWq5UNXbb79VJS34DBMgeAjwzTffbIgWKY6TloF/MYnoVHeqeX4BO/oLAYSQnH/++TVLOeoE6kKOS5k3b77lR4RDN0k0FL8WCEDmNwglSHTYCJ+1PAzEE0G4fH6/VFIkTEoB4jMC1dWIK9LSXXfdacG5JvXon3h8U6eylpnLNOhZzy0WQTOpSzFk66VCHjFiuOCRrA+O+0DCgHkBQcOkzVI/yId4zz13m+SJ1Ma+6NZDWU6aESnFS5LPzDVS4llnnul+9etfu5tuuikhqls3263AZKa0FmgukIiDapg9z/6FeWGPchjqGWeeobUiTUgJhgTbHmvkvvvudbfeeotJnRd/+MOuImgGiqzvbH8b5btfU41SV6lKNMfvSb28QLZuYMSLtY7mhvPmeJl0nsWRpepr5Ov7tuTkgYrhFC6RTY1KAMnEErAW24Bc86/OUn+AFMh3BrfavUdVaiPDnFrgOFWwKYjNOPvss6VWkOdZsRK1RdaELVs2u2uv/aWlbNq+c7uqUuCxnkN64pBGXGpRFVnRDySeRb9OaicIBicJoxIbMWqEpYEh2BWVDZIInGblkEp3jIgKSBtOGYLHMQVkKOjRK1J9hL7SuBAcmxFYQaBQbWAPKYjzimGmzx0UEL1Udh6I4djDxhbq6fU7qsHrr7/eiOjRx4rIUcLG83Vt3bLVjNvA4u6773ar1yrbtwowoWzXAXVIX8CkNymgVMh3+Jvf/MYt0sa6UGrUyy+/XK7/wxOPrRIbG+cWkBfqGmBC0CcBnyOGjyguGaseEDnqQiQM4I4ElxL3LKLSeJA+4PCxaxwyclSVpOqRIZLNnXfeYQHIcPoF6lY9zyGe5JiEmAML7C2bhYRRT7IQQMRI19hXUB+aY4TKEhHav/zlr3aCNL994hOfEHK50B12+GEGM7hgGAIkKNSpAwYOcIcfcbgdDopEC8FEimf94NRhJ/iWgKM12JQlrDHfPo4LpHdCMr7nnsRGuX7jekEj+UdXNsquhv0OmLAfOKkbIoOERWG/XHjhha5/qX3v5xKHpYOHHewOH3uEYNXGPar9xvoeo6Bgjs4pijOaAhYeBmgNKKnGo7HaUv3YPTm+BMkc3MIeRrU+R8wexB4mnlOfzSkszEljtV9GPfsucWKjS40Hdwz3BMJATw+nn6rxbNajV/juAWeIXsgGNdnb4nQR/Ylbsk2rwsKhXlQnBOumwZf8mN3YtKNrPPPAA/ebg8Hf/36z27h5Y4qEA/JhoyEBgCRYlBh/WUDYM8477zzbaNh4zHU2IMgwDt93rqOiATEdLm4fwy/SGg4UqBdTby//HNwpsMLQj8qTrOsQPnNZDm2EusO7nmUBd1BiWCQ+czpA1RT6on1FfdhWkBKsv8leK4D7vffcY4iYMS5esthQTkKYqhreKElgrFSmI4aPNMTz5z//2SSyixXk/MEPfrDQZsJjcZ/DxvJzAvJi0y1fsVwwedb6zXljqQFc9zNPSI5IkBAay4Ih26EhCcYQwz3Mt95B/qyZkG09PWnZ9+HFF6a4FZJskfIM2cVF95DyCoJ0ncZ3j+CyXiq5NjqkM8YOO0SsYZyOPvoYR3YTJASkNaSm44473l111VVu1OhRVZ6hpeChWkE8IxTPx1pD+n3++RcsBKKyorLKw7Gwl3vmG/Ci3xo67uIwV9iY8FhctUaxTx4irBTWy45dO3T/brMpMpdoJZCOUXkDjxMnnVh9v1BJdh59xewP1giqPdYmjN+RRxxZswt/6JT1rhGKYLBwwUJT8TJGEjCnRxSFNV3XZnhOr51i9mDW//D7P7jrBSf2MVoECHgH2UCTsJFpdlp4AYHyz9e12frcv28SJwEQryrsMXCDFJAtXFWBOiaGWIzMwnXVg50Gm8AiIe6dcjgYxPHcfuPAVcOdbRHnD7IxbpMi5JUYWHVjvIj0Ga734osvFqF5xBLbwgkHdQ2Pcvu69etcG3FtJ5xwvCGwJ5940qSP008/zX38459IjL5ZhBP6XOQdbhCXb4gEByWStJS8hcG+gFstaiYkJjz+UE2R4DaMs0iVySVbqK3k9dVF6sRtJmHgEt1eRnvKu5LUyLpxgs6wOnCw4OaJQwoTPY/k+aUvfdkINfnVzCjiC3AJ2GPL1i2WAxFVJogHlcvEiccZLAuM+cXmMb7m+wBcu3bpZmskqMhMKqNJzd8SEcknn3zKJEikpTGHjqmayxIILfSb4zRQq2G0P1oSI95lFJiMh8WJg0CR4ApsUn6dzJTe/4c//KG7X6pHVJsJDELNVQNZrbO1OH151MhDjAHAvsT6vuKKK9yAQcmhfgWIN16HoTre/XXGTugE6r3p06eZ6qvAuSB+pqk/x2tb/UNK/s//+A+zs67bsM5gksAl6XyA0WZJV2hGKisrdQrz46Yuh3E57TR5kyJ5ZtdfNP6CIXmYkCYNhxSYFGx+wDe185WCQSk4l7q/1HXf19tuvXywwh8AACAASURBVM398Y9/MEkaPMRes8NSaYdXGFOpeuLrvm9blZwY5xqcjh7QAag4xXz2s591n7rySju1GtU1djacmMBv27dvdzAr5uXaWOMro7/7FnHyE7ZJkgeSCdmyCRbF6GnBojFgywWyODeMxWQBmCPjNLn9usiQvUMThjoPbzbiVPr2l8QQI0HWDhsitMO7kB7cys+vucatkbGfS5lH/IpLEPuoUYdY4tRfyoUYL60rr/xUguSLTWwt48EOc0C37kq4Ot0yvB8iBImTBtIlNhsQHF5j2DrCgXgFC79Y/f4aRA5pAQM+gcQEokLwnn76KTtOBE87i+/KFj3/prKZIzEtWbpEnkKIJHFDyc4LyAd39DFyQ39cx7/DEV8iwjT04KGFz8QIiKrCi7bDZ38PCUZRY2DjwjkBJ4YgDSMJMuenK4bJbGnZ8We/h7HZ9VaG1IApHGkIzH1euQXhgmFkCk78jeriaPtbb73VVJtVSDhUnrxzfYtUfV27djNVFs4VqJ8vkRR5iAhWnUrUNk5ArMinnnraHE+Okr2qxmDfOjVUh5ujecNxBiJ/u+wh7yx/xyoJ6yH+zCO7du20pMTAnDklATMq3/Q4+OSBmjsSz6s+wxCBlNE6ENN29LHHVF8LNddYv189DJ4Sg/S3v11vkjiqZTQf7FNwEsQzDScopxXViSoPxx5MCuDHk0+e5L7xjW+YiYBUUcw3J5ej7oXQo+pDag027lTdW057Dbxn3yFOTKaQDm7iqPHwcsJeAtfOEfLVkFQWcGFDcD3+rK9wvsRB4FL9jhD7EKnF8OqxoEuJ/seM04KNn9PzRpiCNob6VFDXXXfdn8U9P5xKVkFySu5ImgZ3wjWzMXD7veOO24XgTzPpr+Ti8G2Eegre/W94p62U0wEEcuCggUb4kByQeHDmgDAZtxwjeCoKdQe4ZODDLfQVgsEGQkWExMGGQh025GBJYaGeuA5dQ5LEkwrEkzQTDyRcSWwt2GKSI0peNxUtR8BXc4GOH09arf6XezRGbEeczQQMtqvv1InERIJakg6zcU3iy5YsHDJjYiAQYzI5AFtscdjUbpYhnzlMHSXCc1H9ODU8/fQzJuUmaDgMiJVSNRV8xj6Fp93cuXMsLup0OTSYM0h8Y7bv2e9V1adeqjhLIEmjAs+qf9O9URMMGuM330/20YplnBAwRQ4fizU0uTp7VV7hUHa7XXL2gGnE+YGwionyyMNuWiChZsdf7Dvwo2gcPLuf5hGYcOoA4QOpF1uxcfrnGgVOqus5aTqelep5lZiVBTqeZYrUwuxfxsheIDgZxjXdo8XGE/qkd9R5b4rgcFo1wfCf+9znkhRkjCVSubN+R8lmin0WhpOYME5WSO3gpdppxOt7v7ee31yo8bCZgBDxSMJVFC82JhB7is2P7gXQ4XMhHMMu1d6OVEvc01oLlKSe2IEel14f4ocaC6479c6jDRqIS1jk/lpbSRgTJkw0jjfozdGYK2Od/oZnk09sQtqBe98pgzCODebZRakL8on7o3GgUmKjQ7w7S0UAYibeBVUeRDzAKm7ICC2Ft4IxJSmX+AlbC95hqFGZA5AmBA8EhxRVqnAcCUHQlFLDStAyAaWbTU0IHHjO1AyUav0q1Zq/Ho0BF+QDJVXjcg+3jfoEKQppG3sR8DAVrYGg8ITm5FLhugmwgqNFXfKAzqxCgoTj5aww1JI20GiNIR0QX4LKuVLq115qt3WrNl6SrBpLWCOhRdb7vffeIxXhKKmcDrUgUitVXap6uNSnzBolhIE1AnHCnmrHxHg1dakqGv86ME9qhYHAqxYNCDF8lqdSJVkTYcUk7wwbmKxdu86cnioqKqo7r5SzVgL8fPWdpOLimB7WNp6wSBhhTad7gz75TlucUE2lpvnJ/IaaHaJLAVeg1nz40YdtD2OD/rA8CQmZQFInFVhRSTeqs63wITgrCeTuk4RD0Het83Qstj6TARx1zFF2L3ZopPTC/V/TIBv+W+MQp3gyG96nOtfwnpA3sQm33HKrRN7E040FgroqQSzqIAvH+qktniUiXNU184gq8ps9pesQPVR6cPq45eKBxqRhewoLkvaKLc7QD/LRhaDLRGpKvPTCoAMi5juIktxicC8Yqa3fAZmE92RI4fFa36mns+wtT8iOxRlUeHYhvrPpiAOKEXB24zGGLOEOHQJx0iWCc3FWoJ1zzznHODTc28Oz4Z26E3C3tnRRtQ2D+ndKbYNk018xLxC+AMf04RgmtUIiuQHjL3OJxAciIA0OhwTCmWJrqD7emitmibXS2mMNgEzhNAkXoK84s8AUVIMjcNVz3INTwibFVBUrYW0AC9qB4OG1hrv4AMWANUZGA5g55pBCjBtwDplHisGi2LW07xEB5lqt0+PhENcJDBdKaoFJC+tRpNzqCsQ62djJFaR3nH8olo8vW2rtRPYBORZJxQlDSR/COsH+XIUr2PNtrH/cG/rPGkjWeYIgQ/+5HnAEOCd0Kdwb3lFP0iZ4JxSkRtYB8/7IY48YkcKLFdsaUmJFRUWVV2Go2L/jSESKNKRA1g7EBgYeb03rmy1evdRcCEcJY6Iv4A3z9K0HDKtDtfYrjUOc4naSeUgGkKyX6r0I9/BLIwyUxYihn2zXbFQ8XHiFxQOAq2+idElU9c8IWLi38HfUPPzM5GI0Js1JpRYCkfxs3nRI9iEeIGOk3uQ6tgEkruRboq6q6kDyKSChnfLKggvkYUPE1NFAeKECYJESjApc4LhIuUSOMYOTdaFq7FXDScZUNc4q1FDVJcT/LabSA97LhNxRI1qNRQgbG5k2cetOSgZu/ipvtMaGbtMmWbLV5zO6uZyPvqkESbSywFlUQQcqxgP1LSf7AodkSWTHWtMkJP3kWZKX7i/bAIZ5kD4GdVz1k3ojGAsG4IZ2siFgt1unbO3JHCQroar1woEFGABHkEdjFOCBZMoana51DteeECf6G+YnO37679cHY6Ej/rv1yRMpG0320ey96SCSG5lvvCVhFgKRTFZD+Bt2S/JgK3k27lafQegpk1iszbSd2j8EYkFWCfY+zGgME5sH1reqSnBNgiuSmmO4Jb8bvgA+KV4ARvawPUJ9/I6mBRUeBDeBarJOEhgm927YtMHdc+/dtodRGX/sYx8zSYpwgRRf6NYtYniekbr4L3/9i3lkwlSjMfnkJ6+wZ5AOQ6F/MJX/89//7eaLkNE+WoovfvGL7uJLL07va+oPDSdOYb0CVKknmEAKHI/pegvnxgCGCAkAmITUw62+I1X96IAvuOD8RIUnInKIJI3Kigofu1K1cViwAUnGyDJsct4DlxKuhW7xHbUSi4CJhSsbpMk9+uijTO0W9iLrLWwiQMPw4/pZ2CATSrLgCjdX1fUEWcG5AlOTLqgMjQHv9Swb1HfmBalvzJjRFnBHvA2uyckgEoQcV89GSbg9RhRNeKYPwA5ja2VlhR03jgoL9VucKiYgsfAo9f72t791b8zRKcL6VwoRC9241rtbGZcHoYaggUBtE1LqChO/LplTuFO85w6TbaifJCdUwki4FMaU7XPoe7H3sMZAPBxECXLB9klBdTpA9aPCY00YJKv+WE5EgmJxXWYd+S4Wa8Z+awOxlvpvqzwZE4JW9NY6XdyltQZs8eicJNXmsXIACKrwqooKgW1z5okSUmNA5uH+7F7KdiisrwDnGN4QdVSMOJfgfOS1elZFvHMSWCVIHW9RxsE+tRI2YrbhMr/DtFAXe/EcaQMIKdm2bavBhb4y57QHDNj7fA+vgGfCO03i/UbhWsCDjIb6uGbJaFk/guWUKS96fBGp3aIBtW2dpCujb3jYwVgE/BIWEE5P2KqIZXpD2iTU1pz8PWXqFOExBdKPG+8OV5hE0qlEUsSWfry8hZ+QM8QKedH27N7TnG72ZGk4cfK9Je8YKgleTAzR68OHc1CfEHHkhLVFbozYOfBagtvDGwpCVgPOKwselUOHWCAmQapIBfQBA7d5RTVSmfPGHFukxAKAcNFvd5OHU4oga2kHt+kk0C2eZLZ2IYEK34jqB1ECU9rC7oSLeUMKma+RbthkuH3PUPAl9YM4GworUivhfnr++RdYF7HhEARaWyGtTlyyRCpACBXc+PHjLO4MCRaCXS7sq/XB01jc14Ht4YcfYfp7YqewLeBh2LtvEvBb7dkyL2AHBQascRA+cK7t/CtsPu0toFqNeBpQuDoSpMy1kEMRNeB8y15xeI05Gcvp9mYhIGALkcFm1psj45u5gHg5zBGpqFQJawQHIrKrAEDUVwROB1f+Us/Wdh2kDL7C7lVRUWFrrt7rrrbGMr8Tu2Z2c7ueLFqRNH1qbQwyc4RaD2cYbEmWBMCv7fCOuv5xucJD+P793//dnvvZz37mbr39VsNHLwpfpsSJZgTmXn16GQOLrR3ihMpw8uST69j7ht1eerbLqdcDgZQfcHuf+cynLUXNP/zDVe4f//EfTcqw4u8jK/dfJVZCRK5QPMZXvvIVS1hq0kBDi2aP5K1kuoYoEbPDxK5RPEhaYoaPz/GrlvaXLV1m+l04dzzQIBSo+rhWULL1Rm3Cod9ww9+kL87Gr4TVBKiqPk9UfM0XvvB5M3SC5Mg+biW0UUuf7eeq6iTabzG1BNICi/oYHaiIdAOzQHAjga5pKTaOUtf0EG7epBOC0YDwkYuPeCROe7US1Oa8Y2DHgUTvHBsRONyoq1E3QMMJEPsLOVxyyaXmZBEM9gGBpw9QSbGKuCHzG31A5YZtCWcOgqhRiUD0brzxRvfOErku09+a4F0MJr4zryrUAImGxKUQKBxFrE4Kz3lYxGOAUK5RXkOuZYfB2gj/qAJb4Qc/eIE8OjuaCg7VV0MLakeyRfRUmEGwPaX9LTbWPXANwoB37HvvJc4QYYzxXgnXKisrZds73wgJUiuaCith7rNALQaweJ1ojhbKSw6YgKDTk3mbety+X0HK4muQvsgHSSwSOPbXSuf0r//6r+4DZ5zmeoqgpIsm6h/2sjMVvvClL3/ZUlShMQFH7td+PzMbEBJjDh5hj9KWvpNjD9tUl05djHk9UOttT5b6kwU/ySRWxGuEPGCoIhB5GfALIg44KCBSAjDSj1yvVCLf+96/GGfGMeaIoAUeV+UsnJqgo/bxeDruhOMMGYDAQLrz355f5YVWalEFRBG/c68KSJ0JZKLwAqIN1D54SeFOjkRVTsFN+6qr/sF1ItloiRIQMWL0mWedZTa0k5TXb5qI02tCpKnnW02wijeXHwPNQUjJZsEGPpj4IN2HZHPWWWdaTi1Sw6xYhguzSk31x33nPrXBQsaB41DFIeHO3F+BoCeeeJIxCTAvxoBEi99UvrqGumbpUo+w43qt2kI13wip3qifOCEIGt6MHH2SlnhuM3XZ1wALP7Y35RoMwWAdslkpxHiRVR5unXgjOMuYeKT1hLaKtaNraBKelQQ2WmlvQGikzYGoEkrAnkmRZfQ8xBKHCXIMSrFTpNkqQs1jwIKofsIlCCR+Gdsheq94/kv0r+Cyv3/92vVylHncJDzWebcDvPdfOXU00T2kLmIO5s+f57Yp1VRVqYJFIFLtZJ8C/8AMMJ9JkucXC+cv2g+1dllwgeDfe+99wmlrjDhZoY5y90etjdRwg9oJ6kHGSHA3ZoTPf/7zhm9/8IMfOLzpzLWdvVViTRJuceyEcYYX8X4k6wOMDbgXRpUUYKay83uZHm2V2pL9jPDAvZMknaVj3hNjVx/qRpyyi14AgfO86aYbzXZx7bW/cN/97ndNqtigtDxw0gyOTUcCy3//yU/MAP8jRcDjY3+y4kgQWa2UAGzyYx3++noIRmTjtlFQ2f1y50U6sDxVYQw1vdNc+F31LVokBwtNIAZtxF3Gg14XLzokENLMbBXxLVnC2PTOmTQsivYmqgdsnWw0FmAgThDXD3zgVFPTkCONnHscq8FGZcHWiIDCQg3j0Pu8t+a5W5XiBiM9dfdT2ptQhstj50wl1URiIA3OAqmIrIRFSP+LFf878Ud4MfE8HF3AqkiYwAkO1gqrjVe0jgj+w3W5Q7sOKTIOzQakA0w67tfRpDzOlDr++OMMMTOvRO6jPkv7WqyfRdYWEvU9SqJLMldyIkJErAh2BCdzDc89NAKo+mqsP26Tzqs9GAGkkMmTJ6e/AnfmDunVSgQHnsHbjvVRWVHh2iv+KikJgQ4w4Rr2N5gXbEKo8oAha+peBeMSi1NQSs1duMlXTPwLjAIMCnN42mmn1z7/hS01yTcYwUmTTtLcH6vM+l0Fsqp/NJiskWT/AAP6jg0V2KBx4DiaaQrIT0sMyFp6TFooQhfY3wTaFyR1rg2utdRd48+hj74NxnGsxv/P//xNC5790Y9+5MZPHF+V7Yb7w76i4mzfwvr313GKgojz2vneTqXxWmGEKF6PqKHBmRAt8PQRUhlavTFuqXEQDf+xfkG4YfACCEi2V6/epqM//EgM651FsF6TYfcVC3A8S9w/6dh/pGMhMGD/tzxALpd3CPfDnVmeO+qrw6Ipa9iqD85vhPJsLZe3DxsP42Mf6VCrOWHEbYexcU0vgieTZ3dqkiYnCVa9owfZj3tr7KgdUB8k2Qr8WPzz6YT773CCGDoh4NOmTbekr21k1ETsDE3379vfffvb39Ym04LQc6SWaS19+y23/N3UG5WVlXakR0GerTAG3gM8+azFtEDH0P/f//2fmy0CQrofkGTB0Qi6j3xzHDcBEZ45c5YdFYD0UOO86DkINRwq2dIt7dHBQ5Lp0W9wdAQe49FIxHkB3H0fsTsOGjjIYp3IFmFJTvUvDCfM9Wil0/my1BJsKA7gGyh3cnTlEAGe5UgFy0bNg+EVHo6vCR6kVSIc4H4hc5DYRz/60TSTtT2i+znNtmJwhUnLSH8cn96nt88bGOot9q5nV2mOgDcqZoOH50HIGr9ZUjhSDslmq3nY6Vn2DFw/+4pkwtiAmPtsOUmeWfSbIGEi+nFGApEieQFP1nmBa3kAaAwLf420Sk899bQZzJEiv/rVryZZQppiX2YHUtt3g4mSsWreza1c65MA9cKS7JzDxhxmMOHIGrJ/wBiRfgd8NGjgga6P9morckVmYZD9rsqRItEIMY84tPzzP/9zVYBzbX1ujN/pk4aFSzcE9wtf+IKtJ+Y7zakZEEY57YUx8oxA0FZSJqpK8DQpkex0A4VQABu0M2TYITMNDhP/9m//lgSjEyPKAZd7qNSPOEX94/A5svj2669gPRUM9rjDPv7Y48Ylovt97PHHpE562X3/+993F3xIxnI9T8LJaokvs4MO7dRlk8QTpueZSBBBOy1sVHAcMAhBSQ8R84ugAKH5iUT9kgSszhFCP6OogZwsAOiFuW/wQYMtI3iBiE1/wksf2WATj5/oDuh6gBbAy0p0usR0yXE5Q8Tjk5+8MqnLj53zkjCAQwRw+KAeEFDJY+Q1hg3aYNjd/ud//scQIgZ/4iEKjLmhad3PfECgSDkEQiaHl81rNN8FHdV11LUgACRgOHg7pwlrvu/3QMXfkH4GJD9y1MhCRsTXSx6/cceOM9XiazoeYusOqb2sJDe0lUcahxSee+45qcsrAbLAG3UrR2DwLPOKTTDtr5/HMLechDp16kuWUwxEHlxvkYatFE6DEdMDhehxfiFrA+uZrBol9Q20pzoee/QxIzzENZn6MvRDP3PmE21D+NEipL8nPbA5GFw52BgT1J3E6m0TPCBQ5hmmfx01L9jeUF/hAECprKiwoGoyrcMM4GFmZxHpmJYCePh2eNstKfqN199QNvO/2KnDqKpBgmOPkPRL8eOJHmmWj8wDatHDJSVuFGHC6WbTlk3qXrI+mLZ2bdrZ2r7oogvtXCbgWlkhJk5whvCivgUm2C1DJvdig8EMMV1MIxnvsY8jyf7TP33DNCbZ9VHs+Ua7lgzNHMtQJxIMXS0Js7+nzm3qOZhkNAJPKUCctQWzPGHiBKsK3wCOJXlQefdImnvhhRclzCz7ur5t1rmToiU/+L4Ulw0tUYfh/DgqG6QIwkCfOVe2n8985jPuM5/9TPXBxQiBeuIX/cp+r62v4f5wH/DUQh2oLAAg3hkzZppapbsWbZoxulideu5txbvgGo2LcYHXGX32yJcFA+cBQUblxgYyjjUeV2B8fd+mvTzN3aXD4LDJsBCQvEA6FNKRfOUrX028jHxMD3WhKz5MxyywWCFOZFyG80EyISCQxQbyJYPzvLfnKfHlE3b2DYgHhP2lL33JHBUKMrIXGTceexWDK00XjU0HSbiAQPlxh0cx4OP4Qt3YVgLCCPMGc0AWdAJzkZ7igx2NkHGj/pNxGYcRpDsyc5vx1wORoFv07KjyLHGv7wMIi0zRqI6RhFDxEWOFmhF44N4LfJEcX1BsB33AbR2VBZuOYyWQOqwwN3Hx3+k/RBDVyiNKO0VMHceR2P1F5ni5Uu3QDwiHefuFe/yawR4HMgBZwhHbIZZxPb4vpMvieHXspqsliYXwBGAyfNgId9lllyWJjPGG1fP0E1U2CVtJZ3TDDTcY04AUx3EjCTx2SXLbbDGAL4lIc88vfnGtZV6HkH7j619PGIgAj2y/MiDaU1/DOsGLEVsjkvqK5SuMaWD9ABMYUGCCd17IksD7MElc2IZRW+HkAqFij5DmaKfsc8CFY13wdmSNkGAZiYE2Pvaxj0uK/ErVETPZNbIHAMAYYtf6RmlS42DcwITUSGi0IE6nnnqqMZfsZ7z5YJ6+/rWvuYEHKS9pM6yFxnElDx1n8vQaJlVaZWWlW75yuXt73tvmBHClMt6mCID7483N90hzgf4bDyoyCyAtgFxLHpBX22yFtnQfxm6kBziCu+66201cNdHsGGmmcu71fUGsR9rAeJjqmjMTZJtGhI+0P5yMev31fzOEfvyJxxdFdhjJkXyw03E8N/fhmgqi5KgINhnHIKDuTFU+HqahXxOOm2CeZQ/c/4C7W7rwa6+91qSojlLH4D4LQsaYCeyo+/PKnXWW7CdppupS8Io2HgTqVCFXdNPAiiwViSMI6scqGOEo8pzGSzaIgTofyEpUT1jQLHKkFJD2hy/5cNV9fm4IELz77nvMK+pb3/qWEVTa3aaTX7mF04VR65h90qvIrBL9OEgbB5ULXnbYy3BiuPHGG0yiIKgVIofenBeSwXnnnWveXJzuW7AeM3Mb+k4zrD3sGARFYpPZsmWz2SBs3UT9IbUOBAxCQsqXguLXOyoTjm9AVYzT0ClaN23JZh/q0T5gnWDrQkqAYFwneEDMOPWVYrZO4CF7ZNpPD0uYqB/96McGa7J4A0teENe2UnNZ/jk5FgCPTmI8WP/YZkHqaX1hDn2dhQPZw9/UB5g9CPT9WvOozz/96c9YHB0Jb0nnQ6wXaqn0fKy4i9qfSII/rPihSZS33367++Mf/2iqdVSY7B3WCNoP4sWwW6H2RoVWiiHdwxBokuaIFWRPEYiLvRjHD0wYmBdYe9hL8QYc6x2FrBMxzm6SXhVW2jjEKdTpF3OlCNMwqfqen/K8Yg4qTNzGzTtFbGGgYbDakNguIAZsJLyO1sg7huBWFhHOFhAV1Ea1qgKLAS1CPJzsiOhPcs+HhADxTKLeNIGo+gICIXs30hCul6YGo46wWT3BMK5N/WbzME4WNRsAHXU6Xr/R161ZZ5w7XAocXsWQCusptrdvfeub7mviUEBqH//4xy1wtUD6Cv3376QQ+cjll7nztYFA6ORAWyfuj3x/IGQ2GASBOClLeksB+UUMQHIx+hsjIn1uo2S3xFixiIn7ITYKrz6x/Ul1ghFqgbcFI0IDTJdPiRGb/w73BxLEoQPPSRu77mOMqFE4JA+p6bLLPmrqG5AMkjawZP6PP+54G0tqQ4nmk3GBVFGVgmCBBdIcHm8gYLhOCBXqZTaipYHK6s3jsUcgKfio4R0qhxiQ/G233W7eiZxRZQjdF+w906fPMLtHAaIP/Q2w0f3sCY65wNDcp59iiTz4sP8AJ5Al0gwqvmGSlonSJ6cajiFIXASApyXMqydwrL3zP3i+rV2QDDAB+QAP4AnDR4aTQdIkAI80Dojn6WNEcKsaaaZPGhsnGAMr9geOKqwfCDTfITao7thzjKVa8bBnH5x7/rkGk8WLF2nfLDKJG7gCE2J/cLY5SFKyaVTidcznctZItcZb7oXgccw6wGEJiXT16lWmtcDWxh4EF9leCesi3nd7YGiNS5z8hOK1FaLKw6BtLPHgMhsWo9t/6MwWNhBR6SdJ5bW/EAHqwbvEAT6u81kImLziik/WqDOuFWZqF4M6UgveWPfff59x2uecfU5yOq4q4Fhw1EvknRqpV9r3LHJXXWkWDP3G/dP9sdcf/djl6QKfIR32TTfdbLms0OmnxE4VM/lwgqTMwdg7efLJtUuJfqNQz0hJALxqLGFj1ba4MvMDgh0vfTcImaO80dmDHEBmqM9ekToIgmDEJixgOhLaC53SuqgcWmkEE/tQUO8inUKwQTKkUAmn3HJEBY4zqPLglI9VH3CjtZIdQ4Q46BfInFetJa4nIhpFnwv36r6hYro+8pGPmFr1b1IB2UGHUmfi2YV0gzODcdzhmUx/gwYArpWAW2yVwJR1gBs/zBmGfBxX7NRWlSOPPtJdffXVpo5EokYKLXDzjvpnD/jvSMBDhw21V62FZwIcaoNHrZXV8Yao/2gjkGQgGBBsmBnsHxAP4HRQxUFWOTBmfcCEmFpKHp/mwBOXGPZ+fBxrzxzyqrXEc7inYVJr5xp+A5kiUJnj0YyrPCm7SGIA/v7mN79ZlZG/mcbeODanACcNAgkBrx8MrLg/s9hGyJh/ktxBqxUG7Qd+LbrvaS+7qxTEe801P3cXfvhCN+nkSZainnOUZsycIXXYG8b5YOg1PWz0fLW6S12IAI3LMPYEEC0xKdgCOuvoB7zoEG+xG7CYbbNnCVOR+jlfhQ3yyCOKnZE6kDOk7rjtDlOx4Ml2saS/Dhxpn7VtnAAAIABJREFUEBf1B8kCu8xEGSQrK4ckG7NI/c1ySZISjgZ9RFgJBEY6wEkAiYnceUiLpewmaX89YkAVhTqV+oAVRmcI9qU6h4i5SItgwqFuOA+Qbfs4SU5w+8E+1SxwiNYNmxq1HZIj6jnmDiYMWyBEpcBJpVhn/bpljeCmTBwJtiAkJnL7EWdVYK/S/dhLkQxQX2EgtzZi5FuqnXCde2u7v1gde+Ja2MfqH8xeiIF7ffbrFoIypJLA2vOSM9NCf8QMwVzC8JColwMhD+iRZLe3W1rqWPcEPGtrw69lbJHJkTkvWJgNeTBxGsP29Fn5B9ip35Ro7ddWdWP+3njESQMg3oQjyLGDkKuMXGHv6Hwl8onh+ppu2rAYo4EvWbzE3D+h2MbpeYSGizOBllB0HCxIHYTaKxg967UIad8vXjx3QC5wXyAaEA6iPkggPb213AkSAcPFG+4WpEN8BSom1D/HjpehFuN1tviJBzn3lerJOMYWVuDqe0n8x1MN7ziILQ4TGFGHjxxevbdFFjOLH2cA1gSqGIz1wxUjBiLuFHuURbDG2wpnFIiBzTegiddO9Zb32BXWDedW4fDztA6iw4AOPAqkplK98fCBMKNuQ+JCgsLhA2JvqtgYuep+xj9SRn/aqM2ppVSzzYVkSvYn+0OYW73jNIRNCfXdZDGJlpkB6dzjBcYSnG1wAukuPEHKJSt1IUyhvmxf9vXvgh97co68QTFxwGiCqyrFCHD44EEVg6tgHWtF9iBcGoYJo8WEMZgTE6/5n2uMq/ynf/on85LBC43MtisVW2IlesY+a+DYbT75yU+6n//85xbUaosrvPSRGBMK6g6M4gXeK9RR1xIvXn1GOkJKYuNjX8GLBbuNFerXPfSx5KIPY+J2IXJUELgAY0O76qpPFSLwaFxWfzzWuF/2Yz1LFsb1rCZ9zPcLNROSKwiVuU5hxI2l2uQ6dEVOBRSeee65Z03CvEiSpB2OR4nhwGbQi7gl5qaoM0x95j1pqeF/fdvYL7H7QaixiaQ2j2g91NiY7oNAwa3ivAJs01CEAJOoglaCYeq8U2PFmR9LzU1d6miqe+lbWPcers889YwI9iOmMj550slVxDiMI1orrA1bI8INKfGqS199m3V5ZK+/18MbhyekT2yyBOPiMET6uSOPUnxlKI2Fk+oBtHoTJ9LSo2dfuXylXJi3GyeMl9AaGRkxZo9T3jZEbgpGWdylKcQO3XD9DY4FSAkxPujQSe+TBof6hcj9U+UggbEfzpI6iyIrq63+BQSD9IThHkSB1xdqBSuCUmpbipFopjkkx8ceecxcdCF2BIdu1Emve7zEfWzMxaW6sA+hYiNRKvE62ASs0E6MPMK1qP2nn3xKNqbrLG4CW0Ea3wUxomSRaBbWTTUu33yd39Qf4reCoZ6MFQvltm4l9DVGfhlE+OzTz9rx36Q1wvHEpGbuCbsy3J8dd/heboezz5f73J64j775eccR6d6773VPSjJHlQtcTA1e03jj3zLw3RPd3+vaELxs3+k/640DKll73bp0M6cz1Kc2H3HJft9Dg66fWk+d5Xht1DM4MhDfc/PNNysu6C2jvOjdcdFEAnlCwbgbZGAjieSwg4fZMz/96U9NfDz6mKMTHTP2I0oWCPr+pA7F+8UvfmEBkMQcYLvhCGErLMyGFtoUckT3ykF2bAoIFAk7sSFAhPv38wQx2z/a94gEIg2yRu11zjlnW3omEBeOHGl2hMbob0PHW9/nNXZihuDyYRBwBMBZgfF1U2LJvv2TA+qser8BYrvgk48/aba4D33oQ+YdSYwW5/SMHjO6kCjVt3978jnmUfDgmHRSdDFO1jwDf+CBBy02DI60qJ3Sr7dHH37EnH3IoDJ58mT7jIME9rhUZb0nx9ScbQkmhCWgCp89+1Vz0ydVmOWMy0vjQkCwDvuSGEk881DTI0wQC2mmDErAVVmc17i9qbG2ehMngvtIp0PQ2vPPP2eI+CplI//0pz+dBmPCUT6kwa9ctdK9NfctQ2yPynZ00qRJJl0VdQsPwND7gnkL3A9/+APzw0ft94lPXFEVNBkAWF/gRcBfLrvY87I3oY7D2IzOH3sHYi+ZLeYpVgt7ixn+aS+0CWFSPXPnzDVJi0kH2ZArDgRDSh36jsvq8BGyzdS3rzVO4R74Uf1GbQsTAvIARsSMDRGMiJtBFUpmkIGDBiZqWnFmtgEEH/TaN/ztBiNG2AvHKmYJ2PRXnAo2PpxbQMp7FWz8vOOizdiPPPII8/4i3RFMGbYS4DEYvT0lzLveycpAMCg58E5XAlueJYwBZxyQBGrtmjIY7IHZ3rNNCCY4UQETwgnC/rGYtr11v+xZCNa9Nb+fseP/Uv4BrLerr/6OSaqmIQprtpnhX2/iRL48AhJR1xGHhCswhMliNoSwERm7H9DdEDzHPW9T1D6OBgSEfv3rX5ObrA/aBBCeE42BsnjhYmUw/55xlHCl5FXDqynlxsOU1BWA3B+e0TsS4DMKJEXNRKLScIYPSAZpjwDguSKs06dPc/2U885sAr6QiYDoepARx52zsUgzYhOs/4jLB0hsxuCN1MiR3Xtl0VhwSoEJaa90TZbQVc4dwIj0RCCSe8X1kspo2PBh6fhRyd4hSRmihvu1EWhfusuziswNSE84z5i0EBiGlgqksE4FD+YeJwYI1NkKQ8A4zxjwqsOehiaBgGjmPTA0O/QMR7YzZiRPbJxBRQ2xRuomEPSgAwfXz7bUUuFWQ78I/CR4mn2Nza2ysiJR7cMAxHu1hjryn8qEgIcn6lNSiqHBwn785S9/JUmSHYLKY3zcjHuy3sSpfbsOrqKiwoJIcQUmfY3FqQAAX8g7hxRBmp5TTplsQV2odCwQMlDoGK7+2UULFrl/+Zd/MTEfNRABqiB/CwjjngCwqK0yp6fwNtXz2muzzT36MNmzsGlZ8fVDYHuQVFQBoARdkm8O13gSa5IChSwGSASksSdDgeUxi56nr6Q2YgEgNR6hGJiChKuFvWmZ3zQGs/tJxYmDB/NtecbMc2q3IWXmM8TsICXC/QMfbG8QfXTZlgA1zB3venWS2/4zzzxtzA3Eaq9ARuo30iDB2wQJW/ZyH4/EBIJkeyslEqpP1gvEC48zmCCCjYlVYg8Q5xTvAdYF0tbs2SQpHZgwQQ1d3y1zRaXzjDbljjvuMGKOjZZA69TuFsbejMixpYKvrH75PcY5a+A40lWx33jBLP/Xf/2XpcYiYBy8XJCA25sqmptZbCVkW7/p11Oo7ULAXOqkUKQ2EDp5sPC2K6lP5zkBhWhwAg5ZtKRVgTAR3JoiNq9SKWuCit0UIUjagoPoINfvE3VeSYGLLl6EHjQgHJAK9gW4W7xbcKCAip1yyqlGOONMAVniiT3qj0qZgoRAlLqVInAq1t1mv6Z+EhyMPY0URWeQJULXLHWT4AKMQLLYX5CQCCUg5dOuXTstESlHceCRZowFHo+o+0LRXF5/3V/NiQa1LcG+MXPT7GOPO8CY/ViRIk11J0npw5dga6oqIWMIxBubKwHenJZMAHMnxSadc8655hWVSophPQssZIeAoEPsYQLSGLsWBYgGdsZPP2egkQ+RPUFQMcwN6yisq9Ret7fskwaCpUkeF6zfXfGu+8Mffm/4dMKEiQZnmH7ynxLG8cUvfkF2poOS5gOsA45skk6VX2mgkeU/Ee7UAHBrJSq/JsLE7SAmkHdJwgQw1BPO2PnP//xPc5oAqeFvb4TJ9xK1IOluWMD1Ljyq9sjfR1ApnD6H2BUQJn8PyDfYTrBDkaG4txALE406jxicEaOU8ZwUNnGXIvxLP1EVgmywP6Gu3CsKY9ALmKO6RYU3btz4pOvRIg7cP+tglLKOk00b+JD9e9KkSabmNXrEM17dmXrp6dIZmmeCoO00TtmvWnrBZpSkR1ppm91K1O1UGtKaJXMGDjbkuHtG3qwwMn107LntF7/G7Hm/Xsj4QR5HEhMjpdpBnftgma4YpruV+JiUQ0hMeOkGuAWVuMGnAdt8HwRbvYYEfiOsBds58ac4sBEvCjNItho73da0IPWqvkkfqj9xolthAdW2kOLfSwFBAPrzn/9kUfIgNc4zGjZC+nrfw9VSjZEtmI1uUkuGAJQNJf/cLJ1ZBGcL50ZQb0EJdfPu29+wboPZvxZIikAMZlOBRDhm3ErcnzDeaNJRGR6ojBGogvaK4hEuCXghHAcrdo24nrCIjWiHcXri85pUUqgvL7vsI2aXYq6QGkM+vgCnFIGrjR69eijw9DTLhEyi03rPa1MDVWOEyVovoz19JZeihUoAAz/+7DrAzR61JfY2nIAw+q/lCHYKz2TXjC6RNw/bKt6j4fj65IF94+/jCrV4UFI4CY4ny0vRbLilcMK+MeTmGYVfW6SD26TsI2itkNhRKf+///dj9+l/+Ic0FZR1sMhabJ6OV7VaP5tTQ3odAyECysui7JzwiNcfiURxRFih4wfIHLFM3nSIohz8hV4aCabebqZqH3UeNhS883AbJwdZWuKN4pEOfcCzhYO/sC2RKmUwKWc2b7IU/Ki0+sl5AvtLutEy40Rq5B6kClyvzbOtJRcRZQjys88+Y9IlxJgD+AoQiUewpEEhUS4wGikbI/ZHjvZAdQAxxtPRcsEF2AbY+Od79UxSI5GPkXRSqUqnJcFHfUWSQWJGfXnaaR+QV6rGVGw9a5yvv/a6EWfsTefILoXzA/Y3XO+Bh2U8oGTWG4SbtE3YMtkDqGEK7DDZ9loSjEJfsn0EdtojrIWpL001okRi0YJ91xLHsTf3yc8BNm+kcZzKLr/8oxbHhHYjTQgd78UWNt7GTfxaz8GRAPSaa65x02ZMc21bt7VMEVB6485V+LtaxzaQYgP7UBuCFetTVBE2AcRcVEhIM+kkZbk33UtwMRwshwKSloicUxVyD24jFVe7dt1NZdNDKZde0oZ7V4QLBN6V1DNe6jAkS728VF9vqXTI7E2+MLhjiw3yv7Uo7lF9xaMHJLxkyVJzMSWgOO1rgL3uI/CYQFJUpCDg446baC73eCrifQUyRmIgLc+hY+VwwniBT9gU+g6SAlkRyE12EdzUC6SR+sx1Yz5DX9VP1I+clRSCwdM5C3Pox/TilBdtLKitLLO7xscLb04IOLYWYHPImEMK4BDggkQGE4ZkTiZ1y5gf2mBcvj+NOcRGrSuseb9NcaiBUEOck0wso6vsafG4GrUT7/PKgKsKKePMS9a+6w84Ndp7KZT8/S0JantecsqOXoBaimQiDxI8SUDaOBxgHOfFZ85dATlWVlTKBfcCIbDkyOZsVUW/MxFhMvTOUeBwv2x+kIx5z8UT4+/HOI2rMLn2SDw5UUiGwMqQH48kpEhvHMXQvXsPS4QKgiaI0o7LiEvUB1yGUSfiZHDYWGWvjvpWtP/NcVF9wtsuOH+gois4AsKPB+INYcJRhNgnJMqOOsk1IFIkLVSZqBY4i4fQgv4DFJwa4MG7R07EO8E04GZNrsOCI92bCQbx3GzTQY4cLcDZVpMnn6K0S16KZAweCePF96LWDKo87HPAI5UONE7WC4ldsePhiAPDw2nGaTuMk7pUZxdl4QC2pHvinlR6ag5Y1LXNQHA0DoJr77rrLtM6nC6CPFw2ZAuiD/dQd9gDdW0nv788CIT9FgAd9l15TzfbXS1Ccuolb66vffWrbsfOHdqEEIvdFtyZOiQIPGRqILEjbsrpQXzlgC3aKKvfXW2ECa4edUvBJgn36R01HoQJTg8CVk0FESZb90LcsMdARAnI5KAu1I7jxo8ruunIF3fyyZMteHmWTuUdo6zbLa2gvsLojzoPqSlFsBESIXASNQ0u1WTt4KiIlIAFiVH3d1ZsF8i8lYJM8fgj0JKDC817j+I3CnBB6iJi/Z13loohGCLg6kfmpTlKtG7owyqthVmzZko6HF14VpcfBsQLaQmnl/HjJ9gaMHggJXqCwzA6KeARm2prcbCEImxS9pRjw1qhTd8uRxkcccThZsPDVnBQ8KhqDljUtU1PrFHLk4UeLQWEaaiOQklLWEvNNb91HdPefH8WxrHmogWPq/mJkwBHcOoRipQHgaUFN26v1rNr4XuEIOsEV1UH4sClHTsTAbYFBTwolR9qPDIXQMBwcR0spJByyTxQZFNx0N4AuVljj0La4DRc7C2obuzspkxBdYXh+8477zKjOocHZvtSwFnyY3aBZSst0q/sLTV+j+AKl4vNo6OkA9zkrXikycf1cqWG82eM2A9Q55VMSKrncImePPlks59AoDgvBjVf9hmOVpkhgo2n2gAF96ZJYWk0O+81waOhsEhGnP4lRmm6VGy7JBlxKq+VqD9bJGUj+XPwI/CwU3s53TbALLybune3Ea3jRNA7aI09JYKGkwUEyw79415P0LAVcCgeEiXu+AU2mrrAIx1JI3+gD/E88Nlv4aWLl9qZVzgbWUaROOg+7kZ2HI3cxby6IhDYS2De/Go9YCdgpS6kAI6Xdzu2z/H3IrCu8ZKfiEXa5Kjo2CRIQ+npn/531HgvvjhFwWpT7STM448/zjyy2gUVRI2NJH1EjYUKhiMeZgqZob7ju6WjiRGUNjCGfwjAGnlvjRg+IrWvWTNhzHwOz9XWfmP8rnaxD5ANA8npBCHQXnKDT7l/9eWdpe9YEB/qN+wHlt0huEbX0AcOeOzXt58RKJxRODhy0MADE+nVzwFrYL8O+1n9nNBpWb71m8W+sB5UCCoEJqnHXw1tNsZPqOogDvQZ5sNyjwWborq0dvVaSXsPmQSZEOpRVWsr2wGb12QcrD/SWyUpsqbKM29Dkm0jEDXdynoi6wbrCOLE8TFWIs4X26DV6OGT3FDGX+tLGfcVuyWsyezznjC9Nect0yCgkiSEAnV4epxFsfryazkEikCgZRCnIh1rtEvaQHidPSUVFPE1qOjsYLtoc4ZgXJAAsTrYC/pwoGGWQGY3Y7aT+p1UPhAkbGRIINihOFivX/9+yd26BwkN7pnzq7ArkCHB7FRRn958/U07Dwo7DTa3kogkeibbnTp992NbKgeI119/3RLzIt1ZbJr/bcG8+e4xOTigpjnxxBMkIShVUR1Ub9SFxIotcaZc+Tkum3Q/lrPQFzwZyajx5ptzTJ0FITIJSgj5uWefk1T6nGVFx14THGbqNM663KxxE3CL6zhqW9SRMTxWyEHnvvvutyPVkbKJySMw1+32APNvaZPxXOkzdfWXzbKDCDISO+uPgxxNUqfoHuAxUwdtEsSLFx+2TpyFNkraekpZ3tEGdNU9tebji9v2ddcFFAX3hrqC1BQRq1kzZhlzUVFRYcTassZAtLKwqHfj+YPvFwjs28SJDSGkBnJZrDQyIFtUainR0W+kUMGjDFUTahTuMe+ouISNFTZjLasDhA0iARETk4WaEFdaELG17Tl/iCQJdJHouH+9slCA2MjE/rOf/cynRjraJDnb3AEJ0H6Wew0II/teS1/Tn/UcUhNIElsTxnxDeHQXD0eOcBbSgfhOlgRRMaQyHUu5TdgY9ML5AakIIjhXHoE4p3QU8mdMGP6RsFCPguQ4xI+0PnDiBBECqxALlNqtYlhkx5/9Xm5nbdxJmiKkIs4WCqfTIsktXLDQ+gTRQH079OChidNCjIVpOy7xdz6zDkSgYGaQMoD9MqlK+8lpJhzMiRNFK2F3pHpggiMNsWe3336bhVZAyNEEENxbrcRjDz+Ga9VursMFP444m8OunbuMmeLYEOYHQm6esH6cOXGqA3zzWw0CzW9zaoqJiJAAuc3IcFBZWWnOFCGbBV5EIAMQJFwxKfrRj6d6/7hfZRKlgqGIWwSZ4SxA/ajwQCScCNxDEfGhsInRzf/kJz8xxI9UwUF+L7441ZwMcCAwQhRKGFtAECIc4SwqJBqkCeohhsvGGsGioJ64s/4eVHnYTSoqKi3GhoJzBEgRwzw54zD2p7nk6gqXQETULbInoPKBAN122+2G4AdXDv7/2zv3GCuqPI+XIoOCr0FABXbSrfJW5DGCb3HHHZ24g3E2LuqsWdfH4CYSZf8x/qNuspPMRrPsbLITM84aITGzihp2dtVgdtTRQWdHdABFAUFafDeCzRsalP1+flXn3rp17+2+j2robn4H2+5bt+rUOd869fud39vuSbAvTIpM76GqMG7tGzasN09N5lsiNSXqpDClvbv3mocg9kUaeJA6yFS0AY9KY898R4FMpBZyvyEJWNN1jIW8eTAU7Cmj5ZFYE87pe4a/9Rvb2ziVGmfOxJUtXbrUJNPR2swgdUycOCH6jeLj/lvMEOmfNYKUBSNDQodBlrT0M9cXrHVqlYV0Y+CBNG7pkbrCo7TX0k8atz0D/cemBg0A7xIbPLw7S+yFCW5ddeffOQJZBPonc0pmSYoidto0gkKDQRlPM7zxQlliPPfQ6Qdjbhakuj8HwqMXFymMXfcpkpIoh0zuNKqnWiyPmmVdUKOSKsSDF37r1i1KQ6dijsqYAHPieDZhLIUe16x5T7att6P1ItoQUlyUSTMEwULFxA6WeZe5tqcnlBAnUkfBQPGWtJIfYmzEMHEMyY94G6QpUs1Yq0Tc0/1W+zuFDZsBHCMgbOSqu0olJDiGa/rzz5PxfpMRXpjB3n17dcuDpkLjmOW5gzIm42eslDZhvO8pAJYClwRJQ/BPOOFEYy5IGBM1N9zZkVispew3hSEnBBeV2efyOCNwEW/CSBITiVnxUhypPi688KJY9RguZG6MJxlTob9a/tA1ZESJNyevGIO68sqrzGEGe9cftTlgPDxj1giVS2lsSArpoFL3hSEh8YUNGB6QZAoAD5gqzxNHFp41fxdssPU8V0G4y2oC/dYkXNYHGJcwpnr6qwUnP+eIQaD/MicRnZUrV0UfSQ0yRXYm4pEwHre1bTTix4s6WW7cEyZMLHrU5f0iJf1hU4EBEsf1irzcyHbBjhtnAjz7CMrEbf3rg1+LPxbFACQEJADzVKTpK5gS7vDxThXvw69NMsO7kN8wMpLUoqrEow41JtIZO9qCQT0s74SYHTRc2iznFu7LMCAcRGDsEET6mDHjPF0fM9Jc3o6EKYwcPdKCmwlOfUlSFE4ASFIvqUjlXrlnQ3iZI4yJBnHev7/TiDJYwZTYsYdA4O3bt1niWYg8Eg/QwdCw/aEaQ2UGEWV3jzqxmsRDwC0EF887MqrjiPHOO6tNgsTAf8kllxbTOaWZUWBQ9YCUuoZNC3jgZg6DOlpqYNyxkdT2qJwGOBwVbFq6B4w6SIjcEqkRSRcGym/Wz2BJSazBbwsPHDK4hgDf5ZrLUEnI06dPl7fgZVJNytWbuXT3HiTfE5rBfeIinRfZ2rE4wEYwqAcvP/eIQKDfMifcnFfr5RwlZwOYAA2mxE4SF+nzzvuu3KTPjHfEfNndC9nschCRwU5zgjz5IJTExPyPghNfVoE5CM83kpQgtuyKw04YVQwMyoYmovH5p59b7RuCPE+Wo8S0adPjjAoishBipCYIDznZkBzYOcNcyErM3MmrRb2lbCMrOOdjBxo/foJ2w7tsTKTUnzRpoiSwKXE5kDwxShF0pEd27wsWLDA3csaN1BjTuPhEpCQIM8ypUxIxTgH79uw1hwSYPQxojAr+jRnz5xbIjVoShg0TQ4Jqb99sDBg8SCGFHRI82CQUVLncUBuATjFFVMFcy/cUWiTrwwoxayRRY/TZQOtmCXLAVtNFHUwc1OLFT2ocH9tYdu7aafM3aTG1WHnebEho5DF87rlnzVHiGKnukIpaW1sN26EqSUItLtYWawpnHZgvz5hNDOuDDBazlJ3/OMIfwnyyzzy5/ZeS1JfKvrRfqlPi2MDFGFOQRLPXZRedf3YEukGgfzCn9PuqvyEm7Bpx00W1RWOHR90mPOPY4WGUPwqbzKF6iZL7IJWgunr00UdNWsD5gJ1vTHQQkg7q/eYNl8pE0h2OGhzb/Hm7DOC/MtUgu3521+zqcVsvqGRSDxsbGipBiA9GaggQfVG/BUmoIDHoVh9+2GbSFvFfEH8YE4Z3VDQQ4kLphhRD6WZddf91eGYJEdwntR0bB7IwgEDYwmcfD+OjHAcMmHkhdYay8djoUM+WlC9JRjJRXSJFYINkLRDsu2jRIvuWFEMmsCbzQ5WHKpEYLyQwpDLGhRoMKaMkgz0dZAeZ3LPhX+pv+47tGsP6aKMk/SA1hjXCQGP44lI0MNEvPvvCArt5zuOl0iXbO+NFhWcxV9lnJzzY/MCkyGrxvLBcuHCh1txOMe3Z5eU6UtdTJJKqvWwI8Mg748wz4qkmz7LhefuFjkAKgb7PnMJLk3p5UPNgT4K4doqYQYxwRmB3B9EOdp5DvhI0RhwMSOcCIW7/st2GkFbl8TmeylHRrj27bNe/TQzsiSefFNP4P6lfZkVzVNzxz74jY3lMoSpOA5sR3lLTvjvNvAYhUq+++orlOINBWcViNSQmKv1ip0IagRB3dHxlRBj8zNaSJjp5EeJUn2vfWysV1gvRF4qdghDHX1WmdBDE3Yr9gbEzF6QC5sMmpBKTLoAj5oPN8SxlKUBCRLJCgsLpgrlbnju13ZIaYURIsDA97G1IXOTJmyHm/S15z1W0U1V8CnUe1PNEamUThat9Oit5kTGV9snGhQ0ONjtUjmwwKNDJxqUkyDk8t/CeCA9sQ8Rt4bKP2nuxGP2SJf9l6bi+d8X3ygcvhsa7BSZI6cS5tUob0NU6LO/EjzgCtSHQ913Jw8uW0LJ2SRiozNjxotZZ/e5qqboOWHwTNqYyN/HacGruLMaYjJM0N9SpguDBOONdcJhEfBsIEf8g0xecf6HtUONiYReoHMWNpYypMg2PO0q+oywBmbBRdeIFeKIycowdM9bKiKNihHFjR0FaICs85QxQCZVkOeiCETYMjvrEboGnIkwCu1uxVlcpJtyDI6g/x48dZyo2VJk33fQ38mqcWsqYKmESiLP6QBIEDwg7hBap8ZyzZciXXWaPy7KDAAAU20lEQVTduvdN6gYPxkOwMRIZPz0iQabB0wTXrllriY+xN23e3G7zrcaYuJRcjcdp3NgH2XyxPs6SerNQzLCGh8NzJiMHeMYOJe9FE6TepZyJga4fNlVrpBL9nTZ6MDPiuso2SOWPrIa7+ymOQGUEUGb07QbRSYhRSD8EsUH9g+2CgE0M4BDbQnAjMw7XHeLZY1Mh4wBjjIcdM6FYYghyQ3FQMDHsTLiGUxIctWRgdHZWIAjp+aQIcejptJGnmTcc/fz+98skWaoUiQgvai7UivzGdoFn4TgRuUL5j3T/eWKV9IsEifG/bVNbiWG/0q2I92FqBAK3t39hO/dzzpkcSwhh/mHu6c8V8CCtFI4i50gigkFD3AnWRg2KOhSG/dVXsQRpjAmJKX5gpfhXGmitxxLCHxgAl2ETo24YeOyX+rK7xjMkqe4333xtUnVra2txfLUwi+QcpCykTzBlLSCZ4kBE2ycni5WySb0hr0G8FLlPIai8uwH6945Agwj0feaUmjiEHE82Xlj06dgMKGvR0tpSlsetS7xqeam77CD1ZaoviB8SENIJrasdcegB4g3hxPaDO7TZRrLENv05+x0dJcfOlnSA2gevPFIUrVVg7UZlU4cIE/cCE0cdZAlXafxK/8RHc/1/7Lggwq8WbCvZG6QlSb57//11FgOEqi128c5eUdtnGLbluJNtEgcJNg44BiAx4XINoybGKM6CIRArYVvbrWo+i7EQh1Rs1RcjuCANo5qdNOlsszHZ+git1vEm58GwcQBBAqNMDAwa77/XZYN8SwwcR5NZsy4vdwap9341o+EnHskI9H2bE09P7+8O1YQiqSiGfLzzCH4l1qeQRSD7lKu/8/GZ3X2f7a+Gz6gZcUZA5bhD3mhBddfVpetEMEfLroIrOteVNMZYjQCF46nvcRTAjgQBfvnl31lgKp5bHIMohVx2hT57AIPC+BmX+keipSYWDeYUGHaWccMsvpZ0QCNBMNeFQGFjoPW0gImu43lAdNkA4DDCBueSSy42pxPbDNC3CRCpm1TDvJ4xcG62H91i9OhRpnbtam3E2MQXdx7otOeGLZW8hYWW7bva2NLrRPfH9sRa+1+9S+acItUnkj54s7Hx4NpqQPrxvBHo+8xJLxT68KVSDS1ZssReUKQAdr9IBLFbtshe8hKmixhWAtNIkIzhGMRx5+Xv0IKLt30nYpluFi2fNP4i0SXHwnHrSv9CvSqYUzVJIX18S8dWuX+PNaJhdoRGW8IM6IedOR55SGOocfC4KlRm7ar/4hS7Oquu79g88ExCq4YJOB4rqQIUh4hJj5KzAiEBzTarlyTizsYG2x54zJ59TTHYmBv0wLyrjZs8e2k8sufZ2hIWrGee4779+8w+Bh715DnM9hs+I7mhOt4tz0YwiYO5J1sJEJL3WquV8VW7iR93BGpAoO8zJ00yZA0g+n+YUuxgO8Hj6aCMyTFliZkTzCUwi8C0uqI8WQYUmFP4XQlfo2MJo0ozrHCupZFJ0upU2x1DoPHgI+nnAdW4gghBNPIgkvSD6zEuxASzUk8Ib0bmlGa+xbEXGXul+VTCoJ5j9MlYDLYqF4LHAdlf2HRY2h1JoMfq94ABzS9fsOUHjzdLFaX2hz+8br8pG98Tcw7TLF1HBy0OCSmlo2NbVSxsXLauIyUUHmjPkuBrMLEGiE0wD6pMI6EzNt4jmFLs+Sesm+g3Hpz/3xGoHYHm3+5wr0qUhcXMT9jwJ7v3suE1uehDSXDykfF2pglt4V562b7hRy9zScqXzGC4lu9ta5qShtKEBOMztX2qN/qAqHOv4uQgdKiOGAf/AnMKruQhvmn4KcNN749qEtddEsKanaoaftUHUvYN92dauIpPnTrNJDxsUDbnVIunXumhlnXZ0IHQP5gUMxxk70fEU4zTCMUv4ZUHISZ4lJgl4p2yNaHqHQyOM6g2sU+CN8x7i6oAx3jkAHgdAyKPIO7jMIV4hcS8JvCbcGzAUQMsPABV2w6db1nK9VzzaBT5pC8YFB55EyZNiLtt8h3NY2zex5GFQD7MibdHtA0ihzqNFxt3U6SYkp2cziNAlp0yLyGpWeKaSUlhtkaw10tDTE8h51sjffTUNdkXWvMn/RASWdqmAlOCQQ09eagxJQJscU5Ax48n2yOPPGJxW9gCmiXGeGKRFgjPvRtuuMEIW2C8pTt5eHOWWeQLFP1zT7Jrr123pqRzMAGjU759SjRZNrFLlbmAekpInr/4xb9buY2dO3dFQ/Gia6QlUyOmiyzfeKrdcsstFvNU2KCo357GID10nDuW/3G5xcFFGwJLivckYTNDCRXGOksec6wRqvMSmI1jC3YnC0AuXlo3MjB9mD/zbm1tKV7fRJ91D8IvcASEQPPMSYs2LlC30koIECPCOp4qe8ZNN91kmaYpPYAROy5l/aq9fG2KMsdm8POf/1tciqCZx5FlAs301ZPXpsYJsaENEC4EPZIxnZ0q9h9UKRZjokaZ9Mcff9yKF2KkL0ubU+t49VDIl0dONQg8WSosbqcXNGNSGkfsVB8HJQ8fNtzUSZdfPkvZy79vThtgQpAqmSHwNMRTzWpzNdqU0X2N4oqssKKcIGBMXQbyNnqfOq5DegKPwAtYJwOlvsRpJEjTbF5gUGzIsLGSxPgdJbxlfZCr0FojzEQbTJLtskZwNjEHGW+OwGFCoDnmpBfggBJvvvXWm8rf9pi8e1bLrfWzaOOHGy2LMi/O3LlzTVJAfUIy0oceesiCZPd27o1GnT7KIs2PqGa6/Dh+hUScU5S37vwkEWmhZlAARBQbdRMM6k/ysMMeMWPmjJjw1AMahEqNeJ4VK2KXYJihtcPF2JMx2RASGwpDGTZ0mDEiUgpdrJ/pYtTDRkgCT8ZK6iCqFP/61/9pYQOtLS3RYB0L38d/VPl/hmC3SUIg3Q/MABVnIcdeamw19VvldnUf1n1j6TVWZ3L9iGGxJx5SEmpHYq5Cdg++5zlOnnyuedaxMUTdZ9J1vc9V9+6QAxExcHi8EswbvCjrnodf4AjkgEDjnCF5gT9TTNGzzz4n1cjn0W233W76+wUL/sUkIwL5rr/+ekudQ9zEAhXQI0aFnd/HSmaJdEVUes0tQ1xqvq43nShCyEtPNnSCQH/wg6sUnzIpGqpdu+X6yzQ8sEhQym6WshowfDIA2M64lpac1/ZBm5KCPmcqV6QRPLwOawvEU+NDtXjSCSdZCMCVV35fHnNXmAqzpMhi6nwygr/++uuqBfVb8yxD6jkm5I/rjigneHRs/cquX7durVRkl5tKrKzF4lzZ4Z48AKMcqMrJMGkkR2peXaFUQqyRSmpyvCxR0aKKI18gQbLEPNXluSdMCLQlewg583gOSPAFW3FPTtj7dgSqINB4+qKEUbR/0W5R9ezsYES4KhPlvuqdVdEgvWS8OOSHI+M0gY433/x30fz58+0FQFKgHEHNu7xaCXKVyfaGw+yMUauh8rQkrGI0VD0NFXJtjOl56u+hUvthC0Dy/PLLzaqcOtxKvKdLhlecG/3IIWOtSr4vXrzY8vnhKk0NJatt1R0hr9hpzgc1Bnb8pJe69dZbzQ5mdY2ESYlqirkkuAw5fogR8OUipgTODhlyvFWT/dagpJhgtXWSHCe7O9nMsecR53TjjTcUqtzmPLv6utP4dmzfYUUEp8lhBRvYX193XTR2wrgiHtkedQ1zRzMBw0ZFyYZvmNaHuX6ncCu5NBzX746tHeaxyeYHLcfNN/9tNE73LGm9Ya1k5+6f+zUCzTEnLezjRRjGjRtraWBQrxwtGwo52kjbP0AqO3TjSE1kRvjRj/4qmjdvntmhyN3V0tJSNDindsb9FnHNETUmhJiX33LX0bp58bnmjNYzzKtt2bLXLFErRATsj1VcTIFJpQgRZeEpAQ/BelJJY/H6w4h+zTXXxDvwXgQyyVuphHvulHNjBlPD2EaePtIyspMLDnUlzBs8Bh8bu5ibI0MKj29k94Twr1ZNJuLhqMCL3e3HP75RtpyEEB8GSalkqhrvILmTwzDZRCDNDVJy1iRJfVVUeP7gwcYHLFDxHZDHX7w+BkXH4HKPUJ7CgxyGu5X1fv37602ixg6MvYvNwcyZ5xdjrbimm/VZdWD+hSPQBAJHaZE2vvRYuJlGPi7Kjt911122m0NyIqv2d0QIHnjg/mg02bRDC2FI6oe8eEZQyjVbxfMbH2l2mIf2c/oFD5gxl3oYsq7DGYDaRTieEP9EOiLciTFc42JNvA5u7pTaoFgeNkAK5KE2g/gjMZn9Jnvfw41rGhOeTC1MQtew1ggmfvrpp6zuEZsd7FWo+k5SslsCWiHYMC4CspHwyQTxlbxKsWVde+21VgNKC6+2e/b0qgk4pO+TflaVnlPqGqrfYkMjWzsesWeeeYap+MADRg7zoe2R6p0aXh+qVMoqVVLGsQRni+skpcEQS5LGVhpTT+Pg/TsCQqA55gSELN4MsXvzjTejO+6YGy2XowSuwBi377//fivfUGiBMYU+whc6TrZudn4QFhgWkoPlUEu3Si9q6Rm951MWo0bHnhBkpAVUfGslDVGEjtLqMCakKTAjKBnbEgGrRPhfJjds3LEHhvIXaWQC8Wl0TIcT5WTsm9o2yZD/qhwk/hRnNhcG4BHc5JE4KcrHMaS0iy660JxMTjz5pHj0rMV0LN7hnFOj9w7PUXNB1YntiHWybVuHecrCmI5RMUC2omDBGmG94ECBep0YwRGnnVrEI4zDmVOjT8SvaxKB5plTdgBazFulTrpj7h3R4meeik4bcVr0jw88EP3k7+cWF352weszbs7YUzZt+si8hQgEDER22DDFumhHN0ZlHiw7NNf3RWKaxaqRzwl2lCcnwS3qOiQCpIF9IjgwclyiMabDmKhJVGDsIc42i38j4+gt1yRzQYoizo5M9Hg1kgwV5xwIMJnYSRs1fvw4i6s7/sTj49H3xzWUWh9pPFgrSJCk5KIaM+uC9YFqExtVIQdlWCMg1J/WSW9Zrz6OmhFo3Fuv0i2SxUz2hIGJCoEd2/Aa4iWCQfYDEVpS0xyndCxEq7e1tRnRIaPB7bffbju8445LjOWVxtDfjyUElWBLCsXx8xf6V1Prj8QmwYNAbFSW/Mw4X+723bX+yJiYc1gfsmeeevqp9nPBRRd0h0aZ9qP7C/wMR6BnEciPOSWED703cUx47NHC7s0+dEEQsKP88le/jC6+8OLo7vl3m5R0rKQkVDU//ek/mScRkhTOFy0trT2LSl/oPWCZZjhd4GtTCud2d15fmH92jMyJH+bIT3aO/XnuWSzC54BHFotq5/txR6AXIdA8c0oRAlITERi5cOFCMZJ90XDFamATwb2VzOFlQY7h5REgqF7OVizHPffcE139w78sQIQjBUbslStXGdMjKaarG1IrqBrhSRPjI4UwdzXPLE6VGFgvejFzGUrA40iYay6AeSe9CYGufOO6H2d68UtXje1j0aJF5il26623RVOmTjHjKwGCVC61xjVpvXZyFzyFfvazf7YA0ZKm89GJY9zGwI8NwVsNCARJglPTf9dw6RFxSpZZ9cdJh+d+JMy1Pz6/I3xOjVH6ZLF3KoEoAXzk1tuwYX306KP/Ya6sc+bMsZ+zzhoTHZBrM0ZqHB1onfs6LRj0k48+KZGA8OS7+odXR0NOkLE69TJ9+vGn5i4MU7rsskvN2F9ogTke4Q/Rp+8IOAKOQH9DoDG1nlha597OaPnyN+Wy+rLZgFC9PfXU0xboiRQ0XFkMyAs3aOAgOTVstPx7eNyRpJIs27Nnz47m3DCnDE/inXD9xdOKDOdPPPGEucTiCIE0hutrwabgO8Iy/PyAI+AIOAL9AYHGmJNmTsmLF15YqrRE/xqRwBTPOpJSzpt3p2VGxv5EfjRKZ3zy2SfRY48tNPfwF198ydL3tLa2FvEL6gcxPRwo3lDS2LffXmWxGu+ufje6RNm6fyJPvZmKTSnYrfoD+j4HR8ARcAQcgYoINMycYDSk0cGDbo+YDUGN5Mw7V3Ym1HJkRkZSQpKixAGqPNzCpyid0b333lvq7puSgGBMeOdtV6oZsiSTLw139M2Sorap6B6p/N0houKz9IOOgCPgCPQbBBoOwt2shK+/UT6uDz7YoGSvp1sWiKliTFRWJW8XiUwPSppat+595dVbYl52MBayFUxTNnLiUsocI2RDIh8cEe4EDFIEbtmyZUpP87TVfrrzznlKSnlzXMPHPZD6zSL0iTgCjoAjkEWgYeZkHVGCnLxkabeKIAXxGwYSnBYqpYjhu2zWgoyTw/aO7dF9990XPfzww6YixE0dj76jlYqlLJYlOzv/7Ag4Ao6AI9AnEWjMW4+pwkRUa6iMMQXmEn4HZlXtTpwXvkszpuQ66tXgPEH9IWxXr732WrRjx84+CbYP2hFwBBwBR6A2BBq2OVWVWlL2o8IQKh3jyyBd6c+P5GpOktdRo0bFeb5gVMn3gwcPllpPNY/0D08+kpp6cwQcAUfAEei/CFSTZw7pjEn6+uCDD0Z33323xUQVWiJJfaSquVu2fBmdfJLKmssjEEblzRFwBBwBR6D/ItC45JQjJvv3x4G5L778opV0J4VRcDXHe4+sE7t377HA3pkzZ8aZyVNSV1UpLscxeleOgCPgCDgChw6BXsGcKLlN2XaSxT7zzDPRihUrxJxalProgLmf79q1yyqWzp//D6r4eXoRnWrqwkOHn9/JEXAEHAFHoAcQaM5bL68BSX23ZfMWi4Vav3691eIhponaRKeeOiKaPm16NE0lM4aoDLw1Z0p5Ie/9OAKOgCPQKxHoHcwJaNKeenxOq+3S0Dlj6pULyQflCDgCjkCeCPQKhwibUJrpZO1JOOc5U8rzuXtfjoAj4Aj0agR6D3MKruPBSzwdnBu+cwbVqxeTD84RcAQcgbwQ6D1qvbxm5P04Ao6AI+AI9HkEeo/k1Oeh9Ak4Ao6AI+AI5IWAM6e8kPR+HAFHwBFwBHJDwJlTblB6R46AI+AIOAJ5IeDMKS8kvR9HwBFwBByB3BBw5pQblN6RI+AIOAKOQF4IOHPKC0nvxxFwBBwBRyA3BJw55Qald+QIOAKOgCOQFwLOnPJC0vtxBBwBR8ARyA0BZ065QekdOQKOgCPgCOSFwP8DaaQeBGp9NCkAAAAASUVORK5CYII=)"
      ],
      "metadata": {
        "id": "pKn1aoF1JrWE"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H-xenhlRaPm5"
      },
      "source": [
        "regularizer_histories = {} # new dictioanry to store regularized model histories\n",
        "regularizer_histories['Tiny'] = model_histories['Tiny']\n",
        "regularizer_histories['Large'] = model_histories['Large']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QqfUn2sMXQoW",
        "outputId": "177a038a-7a52-4b41-e668-a530e02de848"
      },
      "source": [
        "from keras.regularizers import l2\n",
        "\n",
        "l2_lambda = 0.001\n",
        "l2_model = Sequential([\n",
        "    Dense(512, activation = 'relu',\n",
        "                 kernel_regularizer = l2(l2_lambda),\n",
        "                 input_shape = (shape,)),\n",
        "    Dense(512, activation = 'relu',\n",
        "                 kernel_regularizer = l2(l2_lambda)),\n",
        "    Dense(512, activation = 'relu',\n",
        "                 kernel_regularizer = l2(l2_lambda)),\n",
        "    Dense(512, activation = 'relu',\n",
        "                 kernel_regularizer = l2(l2_lambda)),\n",
        "    Dense(1, activation = \"sigmoid\")\n",
        "])\n",
        "\n",
        "regularizer_histories['l2'] = compile_and_fit(l2_model, max_epochs = EPOCHS)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_30\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_137 (Dense)           (None, 512)               9728      \n",
            "                                                                 \n",
            " dense_138 (Dense)           (None, 512)               262656    \n",
            "                                                                 \n",
            " dense_139 (Dense)           (None, 512)               262656    \n",
            "                                                                 \n",
            " dense_140 (Dense)           (None, 512)               262656    \n",
            "                                                                 \n",
            " dense_141 (Dense)           (None, 1)                 513       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 798,209\n",
            "Trainable params: 798,209\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/200\n",
            "33/33 [==============================] - 1s 10ms/step - loss: 1.5994 - accuracy: 0.7670 - val_loss: 1.1589 - val_accuracy: 0.7744\n",
            "Epoch 2/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.9468 - accuracy: 0.7921 - val_loss: 0.7902 - val_accuracy: 0.7867\n",
            "Epoch 3/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.7001 - accuracy: 0.7966 - val_loss: 0.6492 - val_accuracy: 0.7756\n",
            "Epoch 4/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.5976 - accuracy: 0.7997 - val_loss: 0.5917 - val_accuracy: 0.7828\n",
            "Epoch 5/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.5497 - accuracy: 0.7992 - val_loss: 0.5461 - val_accuracy: 0.7839\n",
            "Epoch 6/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.5177 - accuracy: 0.8009 - val_loss: 0.5231 - val_accuracy: 0.7872\n",
            "Epoch 7/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4980 - accuracy: 0.8023 - val_loss: 0.5078 - val_accuracy: 0.7911\n",
            "Epoch 8/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4854 - accuracy: 0.8017 - val_loss: 0.5111 - val_accuracy: 0.7789\n",
            "Epoch 9/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4749 - accuracy: 0.8049 - val_loss: 0.4999 - val_accuracy: 0.7867\n",
            "Epoch 10/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4718 - accuracy: 0.8003 - val_loss: 0.5011 - val_accuracy: 0.7800\n",
            "Epoch 11/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4696 - accuracy: 0.8024 - val_loss: 0.4887 - val_accuracy: 0.7889\n",
            "Epoch 12/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4618 - accuracy: 0.8057 - val_loss: 0.4874 - val_accuracy: 0.7867\n",
            "Epoch 13/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4608 - accuracy: 0.8027 - val_loss: 0.4805 - val_accuracy: 0.7883\n",
            "Epoch 14/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4579 - accuracy: 0.8030 - val_loss: 0.4870 - val_accuracy: 0.7839\n",
            "Epoch 15/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4539 - accuracy: 0.8057 - val_loss: 0.4768 - val_accuracy: 0.7878\n",
            "Epoch 16/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4535 - accuracy: 0.8055 - val_loss: 0.4822 - val_accuracy: 0.7828\n",
            "Epoch 17/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4507 - accuracy: 0.8054 - val_loss: 0.4767 - val_accuracy: 0.7872\n",
            "Epoch 18/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4472 - accuracy: 0.8072 - val_loss: 0.4746 - val_accuracy: 0.7894\n",
            "Epoch 19/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4465 - accuracy: 0.8072 - val_loss: 0.4793 - val_accuracy: 0.7828\n",
            "Epoch 20/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4492 - accuracy: 0.8033 - val_loss: 0.4791 - val_accuracy: 0.7794\n",
            "Epoch 21/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4508 - accuracy: 0.8053 - val_loss: 0.4732 - val_accuracy: 0.7933\n",
            "Epoch 22/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4445 - accuracy: 0.8071 - val_loss: 0.4720 - val_accuracy: 0.7872\n",
            "Epoch 23/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4419 - accuracy: 0.8065 - val_loss: 0.4761 - val_accuracy: 0.7861\n",
            "Epoch 24/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4425 - accuracy: 0.8073 - val_loss: 0.4776 - val_accuracy: 0.7806\n",
            "Epoch 25/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4421 - accuracy: 0.8077 - val_loss: 0.4771 - val_accuracy: 0.7850\n",
            "Epoch 26/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4414 - accuracy: 0.8059 - val_loss: 0.4705 - val_accuracy: 0.7906\n",
            "Epoch 27/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4406 - accuracy: 0.8070 - val_loss: 0.4760 - val_accuracy: 0.7811\n",
            "Epoch 28/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4399 - accuracy: 0.8093 - val_loss: 0.4705 - val_accuracy: 0.7872\n",
            "Epoch 29/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4403 - accuracy: 0.8085 - val_loss: 0.4744 - val_accuracy: 0.7928\n",
            "Epoch 30/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4409 - accuracy: 0.8085 - val_loss: 0.4753 - val_accuracy: 0.7817\n",
            "Epoch 31/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4405 - accuracy: 0.8061 - val_loss: 0.4789 - val_accuracy: 0.7822\n",
            "Epoch 32/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4409 - accuracy: 0.8069 - val_loss: 0.4687 - val_accuracy: 0.7922\n",
            "Epoch 33/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4346 - accuracy: 0.8113 - val_loss: 0.4709 - val_accuracy: 0.7872\n",
            "Epoch 34/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4351 - accuracy: 0.8090 - val_loss: 0.4716 - val_accuracy: 0.7878\n",
            "Epoch 35/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4354 - accuracy: 0.8097 - val_loss: 0.4903 - val_accuracy: 0.7789\n",
            "Epoch 36/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4401 - accuracy: 0.8064 - val_loss: 0.4675 - val_accuracy: 0.7922\n",
            "Epoch 37/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4335 - accuracy: 0.8125 - val_loss: 0.4697 - val_accuracy: 0.7889\n",
            "Epoch 38/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4310 - accuracy: 0.8132 - val_loss: 0.4743 - val_accuracy: 0.7883\n",
            "Epoch 39/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4354 - accuracy: 0.8118 - val_loss: 0.4900 - val_accuracy: 0.7767\n",
            "Epoch 40/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4392 - accuracy: 0.8096 - val_loss: 0.4779 - val_accuracy: 0.7844\n",
            "Epoch 41/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4344 - accuracy: 0.8103 - val_loss: 0.4795 - val_accuracy: 0.7822\n",
            "Epoch 42/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4306 - accuracy: 0.8134 - val_loss: 0.4759 - val_accuracy: 0.7822\n",
            "Epoch 43/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4331 - accuracy: 0.8115 - val_loss: 0.4830 - val_accuracy: 0.7844\n",
            "Epoch 44/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4301 - accuracy: 0.8134 - val_loss: 0.4762 - val_accuracy: 0.7794\n",
            "Epoch 45/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4327 - accuracy: 0.8131 - val_loss: 0.4727 - val_accuracy: 0.7889\n",
            "Epoch 46/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4307 - accuracy: 0.8121 - val_loss: 0.4781 - val_accuracy: 0.7900\n",
            "Epoch 47/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4285 - accuracy: 0.8152 - val_loss: 0.4766 - val_accuracy: 0.7839\n",
            "Epoch 48/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4308 - accuracy: 0.8143 - val_loss: 0.4974 - val_accuracy: 0.7694\n",
            "Epoch 49/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4330 - accuracy: 0.8114 - val_loss: 0.4749 - val_accuracy: 0.7833\n",
            "Epoch 50/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4279 - accuracy: 0.8121 - val_loss: 0.4770 - val_accuracy: 0.7778\n",
            "Epoch 51/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4260 - accuracy: 0.8154 - val_loss: 0.4829 - val_accuracy: 0.7811\n",
            "Epoch 52/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4288 - accuracy: 0.8141 - val_loss: 0.4749 - val_accuracy: 0.7928\n",
            "Epoch 53/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4269 - accuracy: 0.8153 - val_loss: 0.4706 - val_accuracy: 0.7878\n",
            "Epoch 54/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4283 - accuracy: 0.8135 - val_loss: 0.4766 - val_accuracy: 0.7878\n",
            "Epoch 55/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4235 - accuracy: 0.8182 - val_loss: 0.4774 - val_accuracy: 0.7806\n",
            "Epoch 56/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4228 - accuracy: 0.8156 - val_loss: 0.4818 - val_accuracy: 0.7839\n",
            "Epoch 57/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4247 - accuracy: 0.8156 - val_loss: 0.4802 - val_accuracy: 0.7839\n",
            "Epoch 58/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4313 - accuracy: 0.8133 - val_loss: 0.4841 - val_accuracy: 0.7778\n",
            "Epoch 59/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4220 - accuracy: 0.8154 - val_loss: 0.4848 - val_accuracy: 0.7811\n",
            "Epoch 60/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4215 - accuracy: 0.8193 - val_loss: 0.4809 - val_accuracy: 0.7844\n",
            "Epoch 61/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4203 - accuracy: 0.8176 - val_loss: 0.4835 - val_accuracy: 0.7822\n",
            "Epoch 62/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4240 - accuracy: 0.8164 - val_loss: 0.4797 - val_accuracy: 0.7867\n",
            "Epoch 63/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4218 - accuracy: 0.8169 - val_loss: 0.4781 - val_accuracy: 0.7861\n",
            "Epoch 64/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4208 - accuracy: 0.8165 - val_loss: 0.4863 - val_accuracy: 0.7861\n",
            "Epoch 65/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4174 - accuracy: 0.8202 - val_loss: 0.4893 - val_accuracy: 0.7844\n",
            "Epoch 66/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4187 - accuracy: 0.8195 - val_loss: 0.4900 - val_accuracy: 0.7761\n",
            "Epoch 67/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4197 - accuracy: 0.8197 - val_loss: 0.4830 - val_accuracy: 0.7811\n",
            "Epoch 68/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4171 - accuracy: 0.8195 - val_loss: 0.4859 - val_accuracy: 0.7839\n",
            "Epoch 69/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4156 - accuracy: 0.8206 - val_loss: 0.4942 - val_accuracy: 0.7800\n",
            "Epoch 70/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4124 - accuracy: 0.8235 - val_loss: 0.4935 - val_accuracy: 0.7856\n",
            "Epoch 71/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4217 - accuracy: 0.8176 - val_loss: 0.4843 - val_accuracy: 0.7806\n",
            "Epoch 72/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4169 - accuracy: 0.8191 - val_loss: 0.4900 - val_accuracy: 0.7822\n",
            "Epoch 73/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4184 - accuracy: 0.8210 - val_loss: 0.4840 - val_accuracy: 0.7789\n",
            "Epoch 74/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4132 - accuracy: 0.8212 - val_loss: 0.4932 - val_accuracy: 0.7694\n",
            "Epoch 75/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4167 - accuracy: 0.8235 - val_loss: 0.4907 - val_accuracy: 0.7828\n",
            "Epoch 76/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4156 - accuracy: 0.8207 - val_loss: 0.4886 - val_accuracy: 0.7828\n",
            "Epoch 77/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4169 - accuracy: 0.8215 - val_loss: 0.4817 - val_accuracy: 0.7839\n",
            "Epoch 78/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4111 - accuracy: 0.8245 - val_loss: 0.4956 - val_accuracy: 0.7767\n",
            "Epoch 79/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4124 - accuracy: 0.8264 - val_loss: 0.5000 - val_accuracy: 0.7783\n",
            "Epoch 80/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4139 - accuracy: 0.8220 - val_loss: 0.4998 - val_accuracy: 0.7728\n",
            "Epoch 81/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4121 - accuracy: 0.8254 - val_loss: 0.4945 - val_accuracy: 0.7844\n",
            "Epoch 82/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4104 - accuracy: 0.8251 - val_loss: 0.4918 - val_accuracy: 0.7828\n",
            "Epoch 83/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4085 - accuracy: 0.8274 - val_loss: 0.5166 - val_accuracy: 0.7706\n",
            "Epoch 84/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4110 - accuracy: 0.8250 - val_loss: 0.4948 - val_accuracy: 0.7789\n",
            "Epoch 85/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4072 - accuracy: 0.8309 - val_loss: 0.5078 - val_accuracy: 0.7689\n",
            "Epoch 86/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4043 - accuracy: 0.8295 - val_loss: 0.5022 - val_accuracy: 0.7750\n",
            "Epoch 87/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4099 - accuracy: 0.8283 - val_loss: 0.4913 - val_accuracy: 0.7811\n",
            "Epoch 88/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4047 - accuracy: 0.8295 - val_loss: 0.4982 - val_accuracy: 0.7728\n",
            "Epoch 89/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4061 - accuracy: 0.8309 - val_loss: 0.4986 - val_accuracy: 0.7872\n",
            "Epoch 90/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4087 - accuracy: 0.8277 - val_loss: 0.5003 - val_accuracy: 0.7811\n",
            "Epoch 91/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4066 - accuracy: 0.8278 - val_loss: 0.5035 - val_accuracy: 0.7811\n",
            "Epoch 92/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4060 - accuracy: 0.8296 - val_loss: 0.5030 - val_accuracy: 0.7728\n",
            "Epoch 93/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4096 - accuracy: 0.8278 - val_loss: 0.5088 - val_accuracy: 0.7711\n",
            "Epoch 94/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4054 - accuracy: 0.8293 - val_loss: 0.5105 - val_accuracy: 0.7761\n",
            "Epoch 95/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4062 - accuracy: 0.8299 - val_loss: 0.5076 - val_accuracy: 0.7800\n",
            "Epoch 96/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.3991 - accuracy: 0.8332 - val_loss: 0.5198 - val_accuracy: 0.7672\n",
            "Epoch 97/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4034 - accuracy: 0.8288 - val_loss: 0.5069 - val_accuracy: 0.7728\n",
            "Epoch 98/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4033 - accuracy: 0.8306 - val_loss: 0.5012 - val_accuracy: 0.7733\n",
            "Epoch 99/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.3976 - accuracy: 0.8361 - val_loss: 0.5137 - val_accuracy: 0.7783\n",
            "Epoch 100/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.3996 - accuracy: 0.8336 - val_loss: 0.5211 - val_accuracy: 0.7761\n",
            "Epoch 101/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4005 - accuracy: 0.8316 - val_loss: 0.5021 - val_accuracy: 0.7833\n",
            "Epoch 102/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.3978 - accuracy: 0.8339 - val_loss: 0.5111 - val_accuracy: 0.7733\n",
            "Epoch 103/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.3976 - accuracy: 0.8346 - val_loss: 0.5232 - val_accuracy: 0.7700\n",
            "Epoch 104/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4007 - accuracy: 0.8286 - val_loss: 0.5179 - val_accuracy: 0.7689\n",
            "Epoch 105/200\n",
            "33/33 [==============================] - 0s 9ms/step - loss: 0.3994 - accuracy: 0.8347 - val_loss: 0.5196 - val_accuracy: 0.7772\n",
            "Epoch 106/200\n",
            "33/33 [==============================] - 0s 12ms/step - loss: 0.3944 - accuracy: 0.8391 - val_loss: 0.5202 - val_accuracy: 0.7789\n",
            "Epoch 107/200\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 0.3922 - accuracy: 0.8380 - val_loss: 0.5338 - val_accuracy: 0.7683\n",
            "Epoch 108/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.3921 - accuracy: 0.8375 - val_loss: 0.5258 - val_accuracy: 0.7728\n",
            "Epoch 109/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.3905 - accuracy: 0.8401 - val_loss: 0.5339 - val_accuracy: 0.7722\n",
            "Epoch 110/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.3947 - accuracy: 0.8370 - val_loss: 0.5296 - val_accuracy: 0.7672\n",
            "Epoch 111/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.3947 - accuracy: 0.8373 - val_loss: 0.5312 - val_accuracy: 0.7717\n",
            "Epoch 112/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.3913 - accuracy: 0.8391 - val_loss: 0.5389 - val_accuracy: 0.7728\n",
            "Epoch 113/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.3905 - accuracy: 0.8390 - val_loss: 0.5267 - val_accuracy: 0.7739\n",
            "Epoch 114/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.3867 - accuracy: 0.8409 - val_loss: 0.5466 - val_accuracy: 0.7700\n",
            "Epoch 115/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.3886 - accuracy: 0.8391 - val_loss: 0.5520 - val_accuracy: 0.7739\n",
            "Epoch 116/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.3932 - accuracy: 0.8391 - val_loss: 0.5260 - val_accuracy: 0.7778\n",
            "Epoch 117/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.3927 - accuracy: 0.8393 - val_loss: 0.5368 - val_accuracy: 0.7756\n",
            "Epoch 118/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.3891 - accuracy: 0.8408 - val_loss: 0.5449 - val_accuracy: 0.7606\n",
            "Epoch 119/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.3900 - accuracy: 0.8415 - val_loss: 0.5300 - val_accuracy: 0.7739\n",
            "Epoch 120/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.3867 - accuracy: 0.8435 - val_loss: 0.5379 - val_accuracy: 0.7667\n",
            "Epoch 121/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.3894 - accuracy: 0.8430 - val_loss: 0.5258 - val_accuracy: 0.7667\n",
            "Epoch 122/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.3855 - accuracy: 0.8443 - val_loss: 0.5490 - val_accuracy: 0.7800\n",
            "Epoch 123/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.3872 - accuracy: 0.8438 - val_loss: 0.5510 - val_accuracy: 0.7639\n",
            "Epoch 124/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.3878 - accuracy: 0.8424 - val_loss: 0.5420 - val_accuracy: 0.7794\n",
            "Epoch 125/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.3839 - accuracy: 0.8435 - val_loss: 0.5548 - val_accuracy: 0.7739\n",
            "Epoch 126/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.3820 - accuracy: 0.8452 - val_loss: 0.5429 - val_accuracy: 0.7767\n",
            "Epoch 127/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.3859 - accuracy: 0.8456 - val_loss: 0.5427 - val_accuracy: 0.7783\n",
            "Epoch 128/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.3797 - accuracy: 0.8496 - val_loss: 0.5625 - val_accuracy: 0.7639\n",
            "Epoch 129/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.3789 - accuracy: 0.8496 - val_loss: 0.5528 - val_accuracy: 0.7700\n",
            "Epoch 130/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.3801 - accuracy: 0.8475 - val_loss: 0.5563 - val_accuracy: 0.7689\n",
            "Epoch 131/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.3782 - accuracy: 0.8494 - val_loss: 0.5633 - val_accuracy: 0.7689\n",
            "Epoch 132/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.3768 - accuracy: 0.8522 - val_loss: 0.5534 - val_accuracy: 0.7617\n",
            "Epoch 133/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.3734 - accuracy: 0.8522 - val_loss: 0.5588 - val_accuracy: 0.7633\n",
            "Epoch 134/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.3704 - accuracy: 0.8542 - val_loss: 0.5710 - val_accuracy: 0.7667\n",
            "Epoch 135/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.3739 - accuracy: 0.8514 - val_loss: 0.5677 - val_accuracy: 0.7739\n",
            "Epoch 136/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.3750 - accuracy: 0.8504 - val_loss: 0.5745 - val_accuracy: 0.7606\n",
            "Epoch 137/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.3805 - accuracy: 0.8454 - val_loss: 0.5701 - val_accuracy: 0.7739\n",
            "Epoch 138/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.3764 - accuracy: 0.8497 - val_loss: 0.5661 - val_accuracy: 0.7600\n",
            "Epoch 139/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.3767 - accuracy: 0.8525 - val_loss: 0.5729 - val_accuracy: 0.7583\n",
            "Epoch 140/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.3764 - accuracy: 0.8502 - val_loss: 0.5729 - val_accuracy: 0.7661\n",
            "Epoch 141/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.3690 - accuracy: 0.8548 - val_loss: 0.5743 - val_accuracy: 0.7689\n",
            "Epoch 142/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.3721 - accuracy: 0.8529 - val_loss: 0.5738 - val_accuracy: 0.7689\n",
            "Epoch 143/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.3734 - accuracy: 0.8543 - val_loss: 0.5791 - val_accuracy: 0.7700\n",
            "Epoch 144/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.3700 - accuracy: 0.8560 - val_loss: 0.5707 - val_accuracy: 0.7617\n",
            "Epoch 145/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.3806 - accuracy: 0.8487 - val_loss: 0.5678 - val_accuracy: 0.7539\n",
            "Epoch 146/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.3664 - accuracy: 0.8572 - val_loss: 0.5756 - val_accuracy: 0.7706\n",
            "Epoch 147/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.3643 - accuracy: 0.8606 - val_loss: 0.6078 - val_accuracy: 0.7456\n",
            "Epoch 148/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.3656 - accuracy: 0.8593 - val_loss: 0.5967 - val_accuracy: 0.7639\n",
            "Epoch 149/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.3655 - accuracy: 0.8573 - val_loss: 0.5870 - val_accuracy: 0.7683\n",
            "Epoch 150/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.3696 - accuracy: 0.8560 - val_loss: 0.6086 - val_accuracy: 0.7728\n",
            "Epoch 151/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.3831 - accuracy: 0.8472 - val_loss: 0.5625 - val_accuracy: 0.7728\n",
            "Epoch 152/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.3686 - accuracy: 0.8562 - val_loss: 0.5820 - val_accuracy: 0.7700\n",
            "Epoch 153/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.3579 - accuracy: 0.8612 - val_loss: 0.5947 - val_accuracy: 0.7700\n",
            "Epoch 154/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.3694 - accuracy: 0.8538 - val_loss: 0.5889 - val_accuracy: 0.7728\n",
            "Epoch 155/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.3697 - accuracy: 0.8567 - val_loss: 0.5775 - val_accuracy: 0.7722\n",
            "Epoch 156/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.3618 - accuracy: 0.8607 - val_loss: 0.5897 - val_accuracy: 0.7611\n",
            "Epoch 157/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.3548 - accuracy: 0.8649 - val_loss: 0.6149 - val_accuracy: 0.7650\n",
            "Epoch 158/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.3564 - accuracy: 0.8643 - val_loss: 0.5961 - val_accuracy: 0.7794\n",
            "Epoch 159/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.3542 - accuracy: 0.8673 - val_loss: 0.6165 - val_accuracy: 0.7544\n",
            "Epoch 160/200\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 0.3593 - accuracy: 0.8626 - val_loss: 0.6126 - val_accuracy: 0.7622\n",
            "Epoch 161/200\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.3549 - accuracy: 0.8652 - val_loss: 0.6115 - val_accuracy: 0.7583\n",
            "Epoch 162/200\n",
            "33/33 [==============================] - 0s 9ms/step - loss: 0.3564 - accuracy: 0.8675 - val_loss: 0.6062 - val_accuracy: 0.7583\n",
            "Epoch 163/200\n",
            "33/33 [==============================] - 0s 9ms/step - loss: 0.3463 - accuracy: 0.8709 - val_loss: 0.6292 - val_accuracy: 0.7539\n",
            "Epoch 164/200\n",
            "33/33 [==============================] - 0s 9ms/step - loss: 0.3496 - accuracy: 0.8683 - val_loss: 0.6315 - val_accuracy: 0.7572\n",
            "Epoch 165/200\n",
            "33/33 [==============================] - 0s 10ms/step - loss: 0.3490 - accuracy: 0.8691 - val_loss: 0.6074 - val_accuracy: 0.7589\n",
            "Epoch 166/200\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 0.3551 - accuracy: 0.8659 - val_loss: 0.6265 - val_accuracy: 0.7706\n",
            "Epoch 167/200\n",
            "33/33 [==============================] - 0s 10ms/step - loss: 0.3538 - accuracy: 0.8668 - val_loss: 0.6431 - val_accuracy: 0.7411\n",
            "Epoch 168/200\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 0.3581 - accuracy: 0.8645 - val_loss: 0.6173 - val_accuracy: 0.7517\n",
            "Epoch 169/200\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 0.3498 - accuracy: 0.8699 - val_loss: 0.6219 - val_accuracy: 0.7350\n",
            "Epoch 170/200\n",
            "33/33 [==============================] - 0s 10ms/step - loss: 0.3442 - accuracy: 0.8746 - val_loss: 0.6469 - val_accuracy: 0.7511\n",
            "Epoch 171/200\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 0.3447 - accuracy: 0.8746 - val_loss: 0.6451 - val_accuracy: 0.7500\n",
            "Epoch 172/200\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 0.3402 - accuracy: 0.8766 - val_loss: 0.6426 - val_accuracy: 0.7544\n",
            "Epoch 173/200\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 0.3332 - accuracy: 0.8808 - val_loss: 0.6689 - val_accuracy: 0.7422\n",
            "Epoch 174/200\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 0.3423 - accuracy: 0.8748 - val_loss: 0.6415 - val_accuracy: 0.7506\n",
            "Epoch 175/200\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.3406 - accuracy: 0.8780 - val_loss: 0.6569 - val_accuracy: 0.7606\n",
            "Epoch 176/200\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.3412 - accuracy: 0.8783 - val_loss: 0.6532 - val_accuracy: 0.7444\n",
            "Epoch 177/200\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.3418 - accuracy: 0.8757 - val_loss: 0.6575 - val_accuracy: 0.7506\n",
            "Epoch 178/200\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.3315 - accuracy: 0.8834 - val_loss: 0.6595 - val_accuracy: 0.7567\n",
            "Epoch 179/200\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.3296 - accuracy: 0.8825 - val_loss: 0.7343 - val_accuracy: 0.7539\n",
            "Epoch 180/200\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.3353 - accuracy: 0.8794 - val_loss: 0.6711 - val_accuracy: 0.7544\n",
            "Epoch 181/200\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.3266 - accuracy: 0.8845 - val_loss: 0.6966 - val_accuracy: 0.7483\n",
            "Epoch 182/200\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 0.3234 - accuracy: 0.8873 - val_loss: 0.6906 - val_accuracy: 0.7556\n",
            "Epoch 183/200\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 0.3321 - accuracy: 0.8814 - val_loss: 0.6870 - val_accuracy: 0.7556\n",
            "Epoch 184/200\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.3299 - accuracy: 0.8827 - val_loss: 0.6990 - val_accuracy: 0.7500\n",
            "Epoch 185/200\n",
            "33/33 [==============================] - 0s 10ms/step - loss: 0.3325 - accuracy: 0.8836 - val_loss: 0.6648 - val_accuracy: 0.7522\n",
            "Epoch 186/200\n",
            "33/33 [==============================] - 0s 14ms/step - loss: 0.3267 - accuracy: 0.8852 - val_loss: 0.7113 - val_accuracy: 0.7267\n",
            "Epoch 187/200\n",
            "33/33 [==============================] - 0s 12ms/step - loss: 0.3358 - accuracy: 0.8806 - val_loss: 0.6671 - val_accuracy: 0.7511\n",
            "Epoch 188/200\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 0.3207 - accuracy: 0.8870 - val_loss: 0.7156 - val_accuracy: 0.7461\n",
            "Epoch 189/200\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 0.3280 - accuracy: 0.8846 - val_loss: 0.6941 - val_accuracy: 0.7444\n",
            "Epoch 190/200\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 0.3230 - accuracy: 0.8869 - val_loss: 0.7191 - val_accuracy: 0.7372\n",
            "Epoch 191/200\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.3262 - accuracy: 0.8838 - val_loss: 0.7161 - val_accuracy: 0.7461\n",
            "Epoch 192/200\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.3190 - accuracy: 0.8892 - val_loss: 0.7012 - val_accuracy: 0.7289\n",
            "Epoch 193/200\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.3185 - accuracy: 0.8899 - val_loss: 0.7193 - val_accuracy: 0.7417\n",
            "Epoch 194/200\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.3212 - accuracy: 0.8901 - val_loss: 0.7122 - val_accuracy: 0.7439\n",
            "Epoch 195/200\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.3157 - accuracy: 0.8957 - val_loss: 0.7428 - val_accuracy: 0.7411\n",
            "Epoch 196/200\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 0.3252 - accuracy: 0.8879 - val_loss: 0.7092 - val_accuracy: 0.7500\n",
            "Epoch 197/200\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 0.3203 - accuracy: 0.8887 - val_loss: 0.7213 - val_accuracy: 0.7578\n",
            "Epoch 198/200\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 0.3148 - accuracy: 0.8923 - val_loss: 0.7259 - val_accuracy: 0.7594\n",
            "Epoch 199/200\n",
            "33/33 [==============================] - 0s 9ms/step - loss: 0.3052 - accuracy: 0.8977 - val_loss: 0.7470 - val_accuracy: 0.7411\n",
            "Epoch 200/200\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 0.3064 - accuracy: 0.8966 - val_loss: 0.7557 - val_accuracy: 0.7472\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JwyY0U8bS9BF"
      },
      "source": [
        "## Dropout"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7fh1gYwHbizF"
      },
      "source": [
        "The intuitive explanation for dropout is that we need to force each node to output features that are useful on their own.\n",
        "\n",
        "Dropout involves randomly \"dropping out\" (i.e. setting to zero) a number of output features of a layer during training. Let's say a given layer would normally have returned a vector [0.2, 0.5, 1.3, 0.8, 1.1] for a given input sample during training; after applying dropout, this vector will have a few zero entries distributed at random, e.g. [0, 0.5, 1.3, 0, 1.1].\n",
        "\n",
        "The \"dropout rate\" is a hyperparameter specifying the fraction of features that are being zeroed-out; it is usually set between 0.2 and 0.5.\n",
        "\n",
        "<center><img src=\"http://cs231n.github.io/assets/nn2/dropout.jpeg\"\n",
        " width=\"400px\"> </center>\n",
        "\n",
        "Note that at test time, no units are dropped out, and instead the layer's output values are scaled down by a factor equal to the dropout rate, so as to balance for the fact that more units are active than at training time."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f_rEjs8FbXHt",
        "outputId": "7c400ce4-8922-408b-8a37-c66cace2954a"
      },
      "source": [
        "from keras.layers import Dropout\n",
        "\n",
        "dropout_rate = 0.5\n",
        "dropout_model = Sequential([\n",
        "    Dense(512, activation='relu', input_shape=(shape,)),\n",
        "    Dropout(dropout_rate),\n",
        "    Dense(512, activation='relu'),\n",
        "    Dropout(dropout_rate),\n",
        "    Dense(512, activation='relu'),\n",
        "    Dropout(dropout_rate),\n",
        "    Dense(512, activation='relu'),\n",
        "    Dropout(dropout_rate),\n",
        "    Dense(1, activation = \"sigmoid\")\n",
        "])\n",
        "\n",
        "regularizer_histories['dropout'] = compile_and_fit(dropout_model, max_epochs = EPOCHS)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_31\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_142 (Dense)           (None, 512)               9728      \n",
            "                                                                 \n",
            " dropout_28 (Dropout)        (None, 512)               0         \n",
            "                                                                 \n",
            " dense_143 (Dense)           (None, 512)               262656    \n",
            "                                                                 \n",
            " dropout_29 (Dropout)        (None, 512)               0         \n",
            "                                                                 \n",
            " dense_144 (Dense)           (None, 512)               262656    \n",
            "                                                                 \n",
            " dropout_30 (Dropout)        (None, 512)               0         \n",
            "                                                                 \n",
            " dense_145 (Dense)           (None, 512)               262656    \n",
            "                                                                 \n",
            " dropout_31 (Dropout)        (None, 512)               0         \n",
            "                                                                 \n",
            " dense_146 (Dense)           (None, 1)                 513       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 798,209\n",
            "Trainable params: 798,209\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/200\n",
            "33/33 [==============================] - 1s 14ms/step - loss: 0.5454 - accuracy: 0.7310 - val_loss: 0.4650 - val_accuracy: 0.7689\n",
            "Epoch 2/200\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.4743 - accuracy: 0.7822 - val_loss: 0.4665 - val_accuracy: 0.7728\n",
            "Epoch 3/200\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.4605 - accuracy: 0.7882 - val_loss: 0.4583 - val_accuracy: 0.7806\n",
            "Epoch 4/200\n",
            "33/33 [==============================] - 0s 13ms/step - loss: 0.4581 - accuracy: 0.7871 - val_loss: 0.4563 - val_accuracy: 0.7811\n",
            "Epoch 5/200\n",
            "33/33 [==============================] - 0s 12ms/step - loss: 0.4553 - accuracy: 0.7908 - val_loss: 0.4565 - val_accuracy: 0.7850\n",
            "Epoch 6/200\n",
            "33/33 [==============================] - 0s 10ms/step - loss: 0.4502 - accuracy: 0.7938 - val_loss: 0.4579 - val_accuracy: 0.7811\n",
            "Epoch 7/200\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.4502 - accuracy: 0.7952 - val_loss: 0.4597 - val_accuracy: 0.7811\n",
            "Epoch 8/200\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.4501 - accuracy: 0.7931 - val_loss: 0.4524 - val_accuracy: 0.7844\n",
            "Epoch 9/200\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 0.4469 - accuracy: 0.7957 - val_loss: 0.4510 - val_accuracy: 0.7839\n",
            "Epoch 10/200\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.4453 - accuracy: 0.7944 - val_loss: 0.4524 - val_accuracy: 0.7856\n",
            "Epoch 11/200\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.4478 - accuracy: 0.7962 - val_loss: 0.4526 - val_accuracy: 0.7867\n",
            "Epoch 12/200\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.4418 - accuracy: 0.7970 - val_loss: 0.4516 - val_accuracy: 0.7861\n",
            "Epoch 13/200\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.4441 - accuracy: 0.7966 - val_loss: 0.4583 - val_accuracy: 0.7911\n",
            "Epoch 14/200\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 0.4413 - accuracy: 0.7984 - val_loss: 0.4534 - val_accuracy: 0.7894\n",
            "Epoch 15/200\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.4401 - accuracy: 0.7996 - val_loss: 0.4598 - val_accuracy: 0.7828\n",
            "Epoch 16/200\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.4406 - accuracy: 0.7988 - val_loss: 0.4580 - val_accuracy: 0.7917\n",
            "Epoch 17/200\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.4413 - accuracy: 0.7998 - val_loss: 0.4519 - val_accuracy: 0.7906\n",
            "Epoch 18/200\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.4378 - accuracy: 0.8004 - val_loss: 0.4532 - val_accuracy: 0.7922\n",
            "Epoch 19/200\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.4358 - accuracy: 0.8013 - val_loss: 0.4569 - val_accuracy: 0.7917\n",
            "Epoch 20/200\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.4375 - accuracy: 0.7997 - val_loss: 0.4549 - val_accuracy: 0.7889\n",
            "Epoch 21/200\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.4369 - accuracy: 0.8019 - val_loss: 0.4515 - val_accuracy: 0.7922\n",
            "Epoch 22/200\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.4344 - accuracy: 0.8027 - val_loss: 0.4552 - val_accuracy: 0.7878\n",
            "Epoch 23/200\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.4363 - accuracy: 0.8029 - val_loss: 0.4587 - val_accuracy: 0.7867\n",
            "Epoch 24/200\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.4354 - accuracy: 0.8010 - val_loss: 0.4534 - val_accuracy: 0.7911\n",
            "Epoch 25/200\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.4356 - accuracy: 0.8007 - val_loss: 0.4565 - val_accuracy: 0.7906\n",
            "Epoch 26/200\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.4336 - accuracy: 0.8028 - val_loss: 0.4531 - val_accuracy: 0.7939\n",
            "Epoch 27/200\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.4324 - accuracy: 0.8027 - val_loss: 0.4503 - val_accuracy: 0.7911\n",
            "Epoch 28/200\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.4294 - accuracy: 0.8030 - val_loss: 0.4523 - val_accuracy: 0.7900\n",
            "Epoch 29/200\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.4312 - accuracy: 0.8030 - val_loss: 0.4527 - val_accuracy: 0.7878\n",
            "Epoch 30/200\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.4324 - accuracy: 0.8035 - val_loss: 0.4557 - val_accuracy: 0.7883\n",
            "Epoch 31/200\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.4326 - accuracy: 0.8029 - val_loss: 0.4509 - val_accuracy: 0.7883\n",
            "Epoch 32/200\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.4305 - accuracy: 0.8023 - val_loss: 0.4532 - val_accuracy: 0.7944\n",
            "Epoch 33/200\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.4302 - accuracy: 0.8030 - val_loss: 0.4557 - val_accuracy: 0.7950\n",
            "Epoch 34/200\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.4317 - accuracy: 0.8040 - val_loss: 0.4532 - val_accuracy: 0.7922\n",
            "Epoch 35/200\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.4286 - accuracy: 0.8058 - val_loss: 0.4549 - val_accuracy: 0.7906\n",
            "Epoch 36/200\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.4278 - accuracy: 0.8077 - val_loss: 0.4517 - val_accuracy: 0.7939\n",
            "Epoch 37/200\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.4301 - accuracy: 0.8038 - val_loss: 0.4511 - val_accuracy: 0.7961\n",
            "Epoch 38/200\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.4279 - accuracy: 0.8069 - val_loss: 0.4512 - val_accuracy: 0.7867\n",
            "Epoch 39/200\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.4293 - accuracy: 0.8054 - val_loss: 0.4535 - val_accuracy: 0.7889\n",
            "Epoch 40/200\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.4279 - accuracy: 0.8072 - val_loss: 0.4540 - val_accuracy: 0.7928\n",
            "Epoch 41/200\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.4261 - accuracy: 0.8067 - val_loss: 0.4537 - val_accuracy: 0.7878\n",
            "Epoch 42/200\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.4277 - accuracy: 0.8044 - val_loss: 0.4541 - val_accuracy: 0.7906\n",
            "Epoch 43/200\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.4233 - accuracy: 0.8075 - val_loss: 0.4510 - val_accuracy: 0.7972\n",
            "Epoch 44/200\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 0.4230 - accuracy: 0.8091 - val_loss: 0.4576 - val_accuracy: 0.7894\n",
            "Epoch 45/200\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 0.4251 - accuracy: 0.8069 - val_loss: 0.4602 - val_accuracy: 0.7883\n",
            "Epoch 46/200\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 0.4247 - accuracy: 0.8083 - val_loss: 0.4528 - val_accuracy: 0.7900\n",
            "Epoch 47/200\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.4243 - accuracy: 0.8077 - val_loss: 0.4526 - val_accuracy: 0.7900\n",
            "Epoch 48/200\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.4215 - accuracy: 0.8081 - val_loss: 0.4550 - val_accuracy: 0.7950\n",
            "Epoch 49/200\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.4213 - accuracy: 0.8092 - val_loss: 0.4539 - val_accuracy: 0.7900\n",
            "Epoch 50/200\n",
            "33/33 [==============================] - 0s 9ms/step - loss: 0.4199 - accuracy: 0.8102 - val_loss: 0.4526 - val_accuracy: 0.7928\n",
            "Epoch 51/200\n",
            "33/33 [==============================] - 0s 9ms/step - loss: 0.4215 - accuracy: 0.8098 - val_loss: 0.4528 - val_accuracy: 0.7917\n",
            "Epoch 52/200\n",
            "33/33 [==============================] - 0s 9ms/step - loss: 0.4239 - accuracy: 0.8072 - val_loss: 0.4568 - val_accuracy: 0.7878\n",
            "Epoch 53/200\n",
            "33/33 [==============================] - 0s 11ms/step - loss: 0.4195 - accuracy: 0.8091 - val_loss: 0.4583 - val_accuracy: 0.7878\n",
            "Epoch 54/200\n",
            "33/33 [==============================] - 0s 10ms/step - loss: 0.4223 - accuracy: 0.8093 - val_loss: 0.4542 - val_accuracy: 0.7889\n",
            "Epoch 55/200\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.4187 - accuracy: 0.8088 - val_loss: 0.4528 - val_accuracy: 0.7956\n",
            "Epoch 56/200\n",
            "33/33 [==============================] - 0s 12ms/step - loss: 0.4177 - accuracy: 0.8118 - val_loss: 0.4541 - val_accuracy: 0.7911\n",
            "Epoch 57/200\n",
            "33/33 [==============================] - 0s 9ms/step - loss: 0.4184 - accuracy: 0.8086 - val_loss: 0.4572 - val_accuracy: 0.7922\n",
            "Epoch 58/200\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.4192 - accuracy: 0.8081 - val_loss: 0.4546 - val_accuracy: 0.7889\n",
            "Epoch 59/200\n",
            "33/33 [==============================] - 0s 9ms/step - loss: 0.4185 - accuracy: 0.8121 - val_loss: 0.4560 - val_accuracy: 0.7872\n",
            "Epoch 60/200\n",
            "33/33 [==============================] - 0s 9ms/step - loss: 0.4172 - accuracy: 0.8120 - val_loss: 0.4617 - val_accuracy: 0.7867\n",
            "Epoch 61/200\n",
            "33/33 [==============================] - 0s 10ms/step - loss: 0.4143 - accuracy: 0.8127 - val_loss: 0.4549 - val_accuracy: 0.7900\n",
            "Epoch 62/200\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.4168 - accuracy: 0.8127 - val_loss: 0.4568 - val_accuracy: 0.7911\n",
            "Epoch 63/200\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.4143 - accuracy: 0.8124 - val_loss: 0.4535 - val_accuracy: 0.7911\n",
            "Epoch 64/200\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 0.4145 - accuracy: 0.8109 - val_loss: 0.4573 - val_accuracy: 0.7839\n",
            "Epoch 65/200\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.4130 - accuracy: 0.8130 - val_loss: 0.4608 - val_accuracy: 0.7833\n",
            "Epoch 66/200\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.4124 - accuracy: 0.8122 - val_loss: 0.4602 - val_accuracy: 0.7889\n",
            "Epoch 67/200\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 0.4124 - accuracy: 0.8120 - val_loss: 0.4588 - val_accuracy: 0.7872\n",
            "Epoch 68/200\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 0.4138 - accuracy: 0.8144 - val_loss: 0.4621 - val_accuracy: 0.7833\n",
            "Epoch 69/200\n",
            "33/33 [==============================] - 0s 9ms/step - loss: 0.4113 - accuracy: 0.8147 - val_loss: 0.4601 - val_accuracy: 0.7872\n",
            "Epoch 70/200\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.4103 - accuracy: 0.8157 - val_loss: 0.4569 - val_accuracy: 0.7800\n",
            "Epoch 71/200\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.4102 - accuracy: 0.8152 - val_loss: 0.4622 - val_accuracy: 0.7917\n",
            "Epoch 72/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4092 - accuracy: 0.8114 - val_loss: 0.4615 - val_accuracy: 0.7872\n",
            "Epoch 73/200\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.4097 - accuracy: 0.8127 - val_loss: 0.4605 - val_accuracy: 0.7850\n",
            "Epoch 74/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4092 - accuracy: 0.8156 - val_loss: 0.4592 - val_accuracy: 0.7850\n",
            "Epoch 75/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4086 - accuracy: 0.8148 - val_loss: 0.4606 - val_accuracy: 0.7883\n",
            "Epoch 76/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4078 - accuracy: 0.8162 - val_loss: 0.4610 - val_accuracy: 0.7878\n",
            "Epoch 77/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4104 - accuracy: 0.8160 - val_loss: 0.4615 - val_accuracy: 0.7894\n",
            "Epoch 78/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4071 - accuracy: 0.8183 - val_loss: 0.4612 - val_accuracy: 0.7894\n",
            "Epoch 79/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4100 - accuracy: 0.8157 - val_loss: 0.4596 - val_accuracy: 0.7906\n",
            "Epoch 80/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4065 - accuracy: 0.8169 - val_loss: 0.4652 - val_accuracy: 0.7833\n",
            "Epoch 81/200\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.4037 - accuracy: 0.8164 - val_loss: 0.4645 - val_accuracy: 0.7883\n",
            "Epoch 82/200\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.4068 - accuracy: 0.8185 - val_loss: 0.4693 - val_accuracy: 0.7778\n",
            "Epoch 83/200\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.4068 - accuracy: 0.8187 - val_loss: 0.4660 - val_accuracy: 0.7817\n",
            "Epoch 84/200\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.4035 - accuracy: 0.8180 - val_loss: 0.4676 - val_accuracy: 0.7850\n",
            "Epoch 85/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4016 - accuracy: 0.8184 - val_loss: 0.4670 - val_accuracy: 0.7833\n",
            "Epoch 86/200\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 0.4032 - accuracy: 0.8171 - val_loss: 0.4644 - val_accuracy: 0.7839\n",
            "Epoch 87/200\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.4028 - accuracy: 0.8172 - val_loss: 0.4680 - val_accuracy: 0.7839\n",
            "Epoch 88/200\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.4057 - accuracy: 0.8173 - val_loss: 0.4658 - val_accuracy: 0.7872\n",
            "Epoch 89/200\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.4011 - accuracy: 0.8230 - val_loss: 0.4691 - val_accuracy: 0.7839\n",
            "Epoch 90/200\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.3997 - accuracy: 0.8222 - val_loss: 0.4729 - val_accuracy: 0.7867\n",
            "Epoch 91/200\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.4016 - accuracy: 0.8201 - val_loss: 0.4702 - val_accuracy: 0.7817\n",
            "Epoch 92/200\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.4010 - accuracy: 0.8165 - val_loss: 0.4717 - val_accuracy: 0.7856\n",
            "Epoch 93/200\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.3994 - accuracy: 0.8190 - val_loss: 0.4697 - val_accuracy: 0.7872\n",
            "Epoch 94/200\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.3972 - accuracy: 0.8210 - val_loss: 0.4735 - val_accuracy: 0.7839\n",
            "Epoch 95/200\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.3955 - accuracy: 0.8234 - val_loss: 0.4712 - val_accuracy: 0.7839\n",
            "Epoch 96/200\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.3990 - accuracy: 0.8219 - val_loss: 0.4718 - val_accuracy: 0.7878\n",
            "Epoch 97/200\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.3949 - accuracy: 0.8219 - val_loss: 0.4699 - val_accuracy: 0.7833\n",
            "Epoch 98/200\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.3965 - accuracy: 0.8221 - val_loss: 0.4706 - val_accuracy: 0.7822\n",
            "Epoch 99/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.3972 - accuracy: 0.8215 - val_loss: 0.4718 - val_accuracy: 0.7867\n",
            "Epoch 100/200\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.3965 - accuracy: 0.8215 - val_loss: 0.4778 - val_accuracy: 0.7817\n",
            "Epoch 101/200\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.3975 - accuracy: 0.8220 - val_loss: 0.4781 - val_accuracy: 0.7856\n",
            "Epoch 102/200\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 0.3915 - accuracy: 0.8236 - val_loss: 0.4766 - val_accuracy: 0.7844\n",
            "Epoch 103/200\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.3918 - accuracy: 0.8262 - val_loss: 0.4715 - val_accuracy: 0.7872\n",
            "Epoch 104/200\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.3925 - accuracy: 0.8238 - val_loss: 0.4758 - val_accuracy: 0.7822\n",
            "Epoch 105/200\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 0.3950 - accuracy: 0.8210 - val_loss: 0.4732 - val_accuracy: 0.7839\n",
            "Epoch 106/200\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.3950 - accuracy: 0.8252 - val_loss: 0.4731 - val_accuracy: 0.7833\n",
            "Epoch 107/200\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 0.3930 - accuracy: 0.8248 - val_loss: 0.4736 - val_accuracy: 0.7828\n",
            "Epoch 108/200\n",
            "33/33 [==============================] - 0s 9ms/step - loss: 0.3904 - accuracy: 0.8272 - val_loss: 0.4802 - val_accuracy: 0.7800\n",
            "Epoch 109/200\n",
            "33/33 [==============================] - 0s 9ms/step - loss: 0.3868 - accuracy: 0.8276 - val_loss: 0.4855 - val_accuracy: 0.7828\n",
            "Epoch 110/200\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 0.3942 - accuracy: 0.8244 - val_loss: 0.4786 - val_accuracy: 0.7778\n",
            "Epoch 111/200\n",
            "33/33 [==============================] - 0s 10ms/step - loss: 0.3896 - accuracy: 0.8278 - val_loss: 0.4761 - val_accuracy: 0.7767\n",
            "Epoch 112/200\n",
            "33/33 [==============================] - 0s 9ms/step - loss: 0.3903 - accuracy: 0.8249 - val_loss: 0.4795 - val_accuracy: 0.7828\n",
            "Epoch 113/200\n",
            "33/33 [==============================] - 0s 9ms/step - loss: 0.3871 - accuracy: 0.8249 - val_loss: 0.4777 - val_accuracy: 0.7817\n",
            "Epoch 114/200\n",
            "33/33 [==============================] - 0s 10ms/step - loss: 0.3867 - accuracy: 0.8268 - val_loss: 0.4857 - val_accuracy: 0.7778\n",
            "Epoch 115/200\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 0.3882 - accuracy: 0.8263 - val_loss: 0.4799 - val_accuracy: 0.7761\n",
            "Epoch 116/200\n",
            "33/33 [==============================] - 0s 9ms/step - loss: 0.3880 - accuracy: 0.8228 - val_loss: 0.4825 - val_accuracy: 0.7772\n",
            "Epoch 117/200\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.3869 - accuracy: 0.8259 - val_loss: 0.4839 - val_accuracy: 0.7711\n",
            "Epoch 118/200\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 0.3856 - accuracy: 0.8278 - val_loss: 0.4830 - val_accuracy: 0.7806\n",
            "Epoch 119/200\n",
            "33/33 [==============================] - 0s 11ms/step - loss: 0.3848 - accuracy: 0.8281 - val_loss: 0.4809 - val_accuracy: 0.7744\n",
            "Epoch 120/200\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 0.3812 - accuracy: 0.8308 - val_loss: 0.4894 - val_accuracy: 0.7750\n",
            "Epoch 121/200\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.3842 - accuracy: 0.8281 - val_loss: 0.4839 - val_accuracy: 0.7817\n",
            "Epoch 122/200\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.3833 - accuracy: 0.8266 - val_loss: 0.4833 - val_accuracy: 0.7761\n",
            "Epoch 123/200\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.3822 - accuracy: 0.8286 - val_loss: 0.4879 - val_accuracy: 0.7789\n",
            "Epoch 124/200\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.3857 - accuracy: 0.8267 - val_loss: 0.4900 - val_accuracy: 0.7778\n",
            "Epoch 125/200\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.3833 - accuracy: 0.8285 - val_loss: 0.4908 - val_accuracy: 0.7783\n",
            "Epoch 126/200\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.3814 - accuracy: 0.8271 - val_loss: 0.4850 - val_accuracy: 0.7822\n",
            "Epoch 127/200\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.3791 - accuracy: 0.8302 - val_loss: 0.4892 - val_accuracy: 0.7778\n",
            "Epoch 128/200\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.3773 - accuracy: 0.8314 - val_loss: 0.4891 - val_accuracy: 0.7778\n",
            "Epoch 129/200\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 0.3772 - accuracy: 0.8323 - val_loss: 0.4866 - val_accuracy: 0.7794\n",
            "Epoch 130/200\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.3729 - accuracy: 0.8320 - val_loss: 0.4868 - val_accuracy: 0.7728\n",
            "Epoch 131/200\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.3802 - accuracy: 0.8299 - val_loss: 0.4895 - val_accuracy: 0.7778\n",
            "Epoch 132/200\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.3782 - accuracy: 0.8303 - val_loss: 0.4945 - val_accuracy: 0.7756\n",
            "Epoch 133/200\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.3770 - accuracy: 0.8335 - val_loss: 0.4919 - val_accuracy: 0.7778\n",
            "Epoch 134/200\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 0.3745 - accuracy: 0.8340 - val_loss: 0.4852 - val_accuracy: 0.7794\n",
            "Epoch 135/200\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 0.3727 - accuracy: 0.8328 - val_loss: 0.4946 - val_accuracy: 0.7767\n",
            "Epoch 136/200\n",
            "33/33 [==============================] - 0s 9ms/step - loss: 0.3813 - accuracy: 0.8281 - val_loss: 0.4887 - val_accuracy: 0.7661\n",
            "Epoch 137/200\n",
            "33/33 [==============================] - 0s 9ms/step - loss: 0.3764 - accuracy: 0.8288 - val_loss: 0.4911 - val_accuracy: 0.7733\n",
            "Epoch 138/200\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.3723 - accuracy: 0.8332 - val_loss: 0.4952 - val_accuracy: 0.7750\n",
            "Epoch 139/200\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 0.3715 - accuracy: 0.8344 - val_loss: 0.4926 - val_accuracy: 0.7700\n",
            "Epoch 140/200\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 0.3757 - accuracy: 0.8328 - val_loss: 0.4856 - val_accuracy: 0.7761\n",
            "Epoch 141/200\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.3737 - accuracy: 0.8356 - val_loss: 0.4897 - val_accuracy: 0.7789\n",
            "Epoch 142/200\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.3737 - accuracy: 0.8343 - val_loss: 0.4939 - val_accuracy: 0.7761\n",
            "Epoch 143/200\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.3723 - accuracy: 0.8323 - val_loss: 0.4935 - val_accuracy: 0.7722\n",
            "Epoch 144/200\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 0.3679 - accuracy: 0.8338 - val_loss: 0.4950 - val_accuracy: 0.7811\n",
            "Epoch 145/200\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.3677 - accuracy: 0.8344 - val_loss: 0.4941 - val_accuracy: 0.7817\n",
            "Epoch 146/200\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.3710 - accuracy: 0.8335 - val_loss: 0.5008 - val_accuracy: 0.7789\n",
            "Epoch 147/200\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.3685 - accuracy: 0.8388 - val_loss: 0.5028 - val_accuracy: 0.7783\n",
            "Epoch 148/200\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 0.3729 - accuracy: 0.8370 - val_loss: 0.5032 - val_accuracy: 0.7778\n",
            "Epoch 149/200\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.3667 - accuracy: 0.8347 - val_loss: 0.5053 - val_accuracy: 0.7772\n",
            "Epoch 150/200\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.3723 - accuracy: 0.8348 - val_loss: 0.4988 - val_accuracy: 0.7772\n",
            "Epoch 151/200\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.3668 - accuracy: 0.8367 - val_loss: 0.5083 - val_accuracy: 0.7733\n",
            "Epoch 152/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.3686 - accuracy: 0.8347 - val_loss: 0.5059 - val_accuracy: 0.7722\n",
            "Epoch 153/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.3695 - accuracy: 0.8354 - val_loss: 0.5037 - val_accuracy: 0.7783\n",
            "Epoch 154/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.3681 - accuracy: 0.8385 - val_loss: 0.5052 - val_accuracy: 0.7722\n",
            "Epoch 155/200\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.3657 - accuracy: 0.8375 - val_loss: 0.5053 - val_accuracy: 0.7794\n",
            "Epoch 156/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.3707 - accuracy: 0.8340 - val_loss: 0.5073 - val_accuracy: 0.7772\n",
            "Epoch 157/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.3635 - accuracy: 0.8369 - val_loss: 0.5018 - val_accuracy: 0.7756\n",
            "Epoch 158/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.3629 - accuracy: 0.8383 - val_loss: 0.5154 - val_accuracy: 0.7689\n",
            "Epoch 159/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.3644 - accuracy: 0.8382 - val_loss: 0.5067 - val_accuracy: 0.7717\n",
            "Epoch 160/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.3616 - accuracy: 0.8424 - val_loss: 0.5092 - val_accuracy: 0.7711\n",
            "Epoch 161/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.3619 - accuracy: 0.8365 - val_loss: 0.5132 - val_accuracy: 0.7739\n",
            "Epoch 162/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.3617 - accuracy: 0.8398 - val_loss: 0.5046 - val_accuracy: 0.7756\n",
            "Epoch 163/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.3602 - accuracy: 0.8419 - val_loss: 0.5051 - val_accuracy: 0.7711\n",
            "Epoch 164/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.3595 - accuracy: 0.8412 - val_loss: 0.5198 - val_accuracy: 0.7678\n",
            "Epoch 165/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.3570 - accuracy: 0.8410 - val_loss: 0.5179 - val_accuracy: 0.7744\n",
            "Epoch 166/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.3599 - accuracy: 0.8389 - val_loss: 0.5129 - val_accuracy: 0.7694\n",
            "Epoch 167/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.3599 - accuracy: 0.8409 - val_loss: 0.5135 - val_accuracy: 0.7728\n",
            "Epoch 168/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.3580 - accuracy: 0.8392 - val_loss: 0.5083 - val_accuracy: 0.7678\n",
            "Epoch 169/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.3611 - accuracy: 0.8415 - val_loss: 0.5143 - val_accuracy: 0.7744\n",
            "Epoch 170/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.3573 - accuracy: 0.8391 - val_loss: 0.5085 - val_accuracy: 0.7678\n",
            "Epoch 171/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.3608 - accuracy: 0.8418 - val_loss: 0.5071 - val_accuracy: 0.7739\n",
            "Epoch 172/200\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.3609 - accuracy: 0.8391 - val_loss: 0.5027 - val_accuracy: 0.7806\n",
            "Epoch 173/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.3599 - accuracy: 0.8396 - val_loss: 0.5070 - val_accuracy: 0.7789\n",
            "Epoch 174/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.3549 - accuracy: 0.8459 - val_loss: 0.5137 - val_accuracy: 0.7706\n",
            "Epoch 175/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.3572 - accuracy: 0.8444 - val_loss: 0.5119 - val_accuracy: 0.7661\n",
            "Epoch 176/200\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.3524 - accuracy: 0.8434 - val_loss: 0.5168 - val_accuracy: 0.7689\n",
            "Epoch 177/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.3550 - accuracy: 0.8427 - val_loss: 0.5173 - val_accuracy: 0.7700\n",
            "Epoch 178/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.3517 - accuracy: 0.8448 - val_loss: 0.5136 - val_accuracy: 0.7706\n",
            "Epoch 179/200\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.3512 - accuracy: 0.8436 - val_loss: 0.5163 - val_accuracy: 0.7678\n",
            "Epoch 180/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.3550 - accuracy: 0.8427 - val_loss: 0.5184 - val_accuracy: 0.7683\n",
            "Epoch 181/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.3559 - accuracy: 0.8441 - val_loss: 0.5181 - val_accuracy: 0.7717\n",
            "Epoch 182/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.3563 - accuracy: 0.8444 - val_loss: 0.5132 - val_accuracy: 0.7650\n",
            "Epoch 183/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.3532 - accuracy: 0.8450 - val_loss: 0.5193 - val_accuracy: 0.7594\n",
            "Epoch 184/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.3538 - accuracy: 0.8437 - val_loss: 0.5203 - val_accuracy: 0.7617\n",
            "Epoch 185/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.3518 - accuracy: 0.8422 - val_loss: 0.5178 - val_accuracy: 0.7633\n",
            "Epoch 186/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.3549 - accuracy: 0.8406 - val_loss: 0.5172 - val_accuracy: 0.7656\n",
            "Epoch 187/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.3493 - accuracy: 0.8453 - val_loss: 0.5252 - val_accuracy: 0.7694\n",
            "Epoch 188/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.3477 - accuracy: 0.8452 - val_loss: 0.5222 - val_accuracy: 0.7661\n",
            "Epoch 189/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.3522 - accuracy: 0.8475 - val_loss: 0.5251 - val_accuracy: 0.7617\n",
            "Epoch 190/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.3488 - accuracy: 0.8487 - val_loss: 0.5251 - val_accuracy: 0.7661\n",
            "Epoch 191/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.3503 - accuracy: 0.8439 - val_loss: 0.5212 - val_accuracy: 0.7683\n",
            "Epoch 192/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.3493 - accuracy: 0.8481 - val_loss: 0.5248 - val_accuracy: 0.7678\n",
            "Epoch 193/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.3457 - accuracy: 0.8490 - val_loss: 0.5241 - val_accuracy: 0.7672\n",
            "Epoch 194/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.3453 - accuracy: 0.8457 - val_loss: 0.5236 - val_accuracy: 0.7700\n",
            "Epoch 195/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.3435 - accuracy: 0.8502 - val_loss: 0.5345 - val_accuracy: 0.7661\n",
            "Epoch 196/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.3429 - accuracy: 0.8513 - val_loss: 0.5284 - val_accuracy: 0.7656\n",
            "Epoch 197/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.3452 - accuracy: 0.8452 - val_loss: 0.5320 - val_accuracy: 0.7667\n",
            "Epoch 198/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.3471 - accuracy: 0.8483 - val_loss: 0.5153 - val_accuracy: 0.7650\n",
            "Epoch 199/200\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.3418 - accuracy: 0.8532 - val_loss: 0.5430 - val_accuracy: 0.7661\n",
            "Epoch 200/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.3462 - accuracy: 0.8481 - val_loss: 0.5280 - val_accuracy: 0.7606\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BpFgMYulhU-3"
      },
      "source": [
        "Now, let's plot the learning curves. What do you notice?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "id": "oVgENUhhhNcQ",
        "outputId": "31f1cc23-2ce7-4181-8f78-039cfa411895"
      },
      "source": [
        "plotter(regularizer_histories)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 576x432 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gcjwERVNHeQR"
      },
      "source": [
        ">(Discussion) Take another look at the loss curves of the dropout model. The model keeps improving until a certain point, after which it starts overfitting, resulting in a poor final model. Suggest one possible solution that can help improve the final model without changing any of its parameters."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def plot1(history):\n",
        "  plt.plot(history.history['loss'], label='loss')\n",
        "  plt.plot(history.history['val_loss'], label='val_loss')\n",
        "  #plt.xscale('log')\n",
        "  plt.xlabel('Epoch')\n",
        "  plt.ylabel('Error [MPG]')\n",
        "  plt.legend()\n",
        "  plt.grid(True)"
      ],
      "metadata": {
        "id": "jAyrb8G3Wtzu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Fzi3Io2u7jf"
      },
      "source": [
        "# Task\n",
        "\n",
        "Your task is to improve the learning curve even further by applying the following models:\n",
        "1. Model 1: Add both L1 and L2 regularization to the large model. Adjust the parameters to minimize overfitting.\n",
        "1. Model 2: Apply early stopping to the large model with dropout and choose appropriate parameters.\n",
        "1. Plot the losses of each of the new models. Comment on your results."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Firstly, I used the generated results from large model without preventing overfitting to compare my results with.**\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "OquwGbliSryi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "regularized_histories_task = {} # new dictioanry to store regularized model histories\n",
        "regularized_histories_task['Large'] = model_histories['Large']"
      ],
      "metadata": {
        "id": "ORP50ewCU6wd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model 1"
      ],
      "metadata": {
        "id": "Hu0cJf8LHNag"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*   **In the following model, L1 Regulariztion is applied on the large model.**\n",
        "*   **I tried different lambda and found out that the best value for it is 0.0004.**\n",
        "*   **This value led to having the validation accuracy very close to the training accuracy (~80%).**\n",
        "*   **So that the model behaves in a similar manner on both training and validation data.**\n"
      ],
      "metadata": {
        "id": "uKiDTlSwTREU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.regularizers import l1\n",
        "\n",
        "l1_lambda = 0.0004\n",
        "l1_model = Sequential([\n",
        "    Dense(512, activation = 'relu',\n",
        "                 kernel_regularizer = l1(l1_lambda),\n",
        "                 input_shape = (shape,)),\n",
        "    Dense(512, activation = 'relu',\n",
        "                 kernel_regularizer = l1(l1_lambda)),\n",
        "    Dense(512, activation = 'relu',\n",
        "                 kernel_regularizer = l1(l1_lambda)),\n",
        "    Dense(512, activation = 'relu',\n",
        "                 kernel_regularizer = l1(l1_lambda)),\n",
        "    Dense(1, activation = \"sigmoid\")\n",
        "])\n",
        "\n",
        "regularized_histories_task['l1'] = compile_and_fit(l1_model, max_epochs = EPOCHS)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3yzsfGo5LofS",
        "outputId": "baa60d04-0706-4bf3-dce9-1b828c5768e7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_32\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_147 (Dense)           (None, 512)               9728      \n",
            "                                                                 \n",
            " dense_148 (Dense)           (None, 512)               262656    \n",
            "                                                                 \n",
            " dense_149 (Dense)           (None, 512)               262656    \n",
            "                                                                 \n",
            " dense_150 (Dense)           (None, 512)               262656    \n",
            "                                                                 \n",
            " dense_151 (Dense)           (None, 1)                 513       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 798,209\n",
            "Trainable params: 798,209\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/200\n",
            "33/33 [==============================] - 1s 10ms/step - loss: 8.8212 - accuracy: 0.7573 - val_loss: 5.1408 - val_accuracy: 0.7717\n",
            "Epoch 2/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 3.0201 - accuracy: 0.7903 - val_loss: 1.3933 - val_accuracy: 0.7756\n",
            "Epoch 3/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.9829 - accuracy: 0.7928 - val_loss: 0.7635 - val_accuracy: 0.7811\n",
            "Epoch 4/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.6919 - accuracy: 0.7958 - val_loss: 0.6517 - val_accuracy: 0.7856\n",
            "Epoch 5/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.6230 - accuracy: 0.7975 - val_loss: 0.6139 - val_accuracy: 0.7872\n",
            "Epoch 6/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.5908 - accuracy: 0.7978 - val_loss: 0.5873 - val_accuracy: 0.7894\n",
            "Epoch 7/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.5730 - accuracy: 0.7980 - val_loss: 0.5705 - val_accuracy: 0.7872\n",
            "Epoch 8/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.5579 - accuracy: 0.7997 - val_loss: 0.5622 - val_accuracy: 0.7844\n",
            "Epoch 9/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.5501 - accuracy: 0.7982 - val_loss: 0.5568 - val_accuracy: 0.7878\n",
            "Epoch 10/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.5421 - accuracy: 0.7980 - val_loss: 0.5484 - val_accuracy: 0.7883\n",
            "Epoch 11/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.5358 - accuracy: 0.7999 - val_loss: 0.5423 - val_accuracy: 0.7883\n",
            "Epoch 12/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.5302 - accuracy: 0.7999 - val_loss: 0.5412 - val_accuracy: 0.7867\n",
            "Epoch 13/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.5281 - accuracy: 0.8003 - val_loss: 0.5355 - val_accuracy: 0.7917\n",
            "Epoch 14/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.5232 - accuracy: 0.8003 - val_loss: 0.5375 - val_accuracy: 0.7894\n",
            "Epoch 15/200\n",
            "33/33 [==============================] - 0s 9ms/step - loss: 0.5210 - accuracy: 0.8001 - val_loss: 0.5294 - val_accuracy: 0.7950\n",
            "Epoch 16/200\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 0.5208 - accuracy: 0.7985 - val_loss: 0.5368 - val_accuracy: 0.7894\n",
            "Epoch 17/200\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.5206 - accuracy: 0.7994 - val_loss: 0.5325 - val_accuracy: 0.7894\n",
            "Epoch 18/200\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.5162 - accuracy: 0.8010 - val_loss: 0.5276 - val_accuracy: 0.7861\n",
            "Epoch 19/200\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.5145 - accuracy: 0.8010 - val_loss: 0.5256 - val_accuracy: 0.7961\n",
            "Epoch 20/200\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.5132 - accuracy: 0.7992 - val_loss: 0.5278 - val_accuracy: 0.7878\n",
            "Epoch 21/200\n",
            "33/33 [==============================] - 0s 9ms/step - loss: 0.5125 - accuracy: 0.8010 - val_loss: 0.5232 - val_accuracy: 0.7922\n",
            "Epoch 22/200\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.5106 - accuracy: 0.8001 - val_loss: 0.5209 - val_accuracy: 0.7900\n",
            "Epoch 23/200\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.5085 - accuracy: 0.8006 - val_loss: 0.5226 - val_accuracy: 0.7911\n",
            "Epoch 24/200\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.5080 - accuracy: 0.8014 - val_loss: 0.5214 - val_accuracy: 0.7906\n",
            "Epoch 25/200\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.5066 - accuracy: 0.8001 - val_loss: 0.5186 - val_accuracy: 0.7894\n",
            "Epoch 26/200\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.5047 - accuracy: 0.8023 - val_loss: 0.5176 - val_accuracy: 0.7917\n",
            "Epoch 27/200\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.5044 - accuracy: 0.8014 - val_loss: 0.5165 - val_accuracy: 0.7922\n",
            "Epoch 28/200\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.5036 - accuracy: 0.8020 - val_loss: 0.5167 - val_accuracy: 0.7944\n",
            "Epoch 29/200\n",
            "33/33 [==============================] - 0s 9ms/step - loss: 0.5022 - accuracy: 0.8004 - val_loss: 0.5181 - val_accuracy: 0.7911\n",
            "Epoch 30/200\n",
            "33/33 [==============================] - 0s 9ms/step - loss: 0.5016 - accuracy: 0.8012 - val_loss: 0.5202 - val_accuracy: 0.7861\n",
            "Epoch 31/200\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.5001 - accuracy: 0.8026 - val_loss: 0.5142 - val_accuracy: 0.7889\n",
            "Epoch 32/200\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.4991 - accuracy: 0.8030 - val_loss: 0.5193 - val_accuracy: 0.7911\n",
            "Epoch 33/200\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.4999 - accuracy: 0.8023 - val_loss: 0.5156 - val_accuracy: 0.7872\n",
            "Epoch 34/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4988 - accuracy: 0.8029 - val_loss: 0.5147 - val_accuracy: 0.7933\n",
            "Epoch 35/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4993 - accuracy: 0.8009 - val_loss: 0.5155 - val_accuracy: 0.7933\n",
            "Epoch 36/200\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.4977 - accuracy: 0.8019 - val_loss: 0.5146 - val_accuracy: 0.7950\n",
            "Epoch 37/200\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 0.4963 - accuracy: 0.8019 - val_loss: 0.5142 - val_accuracy: 0.7911\n",
            "Epoch 38/200\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 0.4966 - accuracy: 0.8006 - val_loss: 0.5130 - val_accuracy: 0.7950\n",
            "Epoch 39/200\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.4954 - accuracy: 0.8029 - val_loss: 0.5121 - val_accuracy: 0.7889\n",
            "Epoch 40/200\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.4951 - accuracy: 0.8015 - val_loss: 0.5117 - val_accuracy: 0.7911\n",
            "Epoch 41/200\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.4946 - accuracy: 0.8016 - val_loss: 0.5110 - val_accuracy: 0.7917\n",
            "Epoch 42/200\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 0.4944 - accuracy: 0.8041 - val_loss: 0.5154 - val_accuracy: 0.7850\n",
            "Epoch 43/200\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.4948 - accuracy: 0.8017 - val_loss: 0.5114 - val_accuracy: 0.7900\n",
            "Epoch 44/200\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.4936 - accuracy: 0.8022 - val_loss: 0.5114 - val_accuracy: 0.7883\n",
            "Epoch 45/200\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.4935 - accuracy: 0.8010 - val_loss: 0.5101 - val_accuracy: 0.7900\n",
            "Epoch 46/200\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.4923 - accuracy: 0.8036 - val_loss: 0.5119 - val_accuracy: 0.7872\n",
            "Epoch 47/200\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.4927 - accuracy: 0.8024 - val_loss: 0.5116 - val_accuracy: 0.7928\n",
            "Epoch 48/200\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.4926 - accuracy: 0.8020 - val_loss: 0.5107 - val_accuracy: 0.7889\n",
            "Epoch 49/200\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 0.4924 - accuracy: 0.8021 - val_loss: 0.5117 - val_accuracy: 0.7883\n",
            "Epoch 50/200\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.4915 - accuracy: 0.8022 - val_loss: 0.5097 - val_accuracy: 0.7900\n",
            "Epoch 51/200\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.4917 - accuracy: 0.8025 - val_loss: 0.5092 - val_accuracy: 0.7928\n",
            "Epoch 52/200\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.4903 - accuracy: 0.8039 - val_loss: 0.5095 - val_accuracy: 0.7872\n",
            "Epoch 53/200\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.4904 - accuracy: 0.8030 - val_loss: 0.5109 - val_accuracy: 0.7856\n",
            "Epoch 54/200\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.4904 - accuracy: 0.8032 - val_loss: 0.5105 - val_accuracy: 0.7894\n",
            "Epoch 55/200\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.4924 - accuracy: 0.8033 - val_loss: 0.5088 - val_accuracy: 0.7906\n",
            "Epoch 56/200\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 0.4893 - accuracy: 0.8030 - val_loss: 0.5103 - val_accuracy: 0.7900\n",
            "Epoch 57/200\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 0.4903 - accuracy: 0.8038 - val_loss: 0.5086 - val_accuracy: 0.7900\n",
            "Epoch 58/200\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.4903 - accuracy: 0.8029 - val_loss: 0.5094 - val_accuracy: 0.7911\n",
            "Epoch 59/200\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.4885 - accuracy: 0.8038 - val_loss: 0.5109 - val_accuracy: 0.7883\n",
            "Epoch 60/200\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.4903 - accuracy: 0.8041 - val_loss: 0.5113 - val_accuracy: 0.7839\n",
            "Epoch 61/200\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.4899 - accuracy: 0.8036 - val_loss: 0.5077 - val_accuracy: 0.7939\n",
            "Epoch 62/200\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.4895 - accuracy: 0.8026 - val_loss: 0.5096 - val_accuracy: 0.7906\n",
            "Epoch 63/200\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.4882 - accuracy: 0.8036 - val_loss: 0.5084 - val_accuracy: 0.7900\n",
            "Epoch 64/200\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.4874 - accuracy: 0.8045 - val_loss: 0.5095 - val_accuracy: 0.7983\n",
            "Epoch 65/200\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.4877 - accuracy: 0.8035 - val_loss: 0.5087 - val_accuracy: 0.7911\n",
            "Epoch 66/200\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.4875 - accuracy: 0.8035 - val_loss: 0.5084 - val_accuracy: 0.7911\n",
            "Epoch 67/200\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.4871 - accuracy: 0.8033 - val_loss: 0.5080 - val_accuracy: 0.7828\n",
            "Epoch 68/200\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.4871 - accuracy: 0.8033 - val_loss: 0.5087 - val_accuracy: 0.7928\n",
            "Epoch 69/200\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.4873 - accuracy: 0.8033 - val_loss: 0.5086 - val_accuracy: 0.7939\n",
            "Epoch 70/200\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.4867 - accuracy: 0.8037 - val_loss: 0.5080 - val_accuracy: 0.7944\n",
            "Epoch 71/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4868 - accuracy: 0.8026 - val_loss: 0.5102 - val_accuracy: 0.7906\n",
            "Epoch 72/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4871 - accuracy: 0.8030 - val_loss: 0.5108 - val_accuracy: 0.7928\n",
            "Epoch 73/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4862 - accuracy: 0.8023 - val_loss: 0.5058 - val_accuracy: 0.7922\n",
            "Epoch 74/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4859 - accuracy: 0.8042 - val_loss: 0.5104 - val_accuracy: 0.7861\n",
            "Epoch 75/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4858 - accuracy: 0.8023 - val_loss: 0.5080 - val_accuracy: 0.7889\n",
            "Epoch 76/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4862 - accuracy: 0.8037 - val_loss: 0.5068 - val_accuracy: 0.7978\n",
            "Epoch 77/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4852 - accuracy: 0.8031 - val_loss: 0.5059 - val_accuracy: 0.7894\n",
            "Epoch 78/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4853 - accuracy: 0.8048 - val_loss: 0.5074 - val_accuracy: 0.7878\n",
            "Epoch 79/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4850 - accuracy: 0.8035 - val_loss: 0.5064 - val_accuracy: 0.7911\n",
            "Epoch 80/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4851 - accuracy: 0.8035 - val_loss: 0.5061 - val_accuracy: 0.7972\n",
            "Epoch 81/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4850 - accuracy: 0.8023 - val_loss: 0.5069 - val_accuracy: 0.7889\n",
            "Epoch 82/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4847 - accuracy: 0.8038 - val_loss: 0.5046 - val_accuracy: 0.7883\n",
            "Epoch 83/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4838 - accuracy: 0.8032 - val_loss: 0.5124 - val_accuracy: 0.7928\n",
            "Epoch 84/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4845 - accuracy: 0.8033 - val_loss: 0.5051 - val_accuracy: 0.7906\n",
            "Epoch 85/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4842 - accuracy: 0.8033 - val_loss: 0.5060 - val_accuracy: 0.7917\n",
            "Epoch 86/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4848 - accuracy: 0.8015 - val_loss: 0.5062 - val_accuracy: 0.7872\n",
            "Epoch 87/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4837 - accuracy: 0.8044 - val_loss: 0.5051 - val_accuracy: 0.7928\n",
            "Epoch 88/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4834 - accuracy: 0.8062 - val_loss: 0.5070 - val_accuracy: 0.7883\n",
            "Epoch 89/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4833 - accuracy: 0.8027 - val_loss: 0.5067 - val_accuracy: 0.7933\n",
            "Epoch 90/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4842 - accuracy: 0.8028 - val_loss: 0.5059 - val_accuracy: 0.7922\n",
            "Epoch 91/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4834 - accuracy: 0.8033 - val_loss: 0.5081 - val_accuracy: 0.7900\n",
            "Epoch 92/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4846 - accuracy: 0.8031 - val_loss: 0.5050 - val_accuracy: 0.7922\n",
            "Epoch 93/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4832 - accuracy: 0.8035 - val_loss: 0.5046 - val_accuracy: 0.7933\n",
            "Epoch 94/200\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 0.4846 - accuracy: 0.8018 - val_loss: 0.5069 - val_accuracy: 0.7922\n",
            "Epoch 95/200\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.4843 - accuracy: 0.8035 - val_loss: 0.5048 - val_accuracy: 0.7906\n",
            "Epoch 96/200\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.4829 - accuracy: 0.8040 - val_loss: 0.5053 - val_accuracy: 0.7883\n",
            "Epoch 97/200\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.4835 - accuracy: 0.8031 - val_loss: 0.5045 - val_accuracy: 0.7900\n",
            "Epoch 98/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4842 - accuracy: 0.8046 - val_loss: 0.5074 - val_accuracy: 0.7944\n",
            "Epoch 99/200\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.4840 - accuracy: 0.8041 - val_loss: 0.5042 - val_accuracy: 0.7906\n",
            "Epoch 100/200\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.4839 - accuracy: 0.8027 - val_loss: 0.5053 - val_accuracy: 0.7939\n",
            "Epoch 101/200\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.4823 - accuracy: 0.8042 - val_loss: 0.5050 - val_accuracy: 0.7928\n",
            "Epoch 102/200\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.4834 - accuracy: 0.8027 - val_loss: 0.5055 - val_accuracy: 0.7928\n",
            "Epoch 103/200\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.4821 - accuracy: 0.8036 - val_loss: 0.5049 - val_accuracy: 0.7883\n",
            "Epoch 104/200\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.4829 - accuracy: 0.8041 - val_loss: 0.5049 - val_accuracy: 0.7906\n",
            "Epoch 105/200\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.4824 - accuracy: 0.8037 - val_loss: 0.5060 - val_accuracy: 0.7894\n",
            "Epoch 106/200\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.4842 - accuracy: 0.8032 - val_loss: 0.5055 - val_accuracy: 0.7928\n",
            "Epoch 107/200\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.4822 - accuracy: 0.8028 - val_loss: 0.5041 - val_accuracy: 0.7917\n",
            "Epoch 108/200\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.4815 - accuracy: 0.8039 - val_loss: 0.5054 - val_accuracy: 0.7950\n",
            "Epoch 109/200\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.4819 - accuracy: 0.8043 - val_loss: 0.5044 - val_accuracy: 0.7939\n",
            "Epoch 110/200\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 0.4824 - accuracy: 0.8044 - val_loss: 0.5039 - val_accuracy: 0.7989\n",
            "Epoch 111/200\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.4826 - accuracy: 0.8049 - val_loss: 0.5042 - val_accuracy: 0.7911\n",
            "Epoch 112/200\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.4811 - accuracy: 0.8040 - val_loss: 0.5056 - val_accuracy: 0.7900\n",
            "Epoch 113/200\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.4817 - accuracy: 0.8040 - val_loss: 0.5054 - val_accuracy: 0.7911\n",
            "Epoch 114/200\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.4818 - accuracy: 0.8044 - val_loss: 0.5028 - val_accuracy: 0.7939\n",
            "Epoch 115/200\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.4816 - accuracy: 0.8039 - val_loss: 0.5056 - val_accuracy: 0.7956\n",
            "Epoch 116/200\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.4827 - accuracy: 0.8046 - val_loss: 0.5048 - val_accuracy: 0.7956\n",
            "Epoch 117/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4811 - accuracy: 0.8048 - val_loss: 0.5043 - val_accuracy: 0.7950\n",
            "Epoch 118/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4807 - accuracy: 0.8052 - val_loss: 0.5060 - val_accuracy: 0.7900\n",
            "Epoch 119/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4819 - accuracy: 0.8027 - val_loss: 0.5056 - val_accuracy: 0.7861\n",
            "Epoch 120/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4809 - accuracy: 0.8034 - val_loss: 0.5052 - val_accuracy: 0.7939\n",
            "Epoch 121/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4805 - accuracy: 0.8054 - val_loss: 0.5050 - val_accuracy: 0.7872\n",
            "Epoch 122/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4800 - accuracy: 0.8047 - val_loss: 0.5028 - val_accuracy: 0.7911\n",
            "Epoch 123/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4805 - accuracy: 0.8041 - val_loss: 0.5048 - val_accuracy: 0.7950\n",
            "Epoch 124/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4795 - accuracy: 0.8048 - val_loss: 0.5062 - val_accuracy: 0.7861\n",
            "Epoch 125/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4811 - accuracy: 0.8049 - val_loss: 0.5047 - val_accuracy: 0.7994\n",
            "Epoch 126/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4803 - accuracy: 0.8036 - val_loss: 0.5045 - val_accuracy: 0.7872\n",
            "Epoch 127/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4804 - accuracy: 0.8033 - val_loss: 0.5041 - val_accuracy: 0.7922\n",
            "Epoch 128/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4806 - accuracy: 0.8043 - val_loss: 0.5066 - val_accuracy: 0.7883\n",
            "Epoch 129/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4806 - accuracy: 0.8048 - val_loss: 0.5035 - val_accuracy: 0.7906\n",
            "Epoch 130/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4791 - accuracy: 0.8041 - val_loss: 0.5076 - val_accuracy: 0.7833\n",
            "Epoch 131/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4799 - accuracy: 0.8045 - val_loss: 0.5033 - val_accuracy: 0.7933\n",
            "Epoch 132/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4797 - accuracy: 0.8051 - val_loss: 0.5051 - val_accuracy: 0.7939\n",
            "Epoch 133/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4800 - accuracy: 0.8041 - val_loss: 0.5054 - val_accuracy: 0.7906\n",
            "Epoch 134/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4796 - accuracy: 0.8040 - val_loss: 0.5059 - val_accuracy: 0.7978\n",
            "Epoch 135/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4811 - accuracy: 0.8019 - val_loss: 0.5047 - val_accuracy: 0.7900\n",
            "Epoch 136/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4791 - accuracy: 0.8049 - val_loss: 0.5082 - val_accuracy: 0.7822\n",
            "Epoch 137/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4795 - accuracy: 0.8046 - val_loss: 0.5035 - val_accuracy: 0.7933\n",
            "Epoch 138/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4800 - accuracy: 0.8038 - val_loss: 0.5039 - val_accuracy: 0.7967\n",
            "Epoch 139/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4782 - accuracy: 0.8052 - val_loss: 0.5033 - val_accuracy: 0.7928\n",
            "Epoch 140/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4794 - accuracy: 0.8051 - val_loss: 0.5037 - val_accuracy: 0.7933\n",
            "Epoch 141/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4788 - accuracy: 0.8049 - val_loss: 0.5031 - val_accuracy: 0.7889\n",
            "Epoch 142/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4791 - accuracy: 0.8054 - val_loss: 0.5061 - val_accuracy: 0.7894\n",
            "Epoch 143/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4798 - accuracy: 0.8042 - val_loss: 0.5032 - val_accuracy: 0.7933\n",
            "Epoch 144/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4792 - accuracy: 0.8036 - val_loss: 0.5069 - val_accuracy: 0.7894\n",
            "Epoch 145/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4787 - accuracy: 0.8040 - val_loss: 0.5035 - val_accuracy: 0.7972\n",
            "Epoch 146/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4784 - accuracy: 0.8045 - val_loss: 0.5032 - val_accuracy: 0.7928\n",
            "Epoch 147/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4790 - accuracy: 0.8060 - val_loss: 0.5038 - val_accuracy: 0.7967\n",
            "Epoch 148/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4792 - accuracy: 0.8061 - val_loss: 0.5029 - val_accuracy: 0.7972\n",
            "Epoch 149/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4789 - accuracy: 0.8036 - val_loss: 0.5025 - val_accuracy: 0.7967\n",
            "Epoch 150/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4788 - accuracy: 0.8057 - val_loss: 0.5064 - val_accuracy: 0.7922\n",
            "Epoch 151/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4785 - accuracy: 0.8055 - val_loss: 0.5044 - val_accuracy: 0.7928\n",
            "Epoch 152/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4778 - accuracy: 0.8056 - val_loss: 0.5031 - val_accuracy: 0.7878\n",
            "Epoch 153/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4797 - accuracy: 0.8051 - val_loss: 0.5048 - val_accuracy: 0.7906\n",
            "Epoch 154/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4791 - accuracy: 0.8046 - val_loss: 0.5035 - val_accuracy: 0.8000\n",
            "Epoch 155/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4783 - accuracy: 0.8044 - val_loss: 0.5054 - val_accuracy: 0.7933\n",
            "Epoch 156/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4774 - accuracy: 0.8057 - val_loss: 0.5041 - val_accuracy: 0.7906\n",
            "Epoch 157/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4776 - accuracy: 0.8049 - val_loss: 0.5036 - val_accuracy: 0.7928\n",
            "Epoch 158/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4776 - accuracy: 0.8060 - val_loss: 0.5037 - val_accuracy: 0.7978\n",
            "Epoch 159/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4776 - accuracy: 0.8067 - val_loss: 0.5036 - val_accuracy: 0.7883\n",
            "Epoch 160/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4773 - accuracy: 0.8055 - val_loss: 0.5045 - val_accuracy: 0.7922\n",
            "Epoch 161/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4775 - accuracy: 0.8062 - val_loss: 0.5035 - val_accuracy: 0.7917\n",
            "Epoch 162/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4781 - accuracy: 0.8033 - val_loss: 0.5041 - val_accuracy: 0.7961\n",
            "Epoch 163/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4785 - accuracy: 0.8043 - val_loss: 0.5034 - val_accuracy: 0.7878\n",
            "Epoch 164/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4771 - accuracy: 0.8060 - val_loss: 0.5046 - val_accuracy: 0.7911\n",
            "Epoch 165/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4774 - accuracy: 0.8049 - val_loss: 0.5057 - val_accuracy: 0.7867\n",
            "Epoch 166/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4784 - accuracy: 0.8058 - val_loss: 0.5041 - val_accuracy: 0.7983\n",
            "Epoch 167/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4777 - accuracy: 0.8049 - val_loss: 0.5051 - val_accuracy: 0.7894\n",
            "Epoch 168/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4770 - accuracy: 0.8054 - val_loss: 0.5062 - val_accuracy: 0.7889\n",
            "Epoch 169/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4776 - accuracy: 0.8060 - val_loss: 0.5041 - val_accuracy: 0.7867\n",
            "Epoch 170/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4770 - accuracy: 0.8056 - val_loss: 0.5040 - val_accuracy: 0.7944\n",
            "Epoch 171/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4771 - accuracy: 0.8064 - val_loss: 0.5039 - val_accuracy: 0.7889\n",
            "Epoch 172/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4770 - accuracy: 0.8067 - val_loss: 0.5061 - val_accuracy: 0.7844\n",
            "Epoch 173/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4768 - accuracy: 0.8064 - val_loss: 0.5056 - val_accuracy: 0.7867\n",
            "Epoch 174/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4769 - accuracy: 0.8064 - val_loss: 0.5043 - val_accuracy: 0.7978\n",
            "Epoch 175/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4773 - accuracy: 0.8048 - val_loss: 0.5043 - val_accuracy: 0.7878\n",
            "Epoch 176/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4774 - accuracy: 0.8051 - val_loss: 0.5047 - val_accuracy: 0.7894\n",
            "Epoch 177/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4762 - accuracy: 0.8056 - val_loss: 0.5047 - val_accuracy: 0.7922\n",
            "Epoch 178/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4767 - accuracy: 0.8062 - val_loss: 0.5054 - val_accuracy: 0.7900\n",
            "Epoch 179/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4773 - accuracy: 0.8070 - val_loss: 0.5037 - val_accuracy: 0.7850\n",
            "Epoch 180/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4757 - accuracy: 0.8076 - val_loss: 0.5036 - val_accuracy: 0.7889\n",
            "Epoch 181/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4762 - accuracy: 0.8069 - val_loss: 0.5027 - val_accuracy: 0.7928\n",
            "Epoch 182/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4758 - accuracy: 0.8052 - val_loss: 0.5048 - val_accuracy: 0.7911\n",
            "Epoch 183/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4768 - accuracy: 0.8059 - val_loss: 0.5062 - val_accuracy: 0.7900\n",
            "Epoch 184/200\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.4765 - accuracy: 0.8070 - val_loss: 0.5035 - val_accuracy: 0.7961\n",
            "Epoch 185/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4756 - accuracy: 0.8059 - val_loss: 0.5051 - val_accuracy: 0.7889\n",
            "Epoch 186/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4762 - accuracy: 0.8060 - val_loss: 0.5043 - val_accuracy: 0.7950\n",
            "Epoch 187/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4760 - accuracy: 0.8071 - val_loss: 0.5062 - val_accuracy: 0.7883\n",
            "Epoch 188/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4763 - accuracy: 0.8057 - val_loss: 0.5074 - val_accuracy: 0.7922\n",
            "Epoch 189/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4762 - accuracy: 0.8062 - val_loss: 0.5041 - val_accuracy: 0.7872\n",
            "Epoch 190/200\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.4761 - accuracy: 0.8065 - val_loss: 0.5088 - val_accuracy: 0.7850\n",
            "Epoch 191/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4767 - accuracy: 0.8053 - val_loss: 0.5040 - val_accuracy: 0.7878\n",
            "Epoch 192/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4755 - accuracy: 0.8044 - val_loss: 0.5049 - val_accuracy: 0.7894\n",
            "Epoch 193/200\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.4750 - accuracy: 0.8064 - val_loss: 0.5044 - val_accuracy: 0.7978\n",
            "Epoch 194/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4760 - accuracy: 0.8054 - val_loss: 0.5049 - val_accuracy: 0.7906\n",
            "Epoch 195/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4747 - accuracy: 0.8076 - val_loss: 0.5039 - val_accuracy: 0.7922\n",
            "Epoch 196/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4760 - accuracy: 0.8063 - val_loss: 0.5065 - val_accuracy: 0.7889\n",
            "Epoch 197/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4759 - accuracy: 0.8060 - val_loss: 0.5071 - val_accuracy: 0.7894\n",
            "Epoch 198/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4763 - accuracy: 0.8070 - val_loss: 0.5037 - val_accuracy: 0.7961\n",
            "Epoch 199/200\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.4754 - accuracy: 0.8080 - val_loss: 0.5042 - val_accuracy: 0.7911\n",
            "Epoch 200/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4749 - accuracy: 0.8072 - val_loss: 0.5024 - val_accuracy: 0.7906\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*   **In the following model, L2 Regulariztion is applied on the large model.**\n",
        "*   **I tried different lambda and found out that the best value for it is 0.01.**\n",
        "*   **This value led to having the validation accuracy very close to the training accuracy (~79%).**\n",
        "*   **So that the model behaves in a similar manner on both training and validation data.**\n"
      ],
      "metadata": {
        "id": "cWSlwfZTWNj8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.regularizers import l2\n",
        "\n",
        "l2_lambda = 0.01\n",
        "l2_model = Sequential([\n",
        "    Dense(512, activation = 'relu',\n",
        "                 kernel_regularizer = l2(l2_lambda),\n",
        "                 input_shape = (shape,)),\n",
        "    Dense(512, activation = 'relu',\n",
        "                 kernel_regularizer = l2(l2_lambda)),\n",
        "    Dense(512, activation = 'relu',\n",
        "                 kernel_regularizer = l2(l2_lambda)),\n",
        "    Dense(512, activation = 'relu',\n",
        "                 kernel_regularizer = l2(l2_lambda)),\n",
        "    Dense(1, activation = \"sigmoid\")\n",
        "])\n",
        "\n",
        "regularized_histories_task['l2'] = compile_and_fit(l2_model, max_epochs = EPOCHS)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W351aamBHMai",
        "outputId": "a98d4031-b61e-4a58-a118-a5de82655491"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_36\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_167 (Dense)           (None, 512)               9728      \n",
            "                                                                 \n",
            " dense_168 (Dense)           (None, 512)               262656    \n",
            "                                                                 \n",
            " dense_169 (Dense)           (None, 512)               262656    \n",
            "                                                                 \n",
            " dense_170 (Dense)           (None, 512)               262656    \n",
            "                                                                 \n",
            " dense_171 (Dense)           (None, 1)                 513       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 798,209\n",
            "Trainable params: 798,209\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/200\n",
            "33/33 [==============================] - 1s 10ms/step - loss: 9.2882 - accuracy: 0.7531 - val_loss: 4.0435 - val_accuracy: 0.7711\n",
            "Epoch 2/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 2.2151 - accuracy: 0.7907 - val_loss: 1.0613 - val_accuracy: 0.7789\n",
            "Epoch 3/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.7566 - accuracy: 0.7917 - val_loss: 0.6038 - val_accuracy: 0.7744\n",
            "Epoch 4/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.5458 - accuracy: 0.7943 - val_loss: 0.5318 - val_accuracy: 0.7772\n",
            "Epoch 5/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.5157 - accuracy: 0.7946 - val_loss: 0.5226 - val_accuracy: 0.7778\n",
            "Epoch 6/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.5079 - accuracy: 0.7973 - val_loss: 0.5161 - val_accuracy: 0.7756\n",
            "Epoch 7/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.5078 - accuracy: 0.7921 - val_loss: 0.5133 - val_accuracy: 0.7767\n",
            "Epoch 8/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.5049 - accuracy: 0.7931 - val_loss: 0.5258 - val_accuracy: 0.7750\n",
            "Epoch 9/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.5012 - accuracy: 0.7951 - val_loss: 0.5088 - val_accuracy: 0.7817\n",
            "Epoch 10/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.5017 - accuracy: 0.7919 - val_loss: 0.5111 - val_accuracy: 0.7767\n",
            "Epoch 11/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4955 - accuracy: 0.7951 - val_loss: 0.5020 - val_accuracy: 0.7800\n",
            "Epoch 12/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4949 - accuracy: 0.7950 - val_loss: 0.5023 - val_accuracy: 0.7789\n",
            "Epoch 13/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4918 - accuracy: 0.7948 - val_loss: 0.5002 - val_accuracy: 0.7817\n",
            "Epoch 14/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4916 - accuracy: 0.7938 - val_loss: 0.4992 - val_accuracy: 0.7828\n",
            "Epoch 15/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4920 - accuracy: 0.7948 - val_loss: 0.5015 - val_accuracy: 0.7850\n",
            "Epoch 16/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4960 - accuracy: 0.7943 - val_loss: 0.4974 - val_accuracy: 0.7861\n",
            "Epoch 17/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4871 - accuracy: 0.7973 - val_loss: 0.4945 - val_accuracy: 0.7861\n",
            "Epoch 18/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4878 - accuracy: 0.7977 - val_loss: 0.4955 - val_accuracy: 0.7822\n",
            "Epoch 19/200\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.4897 - accuracy: 0.7946 - val_loss: 0.4981 - val_accuracy: 0.7822\n",
            "Epoch 20/200\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.4878 - accuracy: 0.7957 - val_loss: 0.5099 - val_accuracy: 0.7772\n",
            "Epoch 21/200\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.4878 - accuracy: 0.7956 - val_loss: 0.4926 - val_accuracy: 0.7817\n",
            "Epoch 22/200\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.4861 - accuracy: 0.7964 - val_loss: 0.4967 - val_accuracy: 0.7811\n",
            "Epoch 23/200\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.4857 - accuracy: 0.7943 - val_loss: 0.4948 - val_accuracy: 0.7839\n",
            "Epoch 24/200\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.4848 - accuracy: 0.7948 - val_loss: 0.4925 - val_accuracy: 0.7844\n",
            "Epoch 25/200\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.4844 - accuracy: 0.7974 - val_loss: 0.4949 - val_accuracy: 0.7844\n",
            "Epoch 26/200\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.4825 - accuracy: 0.7977 - val_loss: 0.4932 - val_accuracy: 0.7867\n",
            "Epoch 27/200\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.4839 - accuracy: 0.7969 - val_loss: 0.4913 - val_accuracy: 0.7839\n",
            "Epoch 28/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4838 - accuracy: 0.7956 - val_loss: 0.5037 - val_accuracy: 0.7822\n",
            "Epoch 29/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4815 - accuracy: 0.7956 - val_loss: 0.4938 - val_accuracy: 0.7844\n",
            "Epoch 30/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4810 - accuracy: 0.7980 - val_loss: 0.4942 - val_accuracy: 0.7822\n",
            "Epoch 31/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4813 - accuracy: 0.7993 - val_loss: 0.4958 - val_accuracy: 0.7867\n",
            "Epoch 32/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4837 - accuracy: 0.7967 - val_loss: 0.4939 - val_accuracy: 0.7828\n",
            "Epoch 33/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4826 - accuracy: 0.7964 - val_loss: 0.5034 - val_accuracy: 0.7817\n",
            "Epoch 34/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4819 - accuracy: 0.7969 - val_loss: 0.4921 - val_accuracy: 0.7806\n",
            "Epoch 35/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4832 - accuracy: 0.7946 - val_loss: 0.4971 - val_accuracy: 0.7806\n",
            "Epoch 36/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4815 - accuracy: 0.7978 - val_loss: 0.4924 - val_accuracy: 0.7833\n",
            "Epoch 37/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4817 - accuracy: 0.7972 - val_loss: 0.4951 - val_accuracy: 0.7850\n",
            "Epoch 38/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4775 - accuracy: 0.7979 - val_loss: 0.4873 - val_accuracy: 0.7883\n",
            "Epoch 39/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4792 - accuracy: 0.7990 - val_loss: 0.4913 - val_accuracy: 0.7839\n",
            "Epoch 40/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4820 - accuracy: 0.7959 - val_loss: 0.4915 - val_accuracy: 0.7850\n",
            "Epoch 41/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4789 - accuracy: 0.7978 - val_loss: 0.4885 - val_accuracy: 0.7850\n",
            "Epoch 42/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4791 - accuracy: 0.7986 - val_loss: 0.4927 - val_accuracy: 0.7844\n",
            "Epoch 43/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4774 - accuracy: 0.7990 - val_loss: 0.4904 - val_accuracy: 0.7856\n",
            "Epoch 44/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4803 - accuracy: 0.7952 - val_loss: 0.4906 - val_accuracy: 0.7806\n",
            "Epoch 45/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4788 - accuracy: 0.7981 - val_loss: 0.4951 - val_accuracy: 0.7794\n",
            "Epoch 46/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4761 - accuracy: 0.7983 - val_loss: 0.4869 - val_accuracy: 0.7861\n",
            "Epoch 47/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4763 - accuracy: 0.7992 - val_loss: 0.4924 - val_accuracy: 0.7872\n",
            "Epoch 48/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4776 - accuracy: 0.7951 - val_loss: 0.4939 - val_accuracy: 0.7817\n",
            "Epoch 49/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4830 - accuracy: 0.7928 - val_loss: 0.4882 - val_accuracy: 0.7811\n",
            "Epoch 50/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4766 - accuracy: 0.7986 - val_loss: 0.4914 - val_accuracy: 0.7850\n",
            "Epoch 51/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4754 - accuracy: 0.7967 - val_loss: 0.4875 - val_accuracy: 0.7861\n",
            "Epoch 52/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4752 - accuracy: 0.7988 - val_loss: 0.4919 - val_accuracy: 0.7800\n",
            "Epoch 53/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4768 - accuracy: 0.7969 - val_loss: 0.4871 - val_accuracy: 0.7833\n",
            "Epoch 54/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4748 - accuracy: 0.7985 - val_loss: 0.4871 - val_accuracy: 0.7889\n",
            "Epoch 55/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4757 - accuracy: 0.7975 - val_loss: 0.4963 - val_accuracy: 0.7811\n",
            "Epoch 56/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4797 - accuracy: 0.7940 - val_loss: 0.5002 - val_accuracy: 0.7833\n",
            "Epoch 57/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4780 - accuracy: 0.7974 - val_loss: 0.4875 - val_accuracy: 0.7850\n",
            "Epoch 58/200\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.4747 - accuracy: 0.7978 - val_loss: 0.4877 - val_accuracy: 0.7800\n",
            "Epoch 59/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4745 - accuracy: 0.7974 - val_loss: 0.4852 - val_accuracy: 0.7872\n",
            "Epoch 60/200\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.4749 - accuracy: 0.7960 - val_loss: 0.4906 - val_accuracy: 0.7833\n",
            "Epoch 61/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4727 - accuracy: 0.7994 - val_loss: 0.4850 - val_accuracy: 0.7872\n",
            "Epoch 62/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4731 - accuracy: 0.7985 - val_loss: 0.5002 - val_accuracy: 0.7778\n",
            "Epoch 63/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4760 - accuracy: 0.7965 - val_loss: 0.4844 - val_accuracy: 0.7878\n",
            "Epoch 64/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4778 - accuracy: 0.7975 - val_loss: 0.4956 - val_accuracy: 0.7822\n",
            "Epoch 65/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4749 - accuracy: 0.7961 - val_loss: 0.4896 - val_accuracy: 0.7828\n",
            "Epoch 66/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4749 - accuracy: 0.7972 - val_loss: 0.4848 - val_accuracy: 0.7850\n",
            "Epoch 67/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4736 - accuracy: 0.7987 - val_loss: 0.4835 - val_accuracy: 0.7867\n",
            "Epoch 68/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4722 - accuracy: 0.7990 - val_loss: 0.4827 - val_accuracy: 0.7878\n",
            "Epoch 69/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4731 - accuracy: 0.7967 - val_loss: 0.4836 - val_accuracy: 0.7839\n",
            "Epoch 70/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4730 - accuracy: 0.7990 - val_loss: 0.4843 - val_accuracy: 0.7856\n",
            "Epoch 71/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4722 - accuracy: 0.7981 - val_loss: 0.4876 - val_accuracy: 0.7867\n",
            "Epoch 72/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4727 - accuracy: 0.7969 - val_loss: 0.4852 - val_accuracy: 0.7850\n",
            "Epoch 73/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4737 - accuracy: 0.7993 - val_loss: 0.4825 - val_accuracy: 0.7856\n",
            "Epoch 74/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4749 - accuracy: 0.7965 - val_loss: 0.4826 - val_accuracy: 0.7872\n",
            "Epoch 75/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4728 - accuracy: 0.7983 - val_loss: 0.4879 - val_accuracy: 0.7839\n",
            "Epoch 76/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4725 - accuracy: 0.7993 - val_loss: 0.4847 - val_accuracy: 0.7889\n",
            "Epoch 77/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4727 - accuracy: 0.7977 - val_loss: 0.4847 - val_accuracy: 0.7889\n",
            "Epoch 78/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4723 - accuracy: 0.7987 - val_loss: 0.4836 - val_accuracy: 0.7911\n",
            "Epoch 79/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4706 - accuracy: 0.7989 - val_loss: 0.4839 - val_accuracy: 0.7861\n",
            "Epoch 80/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4701 - accuracy: 0.7998 - val_loss: 0.4863 - val_accuracy: 0.7867\n",
            "Epoch 81/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4707 - accuracy: 0.7985 - val_loss: 0.4842 - val_accuracy: 0.7850\n",
            "Epoch 82/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4731 - accuracy: 0.7993 - val_loss: 0.4834 - val_accuracy: 0.7872\n",
            "Epoch 83/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4712 - accuracy: 0.7975 - val_loss: 0.4842 - val_accuracy: 0.7844\n",
            "Epoch 84/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4710 - accuracy: 0.7986 - val_loss: 0.4836 - val_accuracy: 0.7872\n",
            "Epoch 85/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4716 - accuracy: 0.7975 - val_loss: 0.4826 - val_accuracy: 0.7906\n",
            "Epoch 86/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4704 - accuracy: 0.7981 - val_loss: 0.4842 - val_accuracy: 0.7822\n",
            "Epoch 87/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4701 - accuracy: 0.7988 - val_loss: 0.4849 - val_accuracy: 0.7833\n",
            "Epoch 88/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4701 - accuracy: 0.7974 - val_loss: 0.4811 - val_accuracy: 0.7894\n",
            "Epoch 89/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4700 - accuracy: 0.7979 - val_loss: 0.4825 - val_accuracy: 0.7911\n",
            "Epoch 90/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4689 - accuracy: 0.8003 - val_loss: 0.4824 - val_accuracy: 0.7867\n",
            "Epoch 91/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4710 - accuracy: 0.7985 - val_loss: 0.4808 - val_accuracy: 0.7906\n",
            "Epoch 92/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4702 - accuracy: 0.8001 - val_loss: 0.4856 - val_accuracy: 0.7828\n",
            "Epoch 93/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4699 - accuracy: 0.7977 - val_loss: 0.4869 - val_accuracy: 0.7861\n",
            "Epoch 94/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4695 - accuracy: 0.8003 - val_loss: 0.4823 - val_accuracy: 0.7850\n",
            "Epoch 95/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4685 - accuracy: 0.8005 - val_loss: 0.4924 - val_accuracy: 0.7817\n",
            "Epoch 96/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4692 - accuracy: 0.7992 - val_loss: 0.4907 - val_accuracy: 0.7839\n",
            "Epoch 97/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4752 - accuracy: 0.7948 - val_loss: 0.4811 - val_accuracy: 0.7894\n",
            "Epoch 98/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4694 - accuracy: 0.7993 - val_loss: 0.4830 - val_accuracy: 0.7856\n",
            "Epoch 99/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4717 - accuracy: 0.7979 - val_loss: 0.4882 - val_accuracy: 0.7822\n",
            "Epoch 100/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4677 - accuracy: 0.7993 - val_loss: 0.4816 - val_accuracy: 0.7883\n",
            "Epoch 101/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4680 - accuracy: 0.7999 - val_loss: 0.4800 - val_accuracy: 0.7911\n",
            "Epoch 102/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4674 - accuracy: 0.8014 - val_loss: 0.4822 - val_accuracy: 0.7850\n",
            "Epoch 103/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4683 - accuracy: 0.8005 - val_loss: 0.4826 - val_accuracy: 0.7856\n",
            "Epoch 104/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4686 - accuracy: 0.7993 - val_loss: 0.4830 - val_accuracy: 0.7878\n",
            "Epoch 105/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4672 - accuracy: 0.7993 - val_loss: 0.4838 - val_accuracy: 0.7844\n",
            "Epoch 106/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4692 - accuracy: 0.7999 - val_loss: 0.4882 - val_accuracy: 0.7794\n",
            "Epoch 107/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4704 - accuracy: 0.7985 - val_loss: 0.4807 - val_accuracy: 0.7850\n",
            "Epoch 108/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4676 - accuracy: 0.8007 - val_loss: 0.4826 - val_accuracy: 0.7894\n",
            "Epoch 109/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4684 - accuracy: 0.7992 - val_loss: 0.4876 - val_accuracy: 0.7822\n",
            "Epoch 110/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4695 - accuracy: 0.7971 - val_loss: 0.4803 - val_accuracy: 0.7861\n",
            "Epoch 111/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4671 - accuracy: 0.8010 - val_loss: 0.4802 - val_accuracy: 0.7889\n",
            "Epoch 112/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4680 - accuracy: 0.8004 - val_loss: 0.4846 - val_accuracy: 0.7839\n",
            "Epoch 113/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4695 - accuracy: 0.7975 - val_loss: 0.4801 - val_accuracy: 0.7894\n",
            "Epoch 114/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4696 - accuracy: 0.7998 - val_loss: 0.4813 - val_accuracy: 0.7883\n",
            "Epoch 115/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4670 - accuracy: 0.8006 - val_loss: 0.4842 - val_accuracy: 0.7883\n",
            "Epoch 116/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4670 - accuracy: 0.7990 - val_loss: 0.4823 - val_accuracy: 0.7878\n",
            "Epoch 117/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4687 - accuracy: 0.7971 - val_loss: 0.4822 - val_accuracy: 0.7883\n",
            "Epoch 118/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4681 - accuracy: 0.7972 - val_loss: 0.4931 - val_accuracy: 0.7856\n",
            "Epoch 119/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4678 - accuracy: 0.7988 - val_loss: 0.4830 - val_accuracy: 0.7894\n",
            "Epoch 120/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4667 - accuracy: 0.8004 - val_loss: 0.4830 - val_accuracy: 0.7856\n",
            "Epoch 121/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4687 - accuracy: 0.8003 - val_loss: 0.4882 - val_accuracy: 0.7867\n",
            "Epoch 122/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4686 - accuracy: 0.7999 - val_loss: 0.4819 - val_accuracy: 0.7839\n",
            "Epoch 123/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4678 - accuracy: 0.7987 - val_loss: 0.4820 - val_accuracy: 0.7856\n",
            "Epoch 124/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4663 - accuracy: 0.7996 - val_loss: 0.4803 - val_accuracy: 0.7883\n",
            "Epoch 125/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4664 - accuracy: 0.8012 - val_loss: 0.4801 - val_accuracy: 0.7844\n",
            "Epoch 126/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4666 - accuracy: 0.8001 - val_loss: 0.4858 - val_accuracy: 0.7844\n",
            "Epoch 127/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4670 - accuracy: 0.7999 - val_loss: 0.4826 - val_accuracy: 0.7933\n",
            "Epoch 128/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4678 - accuracy: 0.8001 - val_loss: 0.4833 - val_accuracy: 0.7883\n",
            "Epoch 129/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4674 - accuracy: 0.7997 - val_loss: 0.4826 - val_accuracy: 0.7833\n",
            "Epoch 130/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4667 - accuracy: 0.7994 - val_loss: 0.4806 - val_accuracy: 0.7894\n",
            "Epoch 131/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4649 - accuracy: 0.8002 - val_loss: 0.4799 - val_accuracy: 0.7883\n",
            "Epoch 132/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4656 - accuracy: 0.8008 - val_loss: 0.4801 - val_accuracy: 0.7867\n",
            "Epoch 133/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4640 - accuracy: 0.8012 - val_loss: 0.4824 - val_accuracy: 0.7878\n",
            "Epoch 134/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4684 - accuracy: 0.7986 - val_loss: 0.4794 - val_accuracy: 0.7856\n",
            "Epoch 135/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4667 - accuracy: 0.8004 - val_loss: 0.4834 - val_accuracy: 0.7850\n",
            "Epoch 136/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4669 - accuracy: 0.8015 - val_loss: 0.4809 - val_accuracy: 0.7833\n",
            "Epoch 137/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4647 - accuracy: 0.8023 - val_loss: 0.4789 - val_accuracy: 0.7900\n",
            "Epoch 138/200\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.4667 - accuracy: 0.7993 - val_loss: 0.4797 - val_accuracy: 0.7878\n",
            "Epoch 139/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4649 - accuracy: 0.8004 - val_loss: 0.4789 - val_accuracy: 0.7894\n",
            "Epoch 140/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4664 - accuracy: 0.7992 - val_loss: 0.4854 - val_accuracy: 0.7833\n",
            "Epoch 141/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4658 - accuracy: 0.7996 - val_loss: 0.4796 - val_accuracy: 0.7900\n",
            "Epoch 142/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4649 - accuracy: 0.7976 - val_loss: 0.4803 - val_accuracy: 0.7900\n",
            "Epoch 143/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4642 - accuracy: 0.7998 - val_loss: 0.4793 - val_accuracy: 0.7878\n",
            "Epoch 144/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4688 - accuracy: 0.7981 - val_loss: 0.4892 - val_accuracy: 0.7844\n",
            "Epoch 145/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4672 - accuracy: 0.7989 - val_loss: 0.4826 - val_accuracy: 0.7861\n",
            "Epoch 146/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4656 - accuracy: 0.8001 - val_loss: 0.4797 - val_accuracy: 0.7900\n",
            "Epoch 147/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4646 - accuracy: 0.8002 - val_loss: 0.4833 - val_accuracy: 0.7872\n",
            "Epoch 148/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4647 - accuracy: 0.8018 - val_loss: 0.4802 - val_accuracy: 0.7861\n",
            "Epoch 149/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4646 - accuracy: 0.8006 - val_loss: 0.4793 - val_accuracy: 0.7867\n",
            "Epoch 150/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4646 - accuracy: 0.8003 - val_loss: 0.4827 - val_accuracy: 0.7883\n",
            "Epoch 151/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4658 - accuracy: 0.7999 - val_loss: 0.4776 - val_accuracy: 0.7878\n",
            "Epoch 152/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4636 - accuracy: 0.8011 - val_loss: 0.4799 - val_accuracy: 0.7878\n",
            "Epoch 153/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4658 - accuracy: 0.7990 - val_loss: 0.4786 - val_accuracy: 0.7878\n",
            "Epoch 154/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4681 - accuracy: 0.7986 - val_loss: 0.4783 - val_accuracy: 0.7883\n",
            "Epoch 155/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4644 - accuracy: 0.7987 - val_loss: 0.4807 - val_accuracy: 0.7883\n",
            "Epoch 156/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4649 - accuracy: 0.8001 - val_loss: 0.4782 - val_accuracy: 0.7872\n",
            "Epoch 157/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4627 - accuracy: 0.8014 - val_loss: 0.4810 - val_accuracy: 0.7878\n",
            "Epoch 158/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4652 - accuracy: 0.8019 - val_loss: 0.4791 - val_accuracy: 0.7867\n",
            "Epoch 159/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4636 - accuracy: 0.8011 - val_loss: 0.4799 - val_accuracy: 0.7856\n",
            "Epoch 160/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4636 - accuracy: 0.8004 - val_loss: 0.4808 - val_accuracy: 0.7850\n",
            "Epoch 161/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4649 - accuracy: 0.7989 - val_loss: 0.4800 - val_accuracy: 0.7867\n",
            "Epoch 162/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4656 - accuracy: 0.7993 - val_loss: 0.4782 - val_accuracy: 0.7872\n",
            "Epoch 163/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4637 - accuracy: 0.8015 - val_loss: 0.4827 - val_accuracy: 0.7867\n",
            "Epoch 164/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4684 - accuracy: 0.7983 - val_loss: 0.4785 - val_accuracy: 0.7900\n",
            "Epoch 165/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4653 - accuracy: 0.7983 - val_loss: 0.4787 - val_accuracy: 0.7883\n",
            "Epoch 166/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4628 - accuracy: 0.8011 - val_loss: 0.4767 - val_accuracy: 0.7894\n",
            "Epoch 167/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4620 - accuracy: 0.8017 - val_loss: 0.4798 - val_accuracy: 0.7867\n",
            "Epoch 168/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4649 - accuracy: 0.8001 - val_loss: 0.4784 - val_accuracy: 0.7906\n",
            "Epoch 169/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4628 - accuracy: 0.8016 - val_loss: 0.4783 - val_accuracy: 0.7933\n",
            "Epoch 170/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4627 - accuracy: 0.8008 - val_loss: 0.4803 - val_accuracy: 0.7844\n",
            "Epoch 171/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4647 - accuracy: 0.8007 - val_loss: 0.4777 - val_accuracy: 0.7889\n",
            "Epoch 172/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4648 - accuracy: 0.7987 - val_loss: 0.4801 - val_accuracy: 0.7906\n",
            "Epoch 173/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4641 - accuracy: 0.8002 - val_loss: 0.4770 - val_accuracy: 0.7878\n",
            "Epoch 174/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4658 - accuracy: 0.8004 - val_loss: 0.4819 - val_accuracy: 0.7850\n",
            "Epoch 175/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4642 - accuracy: 0.8002 - val_loss: 0.4827 - val_accuracy: 0.7850\n",
            "Epoch 176/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4643 - accuracy: 0.8009 - val_loss: 0.4836 - val_accuracy: 0.7883\n",
            "Epoch 177/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4644 - accuracy: 0.7993 - val_loss: 0.4792 - val_accuracy: 0.7906\n",
            "Epoch 178/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4625 - accuracy: 0.8006 - val_loss: 0.4766 - val_accuracy: 0.7928\n",
            "Epoch 179/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4622 - accuracy: 0.8001 - val_loss: 0.4786 - val_accuracy: 0.7911\n",
            "Epoch 180/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4617 - accuracy: 0.8004 - val_loss: 0.4770 - val_accuracy: 0.7894\n",
            "Epoch 181/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4651 - accuracy: 0.8003 - val_loss: 0.4807 - val_accuracy: 0.7956\n",
            "Epoch 182/200\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.4649 - accuracy: 0.7980 - val_loss: 0.4777 - val_accuracy: 0.7894\n",
            "Epoch 183/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4653 - accuracy: 0.8018 - val_loss: 0.4774 - val_accuracy: 0.7883\n",
            "Epoch 184/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4631 - accuracy: 0.7989 - val_loss: 0.4795 - val_accuracy: 0.7856\n",
            "Epoch 185/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4630 - accuracy: 0.8010 - val_loss: 0.4787 - val_accuracy: 0.7872\n",
            "Epoch 186/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4642 - accuracy: 0.8009 - val_loss: 0.4783 - val_accuracy: 0.7900\n",
            "Epoch 187/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4621 - accuracy: 0.8021 - val_loss: 0.4779 - val_accuracy: 0.7856\n",
            "Epoch 188/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4636 - accuracy: 0.7999 - val_loss: 0.4782 - val_accuracy: 0.7878\n",
            "Epoch 189/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4655 - accuracy: 0.7969 - val_loss: 0.4767 - val_accuracy: 0.7878\n",
            "Epoch 190/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4636 - accuracy: 0.8006 - val_loss: 0.4788 - val_accuracy: 0.7939\n",
            "Epoch 191/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4631 - accuracy: 0.7998 - val_loss: 0.4779 - val_accuracy: 0.7883\n",
            "Epoch 192/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4626 - accuracy: 0.7989 - val_loss: 0.4765 - val_accuracy: 0.7894\n",
            "Epoch 193/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4636 - accuracy: 0.8019 - val_loss: 0.4766 - val_accuracy: 0.7917\n",
            "Epoch 194/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4631 - accuracy: 0.8007 - val_loss: 0.4774 - val_accuracy: 0.7883\n",
            "Epoch 195/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4616 - accuracy: 0.8005 - val_loss: 0.4786 - val_accuracy: 0.7906\n",
            "Epoch 196/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4619 - accuracy: 0.8000 - val_loss: 0.4808 - val_accuracy: 0.7894\n",
            "Epoch 197/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4624 - accuracy: 0.8006 - val_loss: 0.4778 - val_accuracy: 0.7922\n",
            "Epoch 198/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4630 - accuracy: 0.8002 - val_loss: 0.4781 - val_accuracy: 0.7872\n",
            "Epoch 199/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4634 - accuracy: 0.7986 - val_loss: 0.4771 - val_accuracy: 0.7894\n",
            "Epoch 200/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4615 - accuracy: 0.8023 - val_loss: 0.4779 - val_accuracy: 0.7900\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model 2"
      ],
      "metadata": {
        "id": "0le_IYG8OC4k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*   **In order to implement the early stopping method, I added the early stopping callback to compile and fit function.**\n",
        "*   **The monitor ic chosen to be the validation loss.**\n",
        "*   **I tried different values for patience and the value of 15 gives optimal results.**\n",
        "*   **The model behaves in a similar manner on both training and validation data with the same loss and accuracy of (~80%).**\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "P0LtOm57XUZh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.callbacks import EarlyStopping\n",
        "\n",
        "def compile_and_fit_early_stop(model, optimizer=None, max_epochs=1000):\n",
        "  if optimizer is None:\n",
        "    optimizer = get_optimizer()\n",
        "  \n",
        "  # Compile model and print summary\n",
        "  model.compile(optimizer = optimizer,\n",
        "                loss = \"binary_crossentropy\",\n",
        "                metrics = ['accuracy'])\n",
        "\n",
        "  model.summary()\n",
        "  keras_callbacks   = [\n",
        "      EarlyStopping(monitor='val_loss', patience=15, mode='min', min_delta=0.0001)]\n",
        "\n",
        "  # Fit model to training data\n",
        "  history = model.fit(\n",
        "    x = preprocessing_input.transform(X_train), # do not forget to normalize X\n",
        "    y = y_train,\n",
        "    batch_size = BATCH_SIZE,\n",
        "    epochs = max_epochs,\n",
        "    validation_split = 0.1,\n",
        "    callbacks=keras_callbacks)\n",
        "\n",
        "  return history"
      ],
      "metadata": {
        "id": "fVP__JORNM8c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.layers import Dropout\n",
        "\n",
        "dropout_rate = 0.5\n",
        "dropout_model = Sequential([\n",
        "    Dense(512, activation='relu', input_shape=(shape,)),\n",
        "    Dropout(dropout_rate),\n",
        "    Dense(512, activation='relu'),\n",
        "    Dropout(dropout_rate),\n",
        "    Dense(512, activation='relu'),\n",
        "    Dropout(dropout_rate),\n",
        "    Dense(512, activation='relu'),\n",
        "    Dropout(dropout_rate),\n",
        "    Dense(1, activation = \"sigmoid\")\n",
        "])\n",
        "\n",
        "regularized_histories_task['dropout_early_stop'] = compile_and_fit_early_stop(dropout_model, max_epochs = EPOCHS)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3ReoshY5PKUw",
        "outputId": "61c497ac-858a-41de-e745-f3824a46b070"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_37\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_172 (Dense)           (None, 512)               9728      \n",
            "                                                                 \n",
            " dropout_36 (Dropout)        (None, 512)               0         \n",
            "                                                                 \n",
            " dense_173 (Dense)           (None, 512)               262656    \n",
            "                                                                 \n",
            " dropout_37 (Dropout)        (None, 512)               0         \n",
            "                                                                 \n",
            " dense_174 (Dense)           (None, 512)               262656    \n",
            "                                                                 \n",
            " dropout_38 (Dropout)        (None, 512)               0         \n",
            "                                                                 \n",
            " dense_175 (Dense)           (None, 512)               262656    \n",
            "                                                                 \n",
            " dropout_39 (Dropout)        (None, 512)               0         \n",
            "                                                                 \n",
            " dense_176 (Dense)           (None, 1)                 513       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 798,209\n",
            "Trainable params: 798,209\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/200\n",
            "33/33 [==============================] - 1s 9ms/step - loss: 0.5456 - accuracy: 0.7240 - val_loss: 0.4716 - val_accuracy: 0.7644\n",
            "Epoch 2/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4762 - accuracy: 0.7806 - val_loss: 0.4594 - val_accuracy: 0.7750\n",
            "Epoch 3/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4646 - accuracy: 0.7865 - val_loss: 0.4554 - val_accuracy: 0.7839\n",
            "Epoch 4/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4584 - accuracy: 0.7901 - val_loss: 0.4599 - val_accuracy: 0.7794\n",
            "Epoch 5/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4539 - accuracy: 0.7925 - val_loss: 0.4563 - val_accuracy: 0.7839\n",
            "Epoch 6/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4553 - accuracy: 0.7905 - val_loss: 0.4616 - val_accuracy: 0.7806\n",
            "Epoch 7/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4502 - accuracy: 0.7932 - val_loss: 0.4572 - val_accuracy: 0.7817\n",
            "Epoch 8/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4487 - accuracy: 0.7935 - val_loss: 0.4584 - val_accuracy: 0.7856\n",
            "Epoch 9/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4491 - accuracy: 0.7945 - val_loss: 0.4649 - val_accuracy: 0.7861\n",
            "Epoch 10/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4468 - accuracy: 0.7975 - val_loss: 0.4551 - val_accuracy: 0.7906\n",
            "Epoch 11/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4473 - accuracy: 0.7962 - val_loss: 0.4537 - val_accuracy: 0.7911\n",
            "Epoch 12/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4434 - accuracy: 0.7980 - val_loss: 0.4533 - val_accuracy: 0.7856\n",
            "Epoch 13/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4422 - accuracy: 0.7967 - val_loss: 0.4570 - val_accuracy: 0.7861\n",
            "Epoch 14/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4457 - accuracy: 0.7964 - val_loss: 0.4566 - val_accuracy: 0.7900\n",
            "Epoch 15/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4417 - accuracy: 0.7982 - val_loss: 0.4501 - val_accuracy: 0.7883\n",
            "Epoch 16/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4427 - accuracy: 0.8003 - val_loss: 0.4582 - val_accuracy: 0.7867\n",
            "Epoch 17/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4377 - accuracy: 0.8017 - val_loss: 0.4565 - val_accuracy: 0.7939\n",
            "Epoch 18/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4385 - accuracy: 0.8019 - val_loss: 0.4527 - val_accuracy: 0.7878\n",
            "Epoch 19/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4393 - accuracy: 0.8024 - val_loss: 0.4584 - val_accuracy: 0.7928\n",
            "Epoch 20/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4388 - accuracy: 0.7991 - val_loss: 0.4532 - val_accuracy: 0.7883\n",
            "Epoch 21/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4351 - accuracy: 0.8010 - val_loss: 0.4587 - val_accuracy: 0.7883\n",
            "Epoch 22/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4373 - accuracy: 0.7999 - val_loss: 0.4541 - val_accuracy: 0.7911\n",
            "Epoch 23/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4350 - accuracy: 0.8043 - val_loss: 0.4546 - val_accuracy: 0.7917\n",
            "Epoch 24/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4352 - accuracy: 0.8021 - val_loss: 0.4560 - val_accuracy: 0.7894\n",
            "Epoch 25/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4294 - accuracy: 0.8034 - val_loss: 0.4511 - val_accuracy: 0.7900\n",
            "Epoch 26/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4321 - accuracy: 0.8015 - val_loss: 0.4527 - val_accuracy: 0.7861\n",
            "Epoch 27/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4324 - accuracy: 0.8042 - val_loss: 0.4527 - val_accuracy: 0.7922\n",
            "Epoch 28/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4347 - accuracy: 0.8007 - val_loss: 0.4525 - val_accuracy: 0.7889\n",
            "Epoch 29/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4321 - accuracy: 0.8033 - val_loss: 0.4538 - val_accuracy: 0.7961\n",
            "Epoch 30/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4318 - accuracy: 0.8030 - val_loss: 0.4539 - val_accuracy: 0.7911\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 3"
      ],
      "metadata": {
        "id": "iG36v5SQYLt9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Here I plotted the losses of the modified models**\n",
        "\n",
        "*   **The model that shows the best results is the model that uses dropout and early stopping to avoid overfitting since it leads to the minimum loss on both validation and training data.**\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "nhYKUo-hYNy7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plotter(regularized_histories_task)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "id": "4aDxY5etPKPw",
        "outputId": "e160a509-d314-4952-9340-8854e14eb2d3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 576x432 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Drl9_1Bm9rSU"
      },
      "source": [
        "## Bonus Questions\n",
        "\n",
        "1. [0.5 pt] In the code above, we used `kernel_regularizer`. What is the difference between `kernel_regularizer`, `bias_regularizer`, and`activity_regularizer` in Keras?\n",
        "> Answer:\n",
        "\n",
        "*   **`kernel_regularizer`: is used to apply penalty on the kernels of layers.**\n",
        "*   **`bias_regularizer`: is used to apply penalty on the bias of the layers.** \n",
        "*   **`activity_regularizer`: is used to apply penalty on the output of the layers.**\n",
        "\n",
        "\n",
        "1. [0.5 pt] Why not to use the dropout during testing instead of scaling?\n",
        "> Answer:\n",
        "**Because the dropout is done randomly by dropping random neurons so that it can't be using during testing.**\n",
        "1. [1 pt] What is the difference between the effect of L1 regularization and that of L2 regularization? Explain in light of the figure below.\n",
        "<center><img src=\"https://miro.medium.com/max/602/0*_pKBpbrub8v6np5x.png\" width=\"300px\"></center>\n",
        "\n",
        "> Answer:\n",
        "\n",
        "**L1 regularization can be used in feauture selection since optimizing the loss function will lead to have one of the weights equal zero (which is shown in the figure \"w1 = 0\"). On the other hand L2 regularization prevent overfitting by putting constraints on the weights, but it doesn't select some features and get rid of the others.**"
      ]
    }
  ]
}